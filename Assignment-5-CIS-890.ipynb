{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ****** Charts and Tensorboard plots are visible in the pdf version submitted ******     Assignment 5 - RNN, LSTM and GRU models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Load and preprocess the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read train data\n",
    "df = pd.read_csv('data/sst_train.txt', sep='\\t', header=None, names=['truth', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dev data\n",
    "dev = pd.read_csv('data/sst_dev.txt', sep='\\t', header=None, names=['truth', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data\n",
    "test = pd.read_csv('data/sst_test.txt', sep='\\t', header=None, names=['truth', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['truth'] = df['truth'].str.replace('__label__', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['truth'] = dev['truth'].str.replace('__label__', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['truth'] = test['truth'].str.replace('__label__', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['truth'] = df['truth'].astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['truth'] = dev['truth'].astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['truth'] = test['truth'].astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  truth                                               text\n",
       "0     4  It 's a lovely film with lovely performances b...\n",
       "1     3  No one goes unindicted here , which is probabl...\n",
       "2     4  And if you 're not nearly moved to tears by a ...\n",
       "3     5                   A warm , funny , engaging film .\n",
       "4     5  Uses sharp humor and insight into human nature..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  truth                                               text\n",
       "0     3                     Effective but too-tepid biopic\n",
       "1     4  If you sometimes like to go to the movies to h...\n",
       "2     5  Emerges as something rare , an issue movie tha...\n",
       "3     3  The film provides some great insight into the ...\n",
       "4     5  Offers that rare combination of entertainment ..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  truth                                               text\n",
       "0     4  The Rock is destined to be the 21st Century 's...\n",
       "1     5  The gorgeously elaborate continuation of `` Th...\n",
       "2     4  Singer/composer Bryan Adams contributes a slew...\n",
       "3     3  You 'd think by now America would have had eno...\n",
       "4     4               Yet the act is still charming here ."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8544"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count training samples\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev data Visualizations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = dev['truth'].value_counts(sort=False).plot(kind='barh')\n",
    "ax.set_xlabel(\"Number of Samples in dev Set\")\n",
    "ax.set_ylabel(\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = dev['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dev)):\n",
    "    integer_encoded = [ord(char) for char in data_dev[i]]\n",
    "    dev_X.append(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X_test = np.array(dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X_t = pad_sequences(dev_X_test, maxlen=267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 267)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_y = pd.get_dummies(dev['truth']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Visualizations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = test['truth'].value_counts(sort=False).plot(kind='barh')\n",
    "ax.set_xlabel(\"Number of Samples in test Set\")\n",
    "ax.set_ylabel(\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    integer_encoded = [ord(char) for char in data_test[i]]\n",
    "    test_X.append(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_2 = np.array(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2210,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_t = pad_sequences(test_X_2, maxlen=267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2210, 267)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = pd.get_dummies(test['truth']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Visualizations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ax = df['truth'].value_counts(sort=False).plot(kind='barh')\n",
    "ax.set_xlabel(\"Number of Samples in training Set\")\n",
    "ax.set_ylabel(\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['truth'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 4, 3, 4, ..., 1, 2, 4, 1, 2]\n",
       "Length: 8544\n",
       "Categories (5, int64): [1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = pd.get_dummies(df['truth']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8544"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "# define input string\n",
    "#data = 'hello world'\n",
    "data = df['text']\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.append(onehot_encoded)\n",
    "\n",
    "#X.append(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8544"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    integer_encoded = [ord(char) for char in data[i]]\n",
    "    X.append(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert encoding\n",
    "#inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = pad_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y_t, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vanilla RNN model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = tf.keras.Sequential()\n",
    "model_rnn.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_rnn.add(layers.Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.add(layers.SimpleRNN(256))\n",
    "model_rnn.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 267, 256)          1280000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,412,613\n",
      "Trainable params: 1,412,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_1=\"logs_1/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_1, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.421986). Check your callbacks.\n",
      "6835/6835 - 29s - loss: 1.6206 - accuracy: 0.2506 - val_loss: 1.5783 - val_accuracy: 0.2709\n",
      "Epoch 2/10\n",
      "6835/6835 - 27s - loss: 1.6193 - accuracy: 0.2600 - val_loss: 1.5777 - val_accuracy: 0.2709\n",
      "Epoch 3/10\n",
      "6835/6835 - 27s - loss: 1.6368 - accuracy: 0.2506 - val_loss: 1.6156 - val_accuracy: 0.2657\n",
      "Epoch 4/10\n",
      "6835/6835 - 26s - loss: 1.5861 - accuracy: 0.2587 - val_loss: 1.5774 - val_accuracy: 0.2697\n",
      "Epoch 5/10\n",
      "6835/6835 - 26s - loss: 1.5891 - accuracy: 0.2648 - val_loss: 1.5862 - val_accuracy: 0.2697\n",
      "Epoch 6/10\n",
      "6835/6835 - 26s - loss: 1.5865 - accuracy: 0.2713 - val_loss: 1.5817 - val_accuracy: 0.2703\n",
      "Epoch 7/10\n",
      "6835/6835 - 26s - loss: 1.5854 - accuracy: 0.2679 - val_loss: 1.5842 - val_accuracy: 0.2709\n",
      "Epoch 8/10\n",
      "6835/6835 - 26s - loss: 1.5814 - accuracy: 0.2614 - val_loss: 1.5969 - val_accuracy: 0.2715\n",
      "Epoch 9/10\n",
      "6835/6835 - 26s - loss: 1.5902 - accuracy: 0.2680 - val_loss: 1.5852 - val_accuracy: 0.2738\n",
      "Epoch 10/10\n",
      "6835/6835 - 26s - loss: 1.5839 - accuracy: 0.2625 - val_loss: 1.5779 - val_accuracy: 0.2709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f15400fb410>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_rnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "\n",
    "model_rnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Visualizations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-631f3b90138c6fab\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-631f3b90138c6fab\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_1/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of vanilla rnn model on dev subset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model_rnn.evaluate(dev_X_t, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26430517"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of vanilla rnn on dev subset = 26.43% ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vanilla RNN model - 2 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.442880). Check your callbacks.\n",
      "6835/6835 - 28s - loss: 1.6073 - accuracy: 0.2534 - val_loss: 1.6004 - val_accuracy: 0.2703\n",
      "Epoch 2/20\n",
      "6835/6835 - 26s - loss: 1.5903 - accuracy: 0.2660 - val_loss: 1.5640 - val_accuracy: 0.2703\n",
      "Epoch 3/20\n",
      "6835/6835 - 26s - loss: 1.6017 - accuracy: 0.2614 - val_loss: 1.5946 - val_accuracy: 0.1861\n",
      "Epoch 4/20\n",
      "6835/6835 - 26s - loss: 1.5865 - accuracy: 0.2631 - val_loss: 1.6006 - val_accuracy: 0.1861\n",
      "Epoch 5/20\n",
      "6835/6835 - 26s - loss: 1.5823 - accuracy: 0.2718 - val_loss: 1.5997 - val_accuracy: 0.2709\n",
      "Epoch 6/20\n",
      "6835/6835 - 26s - loss: 1.5830 - accuracy: 0.2691 - val_loss: 1.5618 - val_accuracy: 0.2738\n",
      "Epoch 7/20\n",
      "6835/6835 - 27s - loss: 1.5866 - accuracy: 0.2658 - val_loss: 1.6199 - val_accuracy: 0.1270\n",
      "Epoch 8/20\n",
      "6835/6835 - 26s - loss: 1.5873 - accuracy: 0.2604 - val_loss: 1.6174 - val_accuracy: 0.2738\n",
      "Epoch 9/20\n",
      "6835/6835 - 26s - loss: 1.5778 - accuracy: 0.2642 - val_loss: 1.5705 - val_accuracy: 0.2750\n",
      "Epoch 10/20\n",
      "6835/6835 - 26s - loss: 1.5817 - accuracy: 0.2729 - val_loss: 1.5705 - val_accuracy: 0.2697\n",
      "Epoch 11/20\n",
      "6835/6835 - 26s - loss: 1.5857 - accuracy: 0.2645 - val_loss: 1.5676 - val_accuracy: 0.2762\n",
      "Epoch 12/20\n",
      "6835/6835 - 26s - loss: 1.5901 - accuracy: 0.2606 - val_loss: 1.5588 - val_accuracy: 0.2721\n",
      "Epoch 13/20\n",
      "6835/6835 - 26s - loss: 1.5801 - accuracy: 0.2603 - val_loss: 1.5693 - val_accuracy: 0.2721\n",
      "Epoch 14/20\n",
      "6835/6835 - 26s - loss: 1.5867 - accuracy: 0.2576 - val_loss: 1.5688 - val_accuracy: 0.2733\n",
      "Epoch 15/20\n",
      "6835/6835 - 26s - loss: 1.5741 - accuracy: 0.2768 - val_loss: 1.5646 - val_accuracy: 0.2709\n",
      "Epoch 16/20\n",
      "6835/6835 - 26s - loss: 1.5921 - accuracy: 0.2711 - val_loss: 1.5761 - val_accuracy: 0.2721\n",
      "Epoch 17/20\n",
      "6835/6835 - 26s - loss: 1.5828 - accuracy: 0.2636 - val_loss: 1.5698 - val_accuracy: 0.2709\n",
      "Epoch 18/20\n",
      "6835/6835 - 26s - loss: 1.5804 - accuracy: 0.2629 - val_loss: 1.5748 - val_accuracy: 0.2703\n",
      "Epoch 19/20\n",
      "6835/6835 - 26s - loss: 1.5795 - accuracy: 0.2686 - val_loss: 1.5655 - val_accuracy: 0.2733\n",
      "Epoch 20/20\n",
      "6835/6835 - 26s - loss: 1.5825 - accuracy: 0.2753 - val_loss: 1.5611 - val_accuracy: 0.2715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f153a809210>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn_2 = tf.keras.Sequential()\n",
    "model_rnn_2.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))\n",
    "\n",
    "model_rnn_2.add(layers.SimpleRNN(256))\n",
    "model_rnn_2.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_rnn_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#model_rnn.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "log_dir_5=\"logs_5/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_5, histogram_freq=1)\n",
    "\n",
    "model_rnn_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With learning_rate = 0.001 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-be57861263d6cdb2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-be57861263d6cdb2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_5/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With learning_rate = 0.01 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f1c09e86ef9ed415\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f1c09e86ef9ed415\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6011;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_4/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vanilla RNN model - 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.440004). Check your callbacks.\n",
      "6835/6835 - 44s - loss: 1.6198 - accuracy: 0.2493 - val_loss: 1.5842 - val_accuracy: 0.2703\n",
      "Epoch 2/20\n",
      "6835/6835 - 41s - loss: 1.6034 - accuracy: 0.2559 - val_loss: 1.5649 - val_accuracy: 0.2709\n",
      "Epoch 3/20\n",
      "6835/6835 - 41s - loss: 1.5930 - accuracy: 0.2613 - val_loss: 1.6117 - val_accuracy: 0.1861\n",
      "Epoch 4/20\n",
      "6835/6835 - 41s - loss: 1.5893 - accuracy: 0.2673 - val_loss: 1.6076 - val_accuracy: 0.1732\n",
      "Epoch 5/20\n",
      "6835/6835 - 41s - loss: 1.5954 - accuracy: 0.2582 - val_loss: 1.5960 - val_accuracy: 0.1861\n",
      "Epoch 6/20\n",
      "6835/6835 - 41s - loss: 1.5898 - accuracy: 0.2614 - val_loss: 1.5620 - val_accuracy: 0.2727\n",
      "Epoch 7/20\n",
      "6835/6835 - 41s - loss: 1.5871 - accuracy: 0.2623 - val_loss: 1.5816 - val_accuracy: 0.2703\n",
      "Epoch 8/20\n",
      "6835/6835 - 41s - loss: 1.5912 - accuracy: 0.2667 - val_loss: 1.5778 - val_accuracy: 0.2709\n",
      "Epoch 9/20\n",
      "6835/6835 - 41s - loss: 1.5873 - accuracy: 0.2604 - val_loss: 1.5837 - val_accuracy: 0.2697\n",
      "Epoch 10/20\n",
      "6835/6835 - 41s - loss: 1.5902 - accuracy: 0.2632 - val_loss: 1.5902 - val_accuracy: 0.2715\n",
      "Epoch 11/20\n",
      "6835/6835 - 41s - loss: 1.5956 - accuracy: 0.2632 - val_loss: 1.5835 - val_accuracy: 0.2709\n",
      "Epoch 12/20\n",
      "6835/6835 - 41s - loss: 1.5853 - accuracy: 0.2647 - val_loss: 1.5678 - val_accuracy: 0.2709\n",
      "Epoch 13/20\n",
      "6835/6835 - 41s - loss: 1.5883 - accuracy: 0.2679 - val_loss: 1.5810 - val_accuracy: 0.2791\n",
      "Epoch 14/20\n",
      "6835/6835 - 41s - loss: 1.5834 - accuracy: 0.2562 - val_loss: 1.5687 - val_accuracy: 0.2850\n",
      "Epoch 15/20\n",
      "6835/6835 - 41s - loss: 1.5777 - accuracy: 0.2736 - val_loss: 1.5724 - val_accuracy: 0.2709\n",
      "Epoch 16/20\n",
      "6835/6835 - 41s - loss: 1.5853 - accuracy: 0.2657 - val_loss: 1.5850 - val_accuracy: 0.2159\n",
      "Epoch 17/20\n",
      "6835/6835 - 41s - loss: 1.5907 - accuracy: 0.2666 - val_loss: 1.5771 - val_accuracy: 0.2709\n",
      "Epoch 18/20\n",
      "6835/6835 - 41s - loss: 1.5803 - accuracy: 0.2657 - val_loss: 1.5687 - val_accuracy: 0.2861\n",
      "Epoch 19/20\n",
      "6835/6835 - 41s - loss: 1.5808 - accuracy: 0.2676 - val_loss: 1.6080 - val_accuracy: 0.1843\n",
      "Epoch 20/20\n",
      "6835/6835 - 41s - loss: 1.5847 - accuracy: 0.2672 - val_loss: 1.5827 - val_accuracy: 0.2499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d457adad0>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn_3 = tf.keras.Sequential()\n",
    "model_rnn_3.add(layers.Embedding(5000, 350, input_length=X_train.shape[1]))\n",
    "\n",
    "model_rnn_3.add(layers.SimpleRNN(350))\n",
    "model_rnn_3.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_rnn_3.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#model_rnn.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "log_dir_19=\"logs_19/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_19, histogram_freq=1)\n",
    "\n",
    "model_rnn_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1112 00:12:30.043277 140338601187136 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_19/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vanilla RNN model - 4  - Double Layer size ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.918587). Check your callbacks.\n",
      "6835/6835 - 53s - loss: 1.7610 - accuracy: 0.2136 - val_loss: 1.5712 - val_accuracy: 0.2657\n",
      "Epoch 2/20\n",
      "6835/6835 - 49s - loss: 1.6294 - accuracy: 0.2530 - val_loss: 1.5805 - val_accuracy: 0.2662\n",
      "Epoch 3/20\n",
      "6835/6835 - 49s - loss: 1.5864 - accuracy: 0.2591 - val_loss: 1.5716 - val_accuracy: 0.2662\n",
      "Epoch 4/20\n",
      "6835/6835 - 51s - loss: 1.5836 - accuracy: 0.2626 - val_loss: 1.5884 - val_accuracy: 0.2703\n",
      "Epoch 5/20\n",
      "6835/6835 - 50s - loss: 1.5904 - accuracy: 0.2634 - val_loss: 1.5725 - val_accuracy: 0.2662\n",
      "Epoch 6/20\n",
      "6835/6835 - 51s - loss: 1.5873 - accuracy: 0.2638 - val_loss: 1.5823 - val_accuracy: 0.2662\n",
      "Epoch 7/20\n",
      "6835/6835 - 49s - loss: 1.5872 - accuracy: 0.2617 - val_loss: 1.5919 - val_accuracy: 0.2703\n",
      "Epoch 8/20\n",
      "6835/6835 - 49s - loss: 1.5893 - accuracy: 0.2655 - val_loss: 1.5754 - val_accuracy: 0.2662\n",
      "Epoch 9/20\n",
      "6835/6835 - 49s - loss: 1.5906 - accuracy: 0.2616 - val_loss: 1.5897 - val_accuracy: 0.2662\n",
      "Epoch 10/20\n",
      "6835/6835 - 50s - loss: 1.5841 - accuracy: 0.2636 - val_loss: 1.5692 - val_accuracy: 0.2703\n",
      "Epoch 11/20\n",
      "6835/6835 - 49s - loss: 1.5837 - accuracy: 0.2597 - val_loss: 1.5797 - val_accuracy: 0.2662\n",
      "Epoch 12/20\n",
      "6835/6835 - 49s - loss: 1.5839 - accuracy: 0.2753 - val_loss: 1.5697 - val_accuracy: 0.2703\n",
      "Epoch 13/20\n",
      "6835/6835 - 49s - loss: 1.5925 - accuracy: 0.2676 - val_loss: 1.5914 - val_accuracy: 0.1861\n",
      "Epoch 14/20\n",
      "6835/6835 - 49s - loss: 1.5848 - accuracy: 0.2553 - val_loss: 1.5916 - val_accuracy: 0.2662\n",
      "Epoch 15/20\n",
      "6835/6835 - 49s - loss: 1.5901 - accuracy: 0.2688 - val_loss: 1.5811 - val_accuracy: 0.2703\n",
      "Epoch 16/20\n",
      "6835/6835 - 49s - loss: 1.5884 - accuracy: 0.2632 - val_loss: 1.6444 - val_accuracy: 0.2662\n",
      "Epoch 17/20\n",
      "6835/6835 - 50s - loss: 1.5871 - accuracy: 0.2641 - val_loss: 1.5696 - val_accuracy: 0.2703\n",
      "Epoch 18/20\n",
      "6835/6835 - 51s - loss: 1.5979 - accuracy: 0.2506 - val_loss: 1.5639 - val_accuracy: 0.2662\n",
      "Epoch 19/20\n",
      "6835/6835 - 51s - loss: 1.5939 - accuracy: 0.2528 - val_loss: 1.5707 - val_accuracy: 0.2703\n",
      "Epoch 20/20\n",
      "6835/6835 - 51s - loss: 1.5829 - accuracy: 0.2606 - val_loss: 1.5771 - val_accuracy: 0.2703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3db5fd2e50>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn_4 = tf.keras.Sequential()\n",
    "model_rnn_4.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))\n",
    "\n",
    "model_rnn_4.add(layers.SimpleRNN(256, return_sequences=True))\n",
    "\n",
    "model_rnn_4.add(layers.SimpleRNN(256))\n",
    "\n",
    "model_rnn_4.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_rnn_4.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#model_rnn.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "log_dir_14=\"logs_14/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_14, histogram_freq=1)\n",
    "\n",
    "model_rnn_4.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubling the layer of vanilla RNN does not improve validation accuracy ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1112 00:12:55.089560 140568162531136 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_14/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_18=\"logs_18/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_18, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(layers.Embedding(5000, 500, input_length=X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.add(layers.LSTM(500, dropout=0.5, recurrent_dropout=0.5))\n",
    "model_lstm.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 267, 500)          2500000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 267, 500)          0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 500)               2002000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 4,504,505\n",
      "Trainable params: 4,504,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/15\n",
      "6835/6835 - 276s - loss: 1.5758 - accuracy: 0.2742 - val_loss: 1.5610 - val_accuracy: 0.2756\n",
      "Epoch 2/15\n",
      "6835/6835 - 272s - loss: 1.5708 - accuracy: 0.2683 - val_loss: 1.5681 - val_accuracy: 0.2756\n",
      "Epoch 3/15\n",
      "6835/6835 - 277s - loss: 1.5636 - accuracy: 0.2907 - val_loss: 1.5733 - val_accuracy: 0.2733\n",
      "Epoch 4/15\n",
      "6835/6835 - 272s - loss: 1.5649 - accuracy: 0.2803 - val_loss: 1.5561 - val_accuracy: 0.2779\n",
      "Epoch 5/15\n",
      "6835/6835 - 271s - loss: 1.5614 - accuracy: 0.2806 - val_loss: 1.5636 - val_accuracy: 0.2750\n",
      "Epoch 6/15\n",
      "6835/6835 - 270s - loss: 1.5574 - accuracy: 0.2907 - val_loss: 1.5569 - val_accuracy: 0.2902\n",
      "Epoch 7/15\n",
      "6835/6835 - 271s - loss: 1.5593 - accuracy: 0.2910 - val_loss: 1.5576 - val_accuracy: 0.2762\n",
      "Epoch 8/15\n",
      "6835/6835 - 271s - loss: 1.5548 - accuracy: 0.2926 - val_loss: 1.5567 - val_accuracy: 0.2844\n",
      "Epoch 9/15\n",
      "6835/6835 - 272s - loss: 1.5549 - accuracy: 0.2854 - val_loss: 1.5529 - val_accuracy: 0.2978\n",
      "Epoch 10/15\n",
      "6835/6835 - 272s - loss: 1.5553 - accuracy: 0.2970 - val_loss: 1.5558 - val_accuracy: 0.2978\n",
      "Epoch 11/15\n",
      "6835/6835 - 271s - loss: 1.5503 - accuracy: 0.2977 - val_loss: 1.5567 - val_accuracy: 0.2867\n",
      "Epoch 12/15\n",
      "6835/6835 - 271s - loss: 1.5450 - accuracy: 0.2945 - val_loss: 1.5562 - val_accuracy: 0.2774\n",
      "Epoch 13/15\n",
      "6835/6835 - 271s - loss: 1.5451 - accuracy: 0.3001 - val_loss: 1.5622 - val_accuracy: 0.2756\n",
      "Epoch 14/15\n",
      "6835/6835 - 271s - loss: 1.5409 - accuracy: 0.3050 - val_loss: 1.5561 - val_accuracy: 0.3037\n",
      "Epoch 15/15\n",
      "6835/6835 - 274s - loss: 1.5380 - accuracy: 0.3078 - val_loss: 1.5553 - val_accuracy: 0.2908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3dbda0a610>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1112 00:13:29.759110 140098707699520 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_18/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210/1 - 29s - loss: 1.5446 - accuracy: 0.2588\n",
      "Test set Accuracy: 25.88%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_lstm.evaluate(test_X_t, test_y, verbose=2)\n",
    "print(\"Test set Accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210/1 - 9s - loss: 1.5518 - accuracy: 0.2683\n",
      "Test set Accuracy: 26.83%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_lstm.evaluate(test_X_t, test_y, verbose=2)\n",
    "print(\"Test set Accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1112 00:13:54.057028 139840611579712 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_17/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Tensorboard Visualizations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d71fc7b85adb0342\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d71fc7b85adb0342\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6013;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_2/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of LSTM model on dev subset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model_lstm.evaluate(dev_X_t, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2770209"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of LSTM on dev subset = 27.70% ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Model - 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_7=\"logs_7/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_7, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.927149). Check your callbacks.\n",
      "6835/6835 - 124s - loss: 1.5718 - accuracy: 0.2713 - val_loss: 1.5603 - val_accuracy: 0.2803\n",
      "Epoch 2/100\n",
      "6835/6835 - 119s - loss: 1.5649 - accuracy: 0.2809 - val_loss: 1.5618 - val_accuracy: 0.2879\n",
      "Epoch 3/100\n",
      "6835/6835 - 119s - loss: 1.5611 - accuracy: 0.2799 - val_loss: 1.5610 - val_accuracy: 0.2756\n",
      "Epoch 4/100\n",
      "6835/6835 - 119s - loss: 1.5587 - accuracy: 0.2909 - val_loss: 1.5599 - val_accuracy: 0.2815\n",
      "Epoch 5/100\n",
      "6835/6835 - 119s - loss: 1.5556 - accuracy: 0.2938 - val_loss: 1.5612 - val_accuracy: 0.2768\n",
      "Epoch 6/100\n",
      "6835/6835 - 120s - loss: 1.5554 - accuracy: 0.2857 - val_loss: 1.5584 - val_accuracy: 0.2756\n",
      "Epoch 7/100\n",
      "6835/6835 - 119s - loss: 1.5474 - accuracy: 0.2951 - val_loss: 1.5655 - val_accuracy: 0.2709\n",
      "Epoch 8/100\n",
      "6835/6835 - 119s - loss: 1.5454 - accuracy: 0.2973 - val_loss: 1.5561 - val_accuracy: 0.2692\n",
      "Epoch 9/100\n",
      "6835/6835 - 119s - loss: 1.5429 - accuracy: 0.3065 - val_loss: 1.5597 - val_accuracy: 0.2779\n",
      "Epoch 10/100\n",
      "6835/6835 - 119s - loss: 1.5360 - accuracy: 0.3090 - val_loss: 1.5652 - val_accuracy: 0.2850\n",
      "Epoch 11/100\n",
      "6835/6835 - 119s - loss: 1.5300 - accuracy: 0.3137 - val_loss: 1.5628 - val_accuracy: 0.2680\n",
      "Epoch 12/100\n",
      "6835/6835 - 119s - loss: 1.5204 - accuracy: 0.3198 - val_loss: 1.5774 - val_accuracy: 0.2785\n",
      "Epoch 13/100\n",
      "6835/6835 - 119s - loss: 1.5138 - accuracy: 0.3302 - val_loss: 1.5834 - val_accuracy: 0.2768\n",
      "Epoch 14/100\n",
      "6835/6835 - 119s - loss: 1.5001 - accuracy: 0.3355 - val_loss: 1.5824 - val_accuracy: 0.2785\n",
      "Epoch 15/100\n",
      "6835/6835 - 119s - loss: 1.4849 - accuracy: 0.3470 - val_loss: 1.5799 - val_accuracy: 0.2855\n",
      "Epoch 16/100\n",
      "6835/6835 - 119s - loss: 1.4676 - accuracy: 0.3532 - val_loss: 1.5934 - val_accuracy: 0.2703\n",
      "Epoch 17/100\n",
      "6835/6835 - 119s - loss: 1.4459 - accuracy: 0.3584 - val_loss: 1.6142 - val_accuracy: 0.2891\n",
      "Epoch 18/100\n",
      "6835/6835 - 119s - loss: 1.4230 - accuracy: 0.3810 - val_loss: 1.6097 - val_accuracy: 0.2820\n",
      "Epoch 19/100\n",
      "6835/6835 - 119s - loss: 1.3918 - accuracy: 0.3931 - val_loss: 1.6219 - val_accuracy: 0.2955\n",
      "Epoch 20/100\n",
      "6835/6835 - 119s - loss: 1.3603 - accuracy: 0.4149 - val_loss: 1.6706 - val_accuracy: 0.2762\n",
      "Epoch 21/100\n",
      "6835/6835 - 119s - loss: 1.3323 - accuracy: 0.4316 - val_loss: 1.6956 - val_accuracy: 0.2902\n",
      "Epoch 22/100\n",
      "6835/6835 - 119s - loss: 1.2845 - accuracy: 0.4636 - val_loss: 1.7701 - val_accuracy: 0.2692\n",
      "Epoch 23/100\n",
      "6835/6835 - 119s - loss: 1.2576 - accuracy: 0.4711 - val_loss: 1.7505 - val_accuracy: 0.2733\n",
      "Epoch 24/100\n",
      "6835/6835 - 119s - loss: 1.2159 - accuracy: 0.4941 - val_loss: 1.7974 - val_accuracy: 0.2703\n",
      "Epoch 25/100\n",
      "6835/6835 - 119s - loss: 1.1812 - accuracy: 0.5122 - val_loss: 1.7915 - val_accuracy: 0.2891\n",
      "Epoch 26/100\n",
      "6835/6835 - 119s - loss: 1.1332 - accuracy: 0.5378 - val_loss: 1.8437 - val_accuracy: 0.2586\n",
      "Epoch 27/100\n",
      "6835/6835 - 120s - loss: 1.1126 - accuracy: 0.5434 - val_loss: 1.8717 - val_accuracy: 0.2785\n",
      "Epoch 28/100\n",
      "6835/6835 - 119s - loss: 1.0735 - accuracy: 0.5634 - val_loss: 1.9005 - val_accuracy: 0.2727\n",
      "Epoch 29/100\n",
      "6835/6835 - 119s - loss: 1.0347 - accuracy: 0.5865 - val_loss: 1.9386 - val_accuracy: 0.2651\n",
      "Epoch 30/100\n",
      "6835/6835 - 119s - loss: 1.0153 - accuracy: 0.5860 - val_loss: 1.9537 - val_accuracy: 0.2809\n",
      "Epoch 31/100\n",
      "6835/6835 - 119s - loss: 0.9879 - accuracy: 0.6057 - val_loss: 2.0278 - val_accuracy: 0.2791\n",
      "Epoch 32/100\n",
      "6835/6835 - 119s - loss: 0.9463 - accuracy: 0.6195 - val_loss: 2.0512 - val_accuracy: 0.2932\n",
      "Epoch 33/100\n",
      "6835/6835 - 119s - loss: 0.9239 - accuracy: 0.6342 - val_loss: 2.1003 - val_accuracy: 0.2744\n",
      "Epoch 34/100\n",
      "6835/6835 - 119s - loss: 0.8997 - accuracy: 0.6508 - val_loss: 2.1515 - val_accuracy: 0.2657\n",
      "Epoch 35/100\n",
      "6835/6835 - 119s - loss: 0.8646 - accuracy: 0.6595 - val_loss: 2.1559 - val_accuracy: 0.2610\n",
      "Epoch 36/100\n",
      "6835/6835 - 119s - loss: 0.8509 - accuracy: 0.6631 - val_loss: 2.1898 - val_accuracy: 0.2715\n",
      "Epoch 37/100\n",
      "6835/6835 - 119s - loss: 0.8259 - accuracy: 0.6756 - val_loss: 2.2500 - val_accuracy: 0.2738\n",
      "Epoch 38/100\n",
      "6835/6835 - 119s - loss: 0.7927 - accuracy: 0.6951 - val_loss: 2.3116 - val_accuracy: 0.2815\n",
      "Epoch 39/100\n",
      "6835/6835 - 119s - loss: 0.7823 - accuracy: 0.6964 - val_loss: 2.2927 - val_accuracy: 0.2697\n",
      "Epoch 40/100\n",
      "6835/6835 - 119s - loss: 0.7584 - accuracy: 0.7027 - val_loss: 2.3527 - val_accuracy: 0.2633\n",
      "Epoch 41/100\n",
      "6835/6835 - 119s - loss: 0.7351 - accuracy: 0.7125 - val_loss: 2.3922 - val_accuracy: 0.2733\n",
      "Epoch 42/100\n",
      "6835/6835 - 119s - loss: 0.7170 - accuracy: 0.7229 - val_loss: 2.4076 - val_accuracy: 0.2715\n",
      "Epoch 43/100\n",
      "6835/6835 - 119s - loss: 0.6961 - accuracy: 0.7327 - val_loss: 2.5029 - val_accuracy: 0.2575\n",
      "Epoch 44/100\n",
      "6835/6835 - 119s - loss: 0.6886 - accuracy: 0.7311 - val_loss: 2.4682 - val_accuracy: 0.2809\n",
      "Epoch 45/100\n",
      "6835/6835 - 119s - loss: 0.6604 - accuracy: 0.7448 - val_loss: 2.5503 - val_accuracy: 0.2662\n",
      "Epoch 46/100\n",
      "6835/6835 - 119s - loss: 0.6524 - accuracy: 0.7542 - val_loss: 2.5718 - val_accuracy: 0.2639\n",
      "Epoch 47/100\n",
      "6835/6835 - 119s - loss: 0.6444 - accuracy: 0.7604 - val_loss: 2.5784 - val_accuracy: 0.2639\n",
      "Epoch 48/100\n",
      "6835/6835 - 119s - loss: 0.6165 - accuracy: 0.7677 - val_loss: 2.5845 - val_accuracy: 0.2586\n",
      "Epoch 49/100\n",
      "6835/6835 - 119s - loss: 0.6054 - accuracy: 0.7735 - val_loss: 2.6094 - val_accuracy: 0.2692\n",
      "Epoch 50/100\n",
      "6835/6835 - 119s - loss: 0.5895 - accuracy: 0.7762 - val_loss: 2.6812 - val_accuracy: 0.2680\n",
      "Epoch 51/100\n",
      "6835/6835 - 119s - loss: 0.5653 - accuracy: 0.7902 - val_loss: 2.7036 - val_accuracy: 0.2680\n",
      "Epoch 52/100\n",
      "6835/6835 - 119s - loss: 0.5705 - accuracy: 0.7860 - val_loss: 2.7497 - val_accuracy: 0.2709\n",
      "Epoch 53/100\n",
      "6835/6835 - 120s - loss: 0.5736 - accuracy: 0.7822 - val_loss: 2.7496 - val_accuracy: 0.2662\n",
      "Epoch 54/100\n",
      "6835/6835 - 119s - loss: 0.5461 - accuracy: 0.7955 - val_loss: 2.7591 - val_accuracy: 0.2674\n",
      "Epoch 55/100\n",
      "6835/6835 - 119s - loss: 0.5382 - accuracy: 0.7977 - val_loss: 2.7760 - val_accuracy: 0.2633\n",
      "Epoch 56/100\n",
      "6835/6835 - 119s - loss: 0.5529 - accuracy: 0.7921 - val_loss: 2.7652 - val_accuracy: 0.2797\n",
      "Epoch 57/100\n",
      "6835/6835 - 119s - loss: 0.5186 - accuracy: 0.8111 - val_loss: 2.8108 - val_accuracy: 0.2686\n",
      "Epoch 58/100\n",
      "6835/6835 - 119s - loss: 0.5127 - accuracy: 0.8083 - val_loss: 2.8163 - val_accuracy: 0.2733\n",
      "Epoch 59/100\n",
      "6835/6835 - 119s - loss: 0.5059 - accuracy: 0.8154 - val_loss: 2.8741 - val_accuracy: 0.2680\n",
      "Epoch 60/100\n",
      "6835/6835 - 119s - loss: 0.5070 - accuracy: 0.8133 - val_loss: 2.8376 - val_accuracy: 0.2627\n",
      "Epoch 61/100\n",
      "6835/6835 - 119s - loss: 0.5089 - accuracy: 0.8151 - val_loss: 2.9131 - val_accuracy: 0.2651\n",
      "Epoch 62/100\n",
      "6835/6835 - 119s - loss: 0.4862 - accuracy: 0.8206 - val_loss: 2.9558 - val_accuracy: 0.2686\n",
      "Epoch 63/100\n",
      "6835/6835 - 119s - loss: 0.4838 - accuracy: 0.8218 - val_loss: 2.9535 - val_accuracy: 0.2633\n",
      "Epoch 64/100\n",
      "6835/6835 - 119s - loss: 0.4627 - accuracy: 0.8347 - val_loss: 2.9746 - val_accuracy: 0.2639\n",
      "Epoch 65/100\n",
      "6835/6835 - 119s - loss: 0.4666 - accuracy: 0.8279 - val_loss: 2.9840 - val_accuracy: 0.2721\n",
      "Epoch 66/100\n",
      "6835/6835 - 119s - loss: 0.4763 - accuracy: 0.8195 - val_loss: 2.9958 - val_accuracy: 0.2709\n",
      "Epoch 67/100\n",
      "6835/6835 - 119s - loss: 0.4469 - accuracy: 0.8332 - val_loss: 3.0122 - val_accuracy: 0.2709\n",
      "Epoch 68/100\n",
      "6835/6835 - 119s - loss: 0.4542 - accuracy: 0.8317 - val_loss: 3.0196 - val_accuracy: 0.2820\n",
      "Epoch 69/100\n",
      "6835/6835 - 119s - loss: 0.4628 - accuracy: 0.8271 - val_loss: 3.0025 - val_accuracy: 0.2738\n",
      "Epoch 70/100\n",
      "6835/6835 - 119s - loss: 0.4509 - accuracy: 0.8313 - val_loss: 2.9931 - val_accuracy: 0.2738\n",
      "Epoch 71/100\n",
      "6835/6835 - 119s - loss: 0.4511 - accuracy: 0.8348 - val_loss: 3.0922 - val_accuracy: 0.2692\n",
      "Epoch 72/100\n",
      "6835/6835 - 120s - loss: 0.4326 - accuracy: 0.8396 - val_loss: 3.0772 - val_accuracy: 0.2692\n",
      "Epoch 73/100\n",
      "6835/6835 - 119s - loss: 0.4333 - accuracy: 0.8418 - val_loss: 3.1165 - val_accuracy: 0.2674\n",
      "Epoch 74/100\n",
      "6835/6835 - 119s - loss: 0.4180 - accuracy: 0.8486 - val_loss: 3.1604 - val_accuracy: 0.2616\n",
      "Epoch 75/100\n",
      "6835/6835 - 119s - loss: 0.4334 - accuracy: 0.8386 - val_loss: 3.1410 - val_accuracy: 0.2598\n",
      "Epoch 76/100\n",
      "6835/6835 - 119s - loss: 0.4199 - accuracy: 0.8458 - val_loss: 3.1195 - val_accuracy: 0.2621\n",
      "Epoch 77/100\n",
      "6835/6835 - 119s - loss: 0.4160 - accuracy: 0.8496 - val_loss: 3.1642 - val_accuracy: 0.2697\n",
      "Epoch 78/100\n",
      "6835/6835 - 119s - loss: 0.3980 - accuracy: 0.8515 - val_loss: 3.1780 - val_accuracy: 0.2850\n",
      "Epoch 79/100\n",
      "6835/6835 - 119s - loss: 0.4147 - accuracy: 0.8495 - val_loss: 3.1549 - val_accuracy: 0.2756\n",
      "Epoch 80/100\n",
      "6835/6835 - 119s - loss: 0.4041 - accuracy: 0.8519 - val_loss: 3.1509 - val_accuracy: 0.2733\n",
      "Epoch 81/100\n",
      "6835/6835 - 119s - loss: 0.4105 - accuracy: 0.8490 - val_loss: 3.1091 - val_accuracy: 0.2738\n",
      "Epoch 82/100\n",
      "6835/6835 - 119s - loss: 0.4000 - accuracy: 0.8535 - val_loss: 3.1670 - val_accuracy: 0.2616\n",
      "Epoch 83/100\n",
      "6835/6835 - 119s - loss: 0.3928 - accuracy: 0.8535 - val_loss: 3.2017 - val_accuracy: 0.2697\n",
      "Epoch 84/100\n",
      "6835/6835 - 119s - loss: 0.3802 - accuracy: 0.8585 - val_loss: 3.2146 - val_accuracy: 0.2668\n",
      "Epoch 85/100\n",
      "6835/6835 - 119s - loss: 0.3788 - accuracy: 0.8657 - val_loss: 3.1977 - val_accuracy: 0.2686\n",
      "Epoch 86/100\n",
      "6835/6835 - 119s - loss: 0.3921 - accuracy: 0.8550 - val_loss: 3.1868 - val_accuracy: 0.2738\n",
      "Epoch 87/100\n",
      "6835/6835 - 119s - loss: 0.3945 - accuracy: 0.8515 - val_loss: 3.2458 - val_accuracy: 0.2774\n",
      "Epoch 88/100\n",
      "6835/6835 - 119s - loss: 0.3892 - accuracy: 0.8616 - val_loss: 3.2510 - val_accuracy: 0.2680\n",
      "Epoch 89/100\n",
      "6835/6835 - 119s - loss: 0.3782 - accuracy: 0.8600 - val_loss: 3.2285 - val_accuracy: 0.2534\n",
      "Epoch 90/100\n",
      "6835/6835 - 119s - loss: 0.3708 - accuracy: 0.8664 - val_loss: 3.2930 - val_accuracy: 0.2534\n",
      "Epoch 91/100\n",
      "6835/6835 - 119s - loss: 0.3959 - accuracy: 0.8511 - val_loss: 3.2636 - val_accuracy: 0.2522\n",
      "Epoch 92/100\n",
      "6835/6835 - 119s - loss: 0.3702 - accuracy: 0.8679 - val_loss: 3.2594 - val_accuracy: 0.2762\n",
      "Epoch 93/100\n",
      "6835/6835 - 119s - loss: 0.3560 - accuracy: 0.8740 - val_loss: 3.3404 - val_accuracy: 0.2721\n",
      "Epoch 94/100\n",
      "6835/6835 - 119s - loss: 0.3616 - accuracy: 0.8691 - val_loss: 3.3122 - val_accuracy: 0.2668\n",
      "Epoch 95/100\n",
      "6835/6835 - 119s - loss: 0.3799 - accuracy: 0.8622 - val_loss: 3.2925 - val_accuracy: 0.2621\n",
      "Epoch 96/100\n",
      "6835/6835 - 119s - loss: 0.3522 - accuracy: 0.8749 - val_loss: 3.3844 - val_accuracy: 0.2674\n",
      "Epoch 97/100\n",
      "6835/6835 - 119s - loss: 0.3498 - accuracy: 0.8721 - val_loss: 3.4072 - val_accuracy: 0.2651\n",
      "Epoch 98/100\n",
      "6835/6835 - 119s - loss: 0.3438 - accuracy: 0.8729 - val_loss: 3.3746 - val_accuracy: 0.2692\n",
      "Epoch 99/100\n",
      "6835/6835 - 119s - loss: 0.3511 - accuracy: 0.8756 - val_loss: 3.4064 - val_accuracy: 0.2534\n",
      "Epoch 100/100\n",
      "6835/6835 - 119s - loss: 0.3461 - accuracy: 0.8767 - val_loss: 3.3585 - val_accuracy: 0.2627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f15068b7390>"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_2 = tf.keras.Sequential()\n",
    "model_lstm_2.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "model_lstm_2.add(layers.Dropout(0.3))\n",
    "\n",
    "model_lstm_2.add(layers.LSTM(256, dropout=0.3, recurrent_dropout=0.2))\n",
    "model_lstm_2.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model_lstm_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#model_lstm.summary()\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "model_lstm_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d2ea8c765697d613\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d2ea8c765697d613\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6014;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_7/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubling LSTM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.967493). Check your callbacks.\n",
      "6835/6835 - 247s - loss: 1.5763 - accuracy: 0.2598 - val_loss: 1.5657 - val_accuracy: 0.2668\n",
      "Epoch 2/10\n",
      "6835/6835 - 237s - loss: 1.5696 - accuracy: 0.2723 - val_loss: 1.5665 - val_accuracy: 0.2873\n",
      "Epoch 3/10\n",
      "6835/6835 - 237s - loss: 1.5683 - accuracy: 0.2783 - val_loss: 1.5698 - val_accuracy: 0.2715\n",
      "Epoch 4/10\n",
      "6835/6835 - 235s - loss: 1.5728 - accuracy: 0.2695 - val_loss: 1.5652 - val_accuracy: 0.2662\n",
      "Epoch 5/10\n",
      "6835/6835 - 235s - loss: 1.5691 - accuracy: 0.2699 - val_loss: 1.5630 - val_accuracy: 0.2697\n",
      "Epoch 6/10\n",
      "6835/6835 - 239s - loss: 1.5677 - accuracy: 0.2742 - val_loss: 1.5570 - val_accuracy: 0.2744\n",
      "Epoch 7/10\n",
      "6835/6835 - 236s - loss: 1.5667 - accuracy: 0.2740 - val_loss: 1.5583 - val_accuracy: 0.2727\n",
      "Epoch 8/10\n",
      "6835/6835 - 235s - loss: 1.5659 - accuracy: 0.2794 - val_loss: 1.5599 - val_accuracy: 0.2727\n",
      "Epoch 9/10\n",
      "6835/6835 - 235s - loss: 1.5654 - accuracy: 0.2780 - val_loss: 1.5584 - val_accuracy: 0.2733\n",
      "Epoch 10/10\n",
      "6835/6835 - 235s - loss: 1.5660 - accuracy: 0.2727 - val_loss: 1.5593 - val_accuracy: 0.2727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3f2c9c76d0>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_3 = tf.keras.Sequential()\n",
    "model_lstm_3.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "model_lstm_3.add(layers.Dropout(0.3))\n",
    "\n",
    "model_lstm_3.add(layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "model_lstm_3.add(layers.LSTM(256, dropout=0.3, recurrent_dropout=0.2))\n",
    "\n",
    "model_lstm_3.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model_lstm_3.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#model_lstm.summary()\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "log_dir_15=\"logs_15/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_15, histogram_freq=1)\n",
    "\n",
    "model_lstm_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubling LSTM layer does not improve validation accuracy ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1112 00:15:39.989207 140272832292672 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_15/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GRU model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = tf.keras.Sequential()\n",
    "model_gru.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.add(layers.GRU(256))\n",
    "model_gru.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 267, 256)          1280000   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,676,037\n",
      "Trainable params: 1,676,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_3=\"logs_3/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_3, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.882006). Check your callbacks.\n",
      "6835/6835 - 72s - loss: 1.5719 - accuracy: 0.2692 - val_loss: 1.5626 - val_accuracy: 0.2779\n",
      "Epoch 2/20\n",
      "6835/6835 - 68s - loss: 1.5653 - accuracy: 0.2742 - val_loss: 1.5680 - val_accuracy: 0.2727\n",
      "Epoch 3/20\n",
      "6835/6835 - 69s - loss: 1.5596 - accuracy: 0.2830 - val_loss: 1.5621 - val_accuracy: 0.2686\n",
      "Epoch 4/20\n",
      "6835/6835 - 69s - loss: 1.5551 - accuracy: 0.2939 - val_loss: 1.5637 - val_accuracy: 0.2744\n",
      "Epoch 5/20\n",
      "6835/6835 - 68s - loss: 1.5519 - accuracy: 0.2951 - val_loss: 1.5624 - val_accuracy: 0.2774\n",
      "Epoch 6/20\n",
      "6835/6835 - 68s - loss: 1.5481 - accuracy: 0.2985 - val_loss: 1.5650 - val_accuracy: 0.2762\n",
      "Epoch 7/20\n",
      "6835/6835 - 68s - loss: 1.5442 - accuracy: 0.3023 - val_loss: 1.5559 - val_accuracy: 0.2873\n",
      "Epoch 8/20\n",
      "6835/6835 - 68s - loss: 1.5398 - accuracy: 0.2999 - val_loss: 1.5626 - val_accuracy: 0.2978\n",
      "Epoch 9/20\n",
      "6835/6835 - 70s - loss: 1.5340 - accuracy: 0.3138 - val_loss: 1.5642 - val_accuracy: 0.2896\n",
      "Epoch 10/20\n",
      "6835/6835 - 68s - loss: 1.5210 - accuracy: 0.3108 - val_loss: 1.5613 - val_accuracy: 0.2972\n",
      "Epoch 11/20\n",
      "6835/6835 - 68s - loss: 1.5047 - accuracy: 0.3230 - val_loss: 1.5706 - val_accuracy: 0.2733\n",
      "Epoch 12/20\n",
      "6835/6835 - 68s - loss: 1.4695 - accuracy: 0.3501 - val_loss: 1.5598 - val_accuracy: 0.2949\n",
      "Epoch 13/20\n",
      "6835/6835 - 68s - loss: 1.4112 - accuracy: 0.3914 - val_loss: 1.5836 - val_accuracy: 0.2955\n",
      "Epoch 14/20\n",
      "6835/6835 - 68s - loss: 1.3208 - accuracy: 0.4388 - val_loss: 1.6476 - val_accuracy: 0.2815\n",
      "Epoch 15/20\n",
      "6835/6835 - 68s - loss: 1.1865 - accuracy: 0.4976 - val_loss: 1.7373 - val_accuracy: 0.2785\n",
      "Epoch 16/20\n",
      "6835/6835 - 68s - loss: 1.0149 - accuracy: 0.5890 - val_loss: 1.8808 - val_accuracy: 0.2955\n",
      "Epoch 17/20\n",
      "6835/6835 - 68s - loss: 0.8169 - accuracy: 0.6825 - val_loss: 2.0533 - val_accuracy: 0.3084\n",
      "Epoch 18/20\n",
      "6835/6835 - 68s - loss: 0.6009 - accuracy: 0.7760 - val_loss: 2.4067 - val_accuracy: 0.2850\n",
      "Epoch 19/20\n",
      "6835/6835 - 68s - loss: 0.4321 - accuracy: 0.8505 - val_loss: 2.6931 - val_accuracy: 0.2943\n",
      "Epoch 20/20\n",
      "6835/6835 - 69s - loss: 0.3065 - accuracy: 0.9002 - val_loss: 3.0015 - val_accuracy: 0.2932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d54e10250>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Tensorboard Visualizations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6015 (pid 20542), started 2 days, 15:33:50 ago. (Use '!kill 20542' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dee0a0c6b94fa43a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dee0a0c6b94fa43a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6015;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_3/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of GRU model on dev subset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model_gru.evaluate(dev_X_t, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25613078"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of GRU model on dev subset = 25.61% ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GRU model - 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 267, 256)          1280000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 267, 256)          0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,676,037\n",
      "Trainable params: 1,676,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.891410). Check your callbacks.\n",
      "6835/6835 - 74s - loss: 1.5734 - accuracy: 0.2658 - val_loss: 1.5675 - val_accuracy: 0.2744\n",
      "Epoch 2/10\n",
      "6835/6835 - 70s - loss: 1.5670 - accuracy: 0.2759 - val_loss: 1.5578 - val_accuracy: 0.2703\n",
      "Epoch 3/10\n",
      "6835/6835 - 70s - loss: 1.5620 - accuracy: 0.2811 - val_loss: 1.5564 - val_accuracy: 0.2896\n",
      "Epoch 4/10\n",
      "6835/6835 - 69s - loss: 1.5583 - accuracy: 0.2878 - val_loss: 1.5621 - val_accuracy: 0.2826\n",
      "Epoch 5/10\n",
      "6835/6835 - 70s - loss: 1.5564 - accuracy: 0.2876 - val_loss: 1.5599 - val_accuracy: 0.2844\n",
      "Epoch 6/10\n",
      "6835/6835 - 70s - loss: 1.5531 - accuracy: 0.2887 - val_loss: 1.5581 - val_accuracy: 0.2861\n",
      "Epoch 7/10\n",
      "6835/6835 - 70s - loss: 1.5491 - accuracy: 0.3031 - val_loss: 1.5573 - val_accuracy: 0.2756\n",
      "Epoch 8/10\n",
      "6835/6835 - 70s - loss: 1.5436 - accuracy: 0.3045 - val_loss: 1.5545 - val_accuracy: 0.2891\n",
      "Epoch 9/10\n",
      "6835/6835 - 70s - loss: 1.5370 - accuracy: 0.3058 - val_loss: 1.5649 - val_accuracy: 0.2779\n",
      "Epoch 10/10\n",
      "6835/6835 - 70s - loss: 1.5311 - accuracy: 0.3154 - val_loss: 1.5549 - val_accuracy: 0.2932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1389b57410>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru_2 = tf.keras.Sequential()\n",
    "model_gru_2.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))\n",
    "\n",
    "\n",
    "model_gru_2.add(layers.Dropout(0.3))\n",
    "\n",
    "model_gru_2.add(layers.GRU(256))\n",
    "\n",
    "\n",
    "model_gru_2.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "model_gru_2.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model_gru_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru_2.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "log_dir_9=\"logs_9/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_9, histogram_freq=1)\n",
    "\n",
    "\n",
    "model_gru_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1112 00:16:19.276887 140082655811392 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_9/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubling GRU layer model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.201706). Check your callbacks.\n",
      "8544/8544 - 160s - loss: 1.5742 - accuracy: 0.2697 - val_loss: 1.5765 - val_accuracy: 0.2543\n",
      "Epoch 2/10\n",
      "8544/8544 - 152s - loss: 1.5666 - accuracy: 0.2796 - val_loss: 1.5808 - val_accuracy: 0.2498\n",
      "Epoch 3/10\n",
      "8544/8544 - 152s - loss: 1.5653 - accuracy: 0.2773 - val_loss: 1.5706 - val_accuracy: 0.2625\n",
      "Epoch 4/10\n",
      "8544/8544 - 152s - loss: 1.5623 - accuracy: 0.2811 - val_loss: 1.5721 - val_accuracy: 0.2725\n",
      "Epoch 5/10\n",
      "8544/8544 - 152s - loss: 1.5599 - accuracy: 0.2786 - val_loss: 1.5771 - val_accuracy: 0.2480\n",
      "Epoch 6/10\n",
      "8544/8544 - 152s - loss: 1.5571 - accuracy: 0.2933 - val_loss: 1.5853 - val_accuracy: 0.2543\n",
      "Epoch 7/10\n",
      "8544/8544 - 152s - loss: 1.5530 - accuracy: 0.2921 - val_loss: 1.5823 - val_accuracy: 0.2707\n",
      "Epoch 8/10\n",
      "8544/8544 - 152s - loss: 1.5515 - accuracy: 0.2949 - val_loss: 1.5787 - val_accuracy: 0.2834\n",
      "Epoch 9/10\n",
      "8544/8544 - 152s - loss: 1.5456 - accuracy: 0.2990 - val_loss: 1.5703 - val_accuracy: 0.2570\n",
      "Epoch 10/10\n",
      "8544/8544 - 153s - loss: 1.5422 - accuracy: 0.2989 - val_loss: 1.5719 - val_accuracy: 0.2725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3dbd302090>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru_3 = tf.keras.Sequential()\n",
    "model_gru_3.add(layers.Embedding(5000, 256, input_length=X_train.shape[1]))\n",
    "\n",
    "\n",
    "model_gru_3.add(layers.Dropout(0.3))\n",
    "\n",
    "model_gru_3.add(layers.GRU(256, return_sequences=True))\n",
    "\n",
    "model_gru_3.add(layers.GRU(256))\n",
    "\n",
    "\n",
    "model_gru_3.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "model_gru_3.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model_gru_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model_gru_3.summary()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "log_dir_16=\"logs_16/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_16, histogram_freq=1)\n",
    "\n",
    "\n",
    "#model_gru_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])\n",
    "\n",
    "model_gru_3.fit(X_t, y_t, validation_data=(dev_X_t, dev_y), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubling GRU model layer does not improve validation accuracy ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1110 00:51:09.285538 139798023501632 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_16/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting - with dropout=0.3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1109 08:42:37.262158 140054096025408 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_8/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6835, 267)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDIRECTIONAL RNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 267, 10)           50000     \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 267, 20)           1680      \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 105       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 54,265\n",
      "Trainable params: 54,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bid = tf.keras.Sequential()\n",
    "\n",
    "model_bid.add(layers.Embedding(5000, 10, input_length=X_train.shape[1]))\n",
    "\n",
    "\n",
    "model_bid.add(layers.Bidirectional(layers.LSTM(10, return_sequences=True)))\n",
    "\n",
    "#model_bid.add(layers.Bidirectional(layers.LSTM(10, return_sequences=True), input_shape=(6835, 267)))\n",
    "\n",
    "\n",
    "\n",
    "model_bid.add(layers.Bidirectional(layers.LSTM(10)))\n",
    "model_bid.add(layers.Dense(5))\n",
    "model_bid.add(layers.Activation('softmax'))\n",
    "model_bid.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model_bid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6835 samples, validate on 1709 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.036925). Check your callbacks.\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.32963, saving model to training_2/cp.ckpt\n",
      "6835/6835 - 46s - loss: 1.5152 - accuracy: 0.3296 - val_loss: 1.5677 - val_accuracy: 0.2867\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: accuracy did not improve from 0.32963\n",
      "6835/6835 - 38s - loss: 1.5167 - accuracy: 0.3251 - val_loss: 1.5604 - val_accuracy: 0.2937\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: accuracy did not improve from 0.32963\n",
      "6835/6835 - 38s - loss: 1.5152 - accuracy: 0.3293 - val_loss: 1.5642 - val_accuracy: 0.2932\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.32963 to 0.33021, saving model to training_2/cp.ckpt\n",
      "6835/6835 - 38s - loss: 1.5127 - accuracy: 0.3302 - val_loss: 1.5741 - val_accuracy: 0.2733\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.33021 to 0.33270, saving model to training_2/cp.ckpt\n",
      "6835/6835 - 38s - loss: 1.5113 - accuracy: 0.3327 - val_loss: 1.5642 - val_accuracy: 0.2803\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: accuracy improved from 0.33270 to 0.33899, saving model to training_2/cp.ckpt\n",
      "6835/6835 - 38s - loss: 1.5082 - accuracy: 0.3390 - val_loss: 1.5851 - val_accuracy: 0.2744\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: accuracy did not improve from 0.33899\n",
      "6835/6835 - 38s - loss: 1.5074 - accuracy: 0.3386 - val_loss: 1.5648 - val_accuracy: 0.2844\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: accuracy did not improve from 0.33899\n",
      "6835/6835 - 38s - loss: 1.5098 - accuracy: 0.3302 - val_loss: 1.5804 - val_accuracy: 0.2885\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.33899\n",
      "6835/6835 - 38s - loss: 1.5092 - accuracy: 0.3333 - val_loss: 1.5714 - val_accuracy: 0.2744\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: accuracy did not improve from 0.33899\n",
      "6835/6835 - 38s - loss: 1.5120 - accuracy: 0.3260 - val_loss: 1.5682 - val_accuracy: 0.2750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3eef9b9910>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "log_dir_13=\"logs_13/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_13, histogram_freq=1)\n",
    "\n",
    "# define checkpoint\n",
    "checkpoint_path = \"training_2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='accuracy', save_best_only=True, mode='max', save_weights_only=True, verbose=1)\n",
    "\n",
    "\n",
    "model_bid.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard_callback, cp_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210/1 - 2s - loss: 1.7128 - accuracy: 0.2986\n",
      "Test set Accuracy: 29.86%\n"
     ]
    }
   ],
   "source": [
    "# Loads the weights\n",
    "model_bid.load_weights(checkpoint_path)\n",
    "\n",
    "loss, acc = model_bid.evaluate(test_X_t, test_y, verbose=2)\n",
    "print(\"Test set Accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-48cb282a731b4844\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-48cb282a731b4844\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_13/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation on test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights\n",
    "model_bid.load_weights(checkpoint_path)\n",
    "\n",
    "loss, acc = model_bid.evaluate(test_X_t, test_y, verbose=2)\n",
    "print(\"Test set Accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set Accuracy: 29.86%  - Using Bidirectional LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dd3b3e10017a9f24\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dd3b3e10017a9f24\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_12/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with 20 epochs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fec0a2483dad6c37\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fec0a2483dad6c37\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_11/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional - with 10 epochs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "E1109 08:42:54.047511 140117797828416 program.py:226] TensorBoard could not bind to any port around 6006 (tried 10 times)\n",
       "ERROR: TensorBoard could not bind to any port around 6006 (tried 10 times)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_10/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Evaluate best model on test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model_lstm.evaluate(test_X_t, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28597286"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the best model on test data = 28.60% ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 7s 3ms/sample - loss: 2.3867 - accuracy: 0.2769\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_gru.evaluate(test_X_t, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 1ms/sample - loss: 1.7375 - accuracy: 0.2860\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_rnn.evaluate(test_X_t, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best LSTM model on the test set ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lstm = np.argmax(model_lstm.predict(test_X_t), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, ..., 3, 3, 1])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 3, 3, 4, 4, 1, 1])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lstm[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2210"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0]\n",
      " [  1  93   6 173   6   0]\n",
      " [  0 177  14 432  10   0]\n",
      " [  0 114  11 259   5   0]\n",
      " [  0 117   6 374  13   0]\n",
      " [  2  81   5 302   9   0]]\n"
     ]
    }
   ],
   "source": [
    "cm_lstm = metrics.confusion_matrix(test['truth'].values, predictions_lstm)\n",
    "print(cm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.16      0.33      0.22       279\n",
      "           2       0.33      0.02      0.04       633\n",
      "           3       0.17      0.67      0.27       389\n",
      "           4       0.30      0.03      0.05       510\n",
      "           5       0.00      0.00      0.00       399\n",
      "\n",
      "    accuracy                           0.17      2210\n",
      "   macro avg       0.16      0.17      0.10      2210\n",
      "weighted avg       0.22      0.17      0.10      2210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demolakstate/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/demolakstate/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['truth'].values, predictions_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best GRU model on the test set ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0]\n",
      " [ 37  88  53  76  25   0]\n",
      " [ 79 198 126 181  49   0]\n",
      " [ 38  88  87 142  34   0]\n",
      " [ 38  87  89 205  91   0]\n",
      " [ 20  45  55 183  96   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      0.32      0.22       279\n",
      "           2       0.31      0.20      0.24       633\n",
      "           3       0.18      0.37      0.24       389\n",
      "           4       0.31      0.18      0.23       510\n",
      "           5       0.00      0.00      0.00       399\n",
      "\n",
      "    accuracy                           0.20      2210\n",
      "   macro avg       0.16      0.18      0.16      2210\n",
      "weighted avg       0.21      0.20      0.19      2210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demolakstate/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/demolakstate/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_gru = np.argmax(model_gru.predict(test_X_t), axis=1)\n",
    "\n",
    "predictions_gru\n",
    "\n",
    "cm_gru = metrics.confusion_matrix(test['truth'].values, predictions_gru)\n",
    "print(cm_gru)\n",
    "\n",
    "print(classification_report(test['truth'].values, predictions_gru))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best RNN model on the test set ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9   2 184  84   0]\n",
      " [ 12   6 431 184   0]\n",
      " [ 17   7 268  97   0]\n",
      " [  6   3 349 152   0]\n",
      " [  0   4 282 113   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.20      0.03      0.06       279\n",
      "           2       0.27      0.01      0.02       633\n",
      "           3       0.18      0.69      0.28       389\n",
      "           4       0.24      0.30      0.27       510\n",
      "           5       0.00      0.00      0.00       399\n",
      "\n",
      "    accuracy                           0.20      2210\n",
      "   macro avg       0.18      0.21      0.12      2210\n",
      "weighted avg       0.19      0.20      0.12      2210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demolakstate/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_rnn = np.argmax(model_rnn_3.predict(test_X_t), axis=1)\n",
    "\n",
    "predictions_rnn\n",
    "\n",
    "cm_rnn = metrics.confusion_matrix(test['truth'].values, predictions_rnn)\n",
    "print(cm_rnn)\n",
    "\n",
    "print(classification_report(test['truth'].values, predictions_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the different models performance ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM had the highest accuracy of 29.86% on the test data. This is followed by LSTM and RNN both with 28.60% and finally GRU model of 27.69%. MODEL_RNN_3 did well in predicting sentences of label 3 - \"Neutral\" with an F-Score of 28%. The model did badly on all other classes, and most especially on label 5. MODEL_GRU did well on labels 3 and 1 with F-Score of 24% and 22% respectively, but badly on the others and especially on label 5. MODEL_LSTM did well on label 3, but badly on all others and especially label 5. In conclusion, all the experiemented model architectures were able to do a good job in classifying sentence sentiments that are \"Neutral\". They all struggled with \"Strongly Positive\"  sentiments. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Text Generation Language Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       the rock is destined to be the 21st century 's...\n",
       "1       the gorgeously elaborate continuation of `` th...\n",
       "2       singer/composer bryan adams contributes a slew...\n",
       "3       you 'd think by now america would have had eno...\n",
       "4                    yet the act is still charming here .\n",
       "                              ...                        \n",
       "8539                                      a real snooze .\n",
       "8540                                       no surprises .\n",
       "8541    we 've seen the hippie-turned-yuppie plot befo...\n",
       "8542    her fans walked out muttering words like `` ho...\n",
       "8543                                  in this case zero .\n",
       "Name: text, Length: 8544, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8544\n",
      "Total Vocab:  8534\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  8444\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "    \n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0646\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.06460, saving model to Weights-LSTM-improvement-01-9.0646-bigger.hdfs\n",
      "Epoch 2/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0822\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.06460\n",
      "Epoch 3/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0538\n",
      "\n",
      "Epoch 00003: loss improved from 9.06460 to 9.05378, saving model to Weights-LSTM-improvement-03-9.0538-bigger.hdfs\n",
      "Epoch 4/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0529\n",
      "\n",
      "Epoch 00004: loss improved from 9.05378 to 9.05290, saving model to Weights-LSTM-improvement-04-9.0529-bigger.hdfs\n",
      "Epoch 5/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0522\n",
      "\n",
      "Epoch 00005: loss improved from 9.05290 to 9.05219, saving model to Weights-LSTM-improvement-05-9.0522-bigger.hdfs\n",
      "Epoch 6/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0516\n",
      "\n",
      "Epoch 00006: loss improved from 9.05219 to 9.05157, saving model to Weights-LSTM-improvement-06-9.0516-bigger.hdfs\n",
      "Epoch 7/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0510\n",
      "\n",
      "Epoch 00007: loss improved from 9.05157 to 9.05103, saving model to Weights-LSTM-improvement-07-9.0510-bigger.hdfs\n",
      "Epoch 8/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0505\n",
      "\n",
      "Epoch 00008: loss improved from 9.05103 to 9.05053, saving model to Weights-LSTM-improvement-08-9.0505-bigger.hdfs\n",
      "Epoch 9/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0501\n",
      "\n",
      "Epoch 00009: loss improved from 9.05053 to 9.05008, saving model to Weights-LSTM-improvement-09-9.0501-bigger.hdfs\n",
      "Epoch 10/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0497\n",
      "\n",
      "Epoch 00010: loss improved from 9.05008 to 9.04968, saving model to Weights-LSTM-improvement-10-9.0497-bigger.hdfs\n",
      "Epoch 11/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0494\n",
      "\n",
      "Epoch 00011: loss improved from 9.04968 to 9.04937, saving model to Weights-LSTM-improvement-11-9.0494-bigger.hdfs\n",
      "Epoch 12/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0490\n",
      "\n",
      "Epoch 00012: loss improved from 9.04937 to 9.04900, saving model to Weights-LSTM-improvement-12-9.0490-bigger.hdfs\n",
      "Epoch 13/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0488\n",
      "\n",
      "Epoch 00013: loss improved from 9.04900 to 9.04877, saving model to Weights-LSTM-improvement-13-9.0488-bigger.hdfs\n",
      "Epoch 14/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0485\n",
      "\n",
      "Epoch 00014: loss improved from 9.04877 to 9.04846, saving model to Weights-LSTM-improvement-14-9.0485-bigger.hdfs\n",
      "Epoch 15/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0482\n",
      "\n",
      "Epoch 00015: loss improved from 9.04846 to 9.04821, saving model to Weights-LSTM-improvement-15-9.0482-bigger.hdfs\n",
      "Epoch 16/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0480\n",
      "\n",
      "Epoch 00016: loss improved from 9.04821 to 9.04799, saving model to Weights-LSTM-improvement-16-9.0480-bigger.hdfs\n",
      "Epoch 17/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0478\n",
      "\n",
      "Epoch 00017: loss improved from 9.04799 to 9.04778, saving model to Weights-LSTM-improvement-17-9.0478-bigger.hdfs\n",
      "Epoch 18/80\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0476\n",
      "\n",
      "Epoch 00018: loss improved from 9.04778 to 9.04758, saving model to Weights-LSTM-improvement-18-9.0476-bigger.hdfs\n",
      "Epoch 19/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0474\n",
      "\n",
      "Epoch 00019: loss improved from 9.04758 to 9.04741, saving model to Weights-LSTM-improvement-19-9.0474-bigger.hdfs\n",
      "Epoch 20/80\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0472\n",
      "\n",
      "Epoch 00020: loss improved from 9.04741 to 9.04725, saving model to Weights-LSTM-improvement-20-9.0472-bigger.hdfs\n",
      "Epoch 21/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0471\n",
      "\n",
      "Epoch 00021: loss improved from 9.04725 to 9.04709, saving model to Weights-LSTM-improvement-21-9.0471-bigger.hdfs\n",
      "Epoch 22/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0470\n",
      "\n",
      "Epoch 00022: loss improved from 9.04709 to 9.04696, saving model to Weights-LSTM-improvement-22-9.0470-bigger.hdfs\n",
      "Epoch 23/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0468\n",
      "\n",
      "Epoch 00023: loss improved from 9.04696 to 9.04682, saving model to Weights-LSTM-improvement-23-9.0468-bigger.hdfs\n",
      "Epoch 24/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0467\n",
      "\n",
      "Epoch 00024: loss improved from 9.04682 to 9.04671, saving model to Weights-LSTM-improvement-24-9.0467-bigger.hdfs\n",
      "Epoch 25/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0466\n",
      "\n",
      "Epoch 00025: loss improved from 9.04671 to 9.04659, saving model to Weights-LSTM-improvement-25-9.0466-bigger.hdfs\n",
      "Epoch 26/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0465\n",
      "\n",
      "Epoch 00026: loss improved from 9.04659 to 9.04647, saving model to Weights-LSTM-improvement-26-9.0465-bigger.hdfs\n",
      "Epoch 27/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0464\n",
      "\n",
      "Epoch 00027: loss improved from 9.04647 to 9.04639, saving model to Weights-LSTM-improvement-27-9.0464-bigger.hdfs\n",
      "Epoch 28/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0463\n",
      "\n",
      "Epoch 00028: loss improved from 9.04639 to 9.04629, saving model to Weights-LSTM-improvement-28-9.0463-bigger.hdfs\n",
      "Epoch 29/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0462\n",
      "\n",
      "Epoch 00029: loss improved from 9.04629 to 9.04621, saving model to Weights-LSTM-improvement-29-9.0462-bigger.hdfs\n",
      "Epoch 30/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0461\n",
      "\n",
      "Epoch 00030: loss improved from 9.04621 to 9.04611, saving model to Weights-LSTM-improvement-30-9.0461-bigger.hdfs\n",
      "Epoch 31/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0460\n",
      "\n",
      "Epoch 00031: loss improved from 9.04611 to 9.04604, saving model to Weights-LSTM-improvement-31-9.0460-bigger.hdfs\n",
      "Epoch 32/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0460\n",
      "\n",
      "Epoch 00032: loss improved from 9.04604 to 9.04597, saving model to Weights-LSTM-improvement-32-9.0460-bigger.hdfs\n",
      "Epoch 33/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0459\n",
      "\n",
      "Epoch 00033: loss improved from 9.04597 to 9.04591, saving model to Weights-LSTM-improvement-33-9.0459-bigger.hdfs\n",
      "Epoch 34/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0458\n",
      "\n",
      "Epoch 00034: loss improved from 9.04591 to 9.04584, saving model to Weights-LSTM-improvement-34-9.0458-bigger.hdfs\n",
      "Epoch 35/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0458\n",
      "\n",
      "Epoch 00035: loss improved from 9.04584 to 9.04577, saving model to Weights-LSTM-improvement-35-9.0458-bigger.hdfs\n",
      "Epoch 36/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0484\n",
      "\n",
      "Epoch 00036: loss did not improve from 9.04577\n",
      "Epoch 37/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0457\n",
      "\n",
      "Epoch 00037: loss improved from 9.04577 to 9.04567, saving model to Weights-LSTM-improvement-37-9.0457-bigger.hdfs\n",
      "Epoch 38/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0456\n",
      "\n",
      "Epoch 00038: loss improved from 9.04567 to 9.04561, saving model to Weights-LSTM-improvement-38-9.0456-bigger.hdfs\n",
      "Epoch 39/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0456\n",
      "\n",
      "Epoch 00039: loss improved from 9.04561 to 9.04556, saving model to Weights-LSTM-improvement-39-9.0456-bigger.hdfs\n",
      "Epoch 40/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00040: loss improved from 9.04556 to 9.04552, saving model to Weights-LSTM-improvement-40-9.0455-bigger.hdfs\n",
      "Epoch 41/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00041: loss improved from 9.04552 to 9.04547, saving model to Weights-LSTM-improvement-41-9.0455-bigger.hdfs\n",
      "Epoch 42/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00042: loss improved from 9.04547 to 9.04543, saving model to Weights-LSTM-improvement-42-9.0454-bigger.hdfs\n",
      "Epoch 43/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00043: loss improved from 9.04543 to 9.04539, saving model to Weights-LSTM-improvement-43-9.0454-bigger.hdfs\n",
      "Epoch 44/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00044: loss improved from 9.04539 to 9.04535, saving model to Weights-LSTM-improvement-44-9.0453-bigger.hdfs\n",
      "Epoch 45/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00045: loss improved from 9.04535 to 9.04531, saving model to Weights-LSTM-improvement-45-9.0453-bigger.hdfs\n",
      "Epoch 46/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00046: loss improved from 9.04531 to 9.04528, saving model to Weights-LSTM-improvement-46-9.0453-bigger.hdfs\n",
      "Epoch 47/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00047: loss improved from 9.04528 to 9.04524, saving model to Weights-LSTM-improvement-47-9.0452-bigger.hdfs\n",
      "Epoch 48/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00048: loss improved from 9.04524 to 9.04521, saving model to Weights-LSTM-improvement-48-9.0452-bigger.hdfs\n",
      "Epoch 49/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00049: loss improved from 9.04521 to 9.04518, saving model to Weights-LSTM-improvement-49-9.0452-bigger.hdfs\n",
      "Epoch 50/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00050: loss improved from 9.04518 to 9.04515, saving model to Weights-LSTM-improvement-50-9.0452-bigger.hdfs\n",
      "Epoch 51/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00051: loss improved from 9.04515 to 9.04513, saving model to Weights-LSTM-improvement-51-9.0451-bigger.hdfs\n",
      "Epoch 52/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00052: loss improved from 9.04513 to 9.04510, saving model to Weights-LSTM-improvement-52-9.0451-bigger.hdfs\n",
      "Epoch 53/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00053: loss improved from 9.04510 to 9.04508, saving model to Weights-LSTM-improvement-53-9.0451-bigger.hdfs\n",
      "Epoch 54/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00054: loss improved from 9.04508 to 9.04505, saving model to Weights-LSTM-improvement-54-9.0450-bigger.hdfs\n",
      "Epoch 55/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00055: loss improved from 9.04505 to 9.04503, saving model to Weights-LSTM-improvement-55-9.0450-bigger.hdfs\n",
      "Epoch 56/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00056: loss improved from 9.04503 to 9.04501, saving model to Weights-LSTM-improvement-56-9.0450-bigger.hdfs\n",
      "Epoch 57/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00057: loss improved from 9.04501 to 9.04499, saving model to Weights-LSTM-improvement-57-9.0450-bigger.hdfs\n",
      "Epoch 58/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00058: loss improved from 9.04499 to 9.04496, saving model to Weights-LSTM-improvement-58-9.0450-bigger.hdfs\n",
      "Epoch 59/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00059: loss improved from 9.04496 to 9.04494, saving model to Weights-LSTM-improvement-59-9.0449-bigger.hdfs\n",
      "Epoch 60/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00060: loss improved from 9.04494 to 9.04493, saving model to Weights-LSTM-improvement-60-9.0449-bigger.hdfs\n",
      "Epoch 61/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00061: loss improved from 9.04493 to 9.04491, saving model to Weights-LSTM-improvement-61-9.0449-bigger.hdfs\n",
      "Epoch 62/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00062: loss improved from 9.04491 to 9.04490, saving model to Weights-LSTM-improvement-62-9.0449-bigger.hdfs\n",
      "Epoch 63/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00063: loss improved from 9.04490 to 9.04488, saving model to Weights-LSTM-improvement-63-9.0449-bigger.hdfs\n",
      "Epoch 64/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00064: loss improved from 9.04488 to 9.04487, saving model to Weights-LSTM-improvement-64-9.0449-bigger.hdfs\n",
      "Epoch 65/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00065: loss improved from 9.04487 to 9.04485, saving model to Weights-LSTM-improvement-65-9.0449-bigger.hdfs\n",
      "Epoch 66/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00066: loss improved from 9.04485 to 9.04484, saving model to Weights-LSTM-improvement-66-9.0448-bigger.hdfs\n",
      "Epoch 67/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00067: loss improved from 9.04484 to 9.04483, saving model to Weights-LSTM-improvement-67-9.0448-bigger.hdfs\n",
      "Epoch 68/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00068: loss improved from 9.04483 to 9.04482, saving model to Weights-LSTM-improvement-68-9.0448-bigger.hdfs\n",
      "Epoch 69/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00069: loss improved from 9.04482 to 9.04480, saving model to Weights-LSTM-improvement-69-9.0448-bigger.hdfs\n",
      "Epoch 70/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00070: loss improved from 9.04480 to 9.04480, saving model to Weights-LSTM-improvement-70-9.0448-bigger.hdfs\n",
      "Epoch 71/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00071: loss improved from 9.04480 to 9.04478, saving model to Weights-LSTM-improvement-71-9.0448-bigger.hdfs\n",
      "Epoch 72/80\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00072: loss improved from 9.04478 to 9.04477, saving model to Weights-LSTM-improvement-72-9.0448-bigger.hdfs\n",
      "Epoch 73/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00073: loss improved from 9.04477 to 9.04477, saving model to Weights-LSTM-improvement-73-9.0448-bigger.hdfs\n",
      "Epoch 74/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00074: loss improved from 9.04477 to 9.04475, saving model to Weights-LSTM-improvement-74-9.0447-bigger.hdfs\n",
      "Epoch 75/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00075: loss improved from 9.04475 to 9.04475, saving model to Weights-LSTM-improvement-75-9.0447-bigger.hdfs\n",
      "Epoch 76/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00076: loss improved from 9.04475 to 9.04473, saving model to Weights-LSTM-improvement-76-9.0447-bigger.hdfs\n",
      "Epoch 77/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00077: loss improved from 9.04473 to 9.04473, saving model to Weights-LSTM-improvement-77-9.0447-bigger.hdfs\n",
      "Epoch 78/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00078: loss improved from 9.04473 to 9.04472, saving model to Weights-LSTM-improvement-78-9.0447-bigger.hdfs\n",
      "Epoch 79/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00079: loss improved from 9.04472 to 9.04471, saving model to Weights-LSTM-improvement-79-9.0447-bigger.hdfs\n",
      "Epoch 80/80\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00080: loss improved from 9.04471 to 9.04470, saving model to Weights-LSTM-improvement-80-9.0447-bigger.hdfs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f12facd0110>"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Weights-LSTM-improvement-{epoch:02d}-{loss:.4f}-bigger.hdfs\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "\n",
    "model.fit(X, y, epochs=80, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8544\n",
      "Total Vocab:  8534\n",
      "Total Patterns:  8444\n",
      "Seed:\n",
      "\" a generic family comedy unlikely to be appreciated by anyone outside the under-10 set .kung pow seems like some futile concoction that was developed hastily after oedekerk and his fellow moviemakers got through crashing a college keg party .kurys seems intimidated by both her subject matter and the period trappings of this debut venture into the heritage business .the film virtually chokes on its own self-consciousness .a manipulative feminist empowerment tale thinly posing as a serious drama about spousal abuse .everything in maid in manhattan is exceedingly pleasant , designed not to offend .it goes down easy , leaving virtually no aftertaste .a profoundly stupid affair , populating its hackneyed and meanspirited storyline with cardboard characters and performers who value cash above credibility .... pays tribute to heroes the way julia roberts hands out awards -- with phony humility barely camouflaging grotesque narcissism .time stands still in more ways that one in clockstoppers , a sci-fi thriller as lazy as it is interminable .as a director , eastwood is off his game -- there 's no real sense of suspense , and none of the plot ` surprises ' are really surprising .eccentric enough to stave off doldrums , caruso 's self-conscious debut is also eminently forgettable .to work , love stories require the full emotional involvement and support of a viewer .that is made almost impossible by events that set the plot in motion .although barbershop boasts some of today 's hottest and hippest acts from the world of television , music and stand-up comedy , this movie strangely enough has the outdated swagger of a shameless ` 70s blaxploitation shuck-and-jive sitcom .a puzzle whose pieces do not fit .some are fascinating and others are not , and in the end , it is almost a good movie .would that greengrass had gone a tad less for grit and a lot more for intelligibility .the good is very , very good ... the rest runs from mildly unimpressive to despairingly awful .` butterfingered ' is the word for the big-fisted direction of jez butterworth , who manages to blast even the smallest sensitivities from the romance with his clamorous approach .be forewarned , if you 're depressed about anything before watching this film , you may just end up trying to drown yourself in a lake afterwards .a terrible adaptation of a play that only ever walked the delicate tightrope between farcical and loathsome .in the wrong hands , i.e. peploe 's , it 's simply unbearablean inexperienced director , mehta has much to learn .a limp eddie murphy vehicle that even he seems embarrassed to be part of .so muddled , repetitive and ragged that it says far less about the horrifying historical reality than about the filmmaker 's characteristic style .a gushy episode of `` m * a * s * h '' only this time from an asian perspective .`` looking for leonard '' just seems to kinda sit in neutral , hoping for a stiff wind to blow it uphill or something .nothing more than four or five mild chuckles surrounded by 86 minutes of overly-familiar and poorly-constructed comedy .definitely in the guilty pleasure b-movie category , reign of fire is so incredibly inane that it is laughingly enjoyable .good-looking but relentlessly lowbrow outing plays like clueless does south fork .entertaining enough , but nothing new... one resurrection too many .this is a film about the irksome , tiresome nature of complacency that remains utterly satisfied to remain the same throughout .even as the hero of the story rediscovers his passion in life , the mood remains oddly detached .demeo is not without talent ; he just needs better material .in spite of featuring a script credited to no fewer than five writers , apparently nobody here bothered to check it twice .no one involved , save dash , shows the slightest aptitude for acting , and the script , credited to director abdul malik abbott and ernest ` tron ' anderson , seems entirely improvised .initially gripping , eventually cloying pow drama .a timid , soggy near miss .works better in the conception than it does in the execution ... winds up seeming just a little too clever .to the vast majority of more casual filmgoers , it will probably be a talky bore .observant intelligence constantly vies with pretension -- and sometimes plain wacky implausibility -- throughout maelstrom .this version of h.g. wells ' time machine was directed by h.g. wells ' great-grandson .they should have found orson welles ' great-grandson .shunji iwai 's all about lily chou chou is a beautifully shot , but ultimately flawed film about growing up in japan .with more character development this might have been an eerie thriller ; with better payoffs , it could have been a thinking man 's monster movie .thriller directorial debut for traffic scribe gaghan has all the right parts , but the pieces do n't quite fit together .... would be a total loss if not for two supporting performances taking place at the movie 's edges .there 's not a single jump-in-your-seat moment and believe it or not , jason actually takes a backseat in his own film to special effects .goldbacher draws on an elegant visual sense and a talent for easy , seductive pacing ... but she and writing partner laurence coriat do n't manage an equally assured narrative coinage .though harris is affecting at times , he can not overcome the sense that pumpkin is a mere plot pawn for two directors with far less endearing disabilities .the documentary is much too conventional -- lots of boring talking heads , etc. -- to do the subject matter justice .the movie itself appears to be running on hypertime in reverse as the truly funny bits get further and further apart .this is not a jackie chan movie .it 's just a movie that happens to have jackie chan in it .and that makes all the difference .far too clever by half , howard 's film is really a series of strung-together moments , with all the spaces in between filled with fantasies , daydreams , memories and one fantastic visual trope after another .the problem with movies about angels is they have a tendency to slip into hokum .a rumor of angels does n't just slip -- it avalanches into forced fuzziness .no big whoop , nothing new to see , zero thrills , too many flashbacks and a choppy ending make for a bad film .i do n't think this movie loves women at all .shankman ... and screenwriter karen janszen bungle their way through the narrative as if it were a series of bible parables and not an actual story .a negligible british comedy .fails to convince the audience that these brats will ever be anything more than losers .slack and uninspired , and peopled mainly by characters so unsympathetic that you 're left with a sour taste in your mouth .skip this turd and pick your nose instead because you 're sure to get more out of the latter experience .what can one say about a balding 50-year-old actor playing an innocent boy carved from a log ?trailer trash cinema so uncool the only thing missing is the `` gadzooks ! ''her film is like a beautiful food entre that is n't heated properly , so that it ends up a bit cold and relatively flavorless .like the world of his film , hartley created a monster but did n't know how to handle it .no new plot conceptions or environmental changes , just different bodies for sharp objects to rip through .needs more impressionistic cinematography and exhilarating point-of-view shots and fewer slow-motion ` grandeur ' shots and quick-cut edits that often detract from the athleticism .in the end , there is n't much to it .a waste of fearless purity in the acting craft .the film is ultimately about as inspiring as a hallmark card .anyone not into high-tech splatterfests is advised to take the warning literally , and log on to something more user-friendly .disreputable doings and exquisite trappings are dampened by a lackluster script and substandard performances .you could easily mistake it for a sketchy work-in-progress that was inexplicably rushed to the megaplexes before its time .directors harry gantz and joe gantz have chosen a fascinating subject matter , but the couples exposing themselves are n't all that interesting .yet another entry in the sentimental oh-those-wacky-brits genre that was ushered in by the full monty and is still straining to produce another smash hit .for those for whom the name woody allen was once a guarantee of something fresh , sometimes funny , and usually genuinely worthwhile , hollywood ending is a depressing experiencefemme fatale offers nothing more than a bait-and-switch that is beyond playing fair with the audience .are we dealing with dreams , visions or being told what actually happened as if it were the third ending of clue ?it could have been something special , but two things drag it down to mediocrity -- director clare peploe 's misunderstanding of marivaux 's rhythms , and mira sorvino 's limitations as a classical actress .fluffy neo-noir hiding behind cutesy film references .imagine susan sontag falling in love with howard stern .like being trapped inside a huge video game , where exciting , inane images keep popping past your head and the same illogical things keep happening over and over again .should have been worth cheering as a breakthrough but is devoid of wit and humor .the best thing about the movie is its personable , amusing cast .these guys seem great to knock back a beer with but they 're simply not funny performers .everything was as superficial as the forced new jersey lowbrow accent uma had .director david fincher and writer david koepp ca n't sustain it .finally coming down off of miramax 's deep shelves after a couple of aborted attempts , waking up in reno makes a strong case for letting sleeping dogs lie .a movie that feels like the pilot episode of a new teen-targeted action tv series .one of the most highly-praised disappointments i 've had the misfortune to watch in quite some time .the animation and backdrops are lush and inventive , yet return to neverland never manages to take us to that elusive , lovely place where we suspend our disbelief .director shekhar kapur and screenwriters michael schiffer and hossein amini have tried hard to modernize and reconceptualize things , but the barriers finally prove to be too great .strong filmmaking requires a clear sense of purpose , and in that oh-so-important category , the four feathers comes up short .the thought of watching this film with an audience full of teenagers fixating on its body humour and reinforcement of stereotypes ( of which they 'll get plenty ) fills me with revulsion . \"\n",
      "one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year ."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-438-ca96e74d3695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "\n",
    "import sys\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Load the network weights\n",
    "filename = \"Weights-LSTM-improvement-80-9.0447-bigger.hdfs\"\n",
    "#filename = \"Weights-LSTM-improvement-44-1.3592-bigger.hdfs\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(700):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 epochs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0181\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.01810, saving model to Weights-LSTM-improvement-01-9.0181-bigger.hdfs\n",
      "Epoch 2/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0817\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.01810\n",
      "Epoch 3/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0610\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.01810\n",
      "Epoch 4/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0511\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.01810\n",
      "Epoch 5/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0238\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.01810\n",
      "Epoch 6/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 8.8395\n",
      "\n",
      "Epoch 00006: loss improved from 9.01810 to 8.83950, saving model to Weights-LSTM-improvement-06-8.8395-bigger.hdfs\n",
      "Epoch 7/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 8.3940\n",
      "\n",
      "Epoch 00007: loss improved from 8.83950 to 8.39402, saving model to Weights-LSTM-improvement-07-8.3940-bigger.hdfs\n",
      "Epoch 8/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 7.7437\n",
      "\n",
      "Epoch 00008: loss improved from 8.39402 to 7.74374, saving model to Weights-LSTM-improvement-08-7.7437-bigger.hdfs\n",
      "Epoch 9/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 6.9858\n",
      "\n",
      "Epoch 00009: loss improved from 7.74374 to 6.98577, saving model to Weights-LSTM-improvement-09-6.9858-bigger.hdfs\n",
      "Epoch 10/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 6.1293\n",
      "\n",
      "Epoch 00010: loss improved from 6.98577 to 6.12929, saving model to Weights-LSTM-improvement-10-6.1293-bigger.hdfs\n",
      "Epoch 11/100\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 5.3213\n",
      "\n",
      "Epoch 00011: loss improved from 6.12929 to 5.32130, saving model to Weights-LSTM-improvement-11-5.3213-bigger.hdfs\n",
      "Epoch 12/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 4.5826\n",
      "\n",
      "Epoch 00012: loss improved from 5.32130 to 4.58260, saving model to Weights-LSTM-improvement-12-4.5826-bigger.hdfs\n",
      "Epoch 13/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 3.9050\n",
      "\n",
      "Epoch 00013: loss improved from 4.58260 to 3.90504, saving model to Weights-LSTM-improvement-13-3.9050-bigger.hdfs\n",
      "Epoch 14/100\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 3.3774\n",
      "\n",
      "Epoch 00014: loss improved from 3.90504 to 3.37736, saving model to Weights-LSTM-improvement-14-3.3774-bigger.hdfs\n",
      "Epoch 15/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 2.8707\n",
      "\n",
      "Epoch 00015: loss improved from 3.37736 to 2.87072, saving model to Weights-LSTM-improvement-15-2.8707-bigger.hdfs\n",
      "Epoch 16/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 2.4300\n",
      "\n",
      "Epoch 00016: loss improved from 2.87072 to 2.43004, saving model to Weights-LSTM-improvement-16-2.4300-bigger.hdfs\n",
      "Epoch 17/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 2.0983\n",
      "\n",
      "Epoch 00017: loss improved from 2.43004 to 2.09834, saving model to Weights-LSTM-improvement-17-2.0983-bigger.hdfs\n",
      "Epoch 18/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 1.7846\n",
      "\n",
      "Epoch 00018: loss improved from 2.09834 to 1.78463, saving model to Weights-LSTM-improvement-18-1.7846-bigger.hdfs\n",
      "Epoch 19/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 1.5632\n",
      "\n",
      "Epoch 00019: loss improved from 1.78463 to 1.56321, saving model to Weights-LSTM-improvement-19-1.5632-bigger.hdfs\n",
      "Epoch 20/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 1.3868\n",
      "\n",
      "Epoch 00020: loss improved from 1.56321 to 1.38678, saving model to Weights-LSTM-improvement-20-1.3868-bigger.hdfs\n",
      "Epoch 21/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 1.2228\n",
      "\n",
      "Epoch 00021: loss improved from 1.38678 to 1.22278, saving model to Weights-LSTM-improvement-21-1.2228-bigger.hdfs\n",
      "Epoch 22/100\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 1.0651\n",
      "\n",
      "Epoch 00022: loss improved from 1.22278 to 1.06507, saving model to Weights-LSTM-improvement-22-1.0651-bigger.hdfs\n",
      "Epoch 23/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 0.9587\n",
      "\n",
      "Epoch 00023: loss improved from 1.06507 to 0.95870, saving model to Weights-LSTM-improvement-23-0.9587-bigger.hdfs\n",
      "Epoch 24/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.8995\n",
      "\n",
      "Epoch 00024: loss improved from 0.95870 to 0.89951, saving model to Weights-LSTM-improvement-24-0.8995-bigger.hdfs\n",
      "Epoch 25/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.7875\n",
      "\n",
      "Epoch 00025: loss improved from 0.89951 to 0.78746, saving model to Weights-LSTM-improvement-25-0.7875-bigger.hdfs\n",
      "Epoch 26/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.7811\n",
      "\n",
      "Epoch 00026: loss improved from 0.78746 to 0.78108, saving model to Weights-LSTM-improvement-26-0.7811-bigger.hdfs\n",
      "Epoch 27/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.6767\n",
      "\n",
      "Epoch 00027: loss improved from 0.78108 to 0.67672, saving model to Weights-LSTM-improvement-27-0.6767-bigger.hdfs\n",
      "Epoch 28/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.5976\n",
      "\n",
      "Epoch 00028: loss improved from 0.67672 to 0.59758, saving model to Weights-LSTM-improvement-28-0.5976-bigger.hdfs\n",
      "Epoch 29/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.5428\n",
      "\n",
      "Epoch 00029: loss improved from 0.59758 to 0.54282, saving model to Weights-LSTM-improvement-29-0.5428-bigger.hdfs\n",
      "Epoch 30/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.5012\n",
      "\n",
      "Epoch 00030: loss improved from 0.54282 to 0.50116, saving model to Weights-LSTM-improvement-30-0.5012-bigger.hdfs\n",
      "Epoch 31/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.4990\n",
      "\n",
      "Epoch 00031: loss improved from 0.50116 to 0.49898, saving model to Weights-LSTM-improvement-31-0.4990-bigger.hdfs\n",
      "Epoch 32/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.4856\n",
      "\n",
      "Epoch 00032: loss improved from 0.49898 to 0.48565, saving model to Weights-LSTM-improvement-32-0.4856-bigger.hdfs\n",
      "Epoch 33/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.4164\n",
      "\n",
      "Epoch 00033: loss improved from 0.48565 to 0.41640, saving model to Weights-LSTM-improvement-33-0.4164-bigger.hdfs\n",
      "Epoch 34/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.3851\n",
      "\n",
      "Epoch 00034: loss improved from 0.41640 to 0.38507, saving model to Weights-LSTM-improvement-34-0.3851-bigger.hdfs\n",
      "Epoch 35/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.3735\n",
      "\n",
      "Epoch 00035: loss improved from 0.38507 to 0.37353, saving model to Weights-LSTM-improvement-35-0.3735-bigger.hdfs\n",
      "Epoch 36/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.3875\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.37353\n",
      "Epoch 37/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.4079\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.37353\n",
      "Epoch 38/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.3157\n",
      "\n",
      "Epoch 00038: loss improved from 0.37353 to 0.31572, saving model to Weights-LSTM-improvement-38-0.3157-bigger.hdfs\n",
      "Epoch 39/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.2834\n",
      "\n",
      "Epoch 00039: loss improved from 0.31572 to 0.28338, saving model to Weights-LSTM-improvement-39-0.2834-bigger.hdfs\n",
      "Epoch 40/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.3030\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.28338\n",
      "Epoch 41/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.4850\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.28338\n",
      "Epoch 42/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.5259\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.28338\n",
      "Epoch 43/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.2622\n",
      "\n",
      "Epoch 00043: loss improved from 0.28338 to 0.26222, saving model to Weights-LSTM-improvement-43-0.2622-bigger.hdfs\n",
      "Epoch 44/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1976\n",
      "\n",
      "Epoch 00044: loss improved from 0.26222 to 0.19764, saving model to Weights-LSTM-improvement-44-0.1976-bigger.hdfs\n",
      "Epoch 45/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1694\n",
      "\n",
      "Epoch 00045: loss improved from 0.19764 to 0.16941, saving model to Weights-LSTM-improvement-45-0.1694-bigger.hdfs\n",
      "Epoch 46/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1536\n",
      "\n",
      "Epoch 00046: loss improved from 0.16941 to 0.15362, saving model to Weights-LSTM-improvement-46-0.1536-bigger.hdfs\n",
      "Epoch 47/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1487\n",
      "\n",
      "Epoch 00047: loss improved from 0.15362 to 0.14869, saving model to Weights-LSTM-improvement-47-0.1487-bigger.hdfs\n",
      "Epoch 48/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1457\n",
      "\n",
      "Epoch 00048: loss improved from 0.14869 to 0.14566, saving model to Weights-LSTM-improvement-48-0.1457-bigger.hdfs\n",
      "Epoch 49/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1514\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.14566\n",
      "Epoch 50/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1502\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.14566\n",
      "Epoch 51/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1478\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.14566\n",
      "Epoch 52/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.2430\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.14566\n",
      "Epoch 53/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.8626\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.14566\n",
      "Epoch 54/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.5516\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.14566\n",
      "Epoch 55/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1922\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.14566\n",
      "Epoch 56/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1086\n",
      "\n",
      "Epoch 00056: loss improved from 0.14566 to 0.10864, saving model to Weights-LSTM-improvement-56-0.1086-bigger.hdfs\n",
      "Epoch 57/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0935\n",
      "\n",
      "Epoch 00057: loss improved from 0.10864 to 0.09348, saving model to Weights-LSTM-improvement-57-0.0935-bigger.hdfs\n",
      "Epoch 58/100\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 0.0779\n",
      "\n",
      "Epoch 00058: loss improved from 0.09348 to 0.07792, saving model to Weights-LSTM-improvement-58-0.0779-bigger.hdfs\n",
      "Epoch 59/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0735\n",
      "\n",
      "Epoch 00059: loss improved from 0.07792 to 0.07351, saving model to Weights-LSTM-improvement-59-0.0735-bigger.hdfs\n",
      "Epoch 60/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0677\n",
      "\n",
      "Epoch 00060: loss improved from 0.07351 to 0.06768, saving model to Weights-LSTM-improvement-60-0.0677-bigger.hdfs\n",
      "Epoch 61/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0649\n",
      "\n",
      "Epoch 00061: loss improved from 0.06768 to 0.06488, saving model to Weights-LSTM-improvement-61-0.0649-bigger.hdfs\n",
      "Epoch 62/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0670\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.06488\n",
      "Epoch 63/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0727\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.06488\n",
      "Epoch 64/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0768\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.06488\n",
      "Epoch 65/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0996\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.06488\n",
      "Epoch 66/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.7409\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.06488\n",
      "Epoch 67/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.7527\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.06488\n",
      "Epoch 68/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1655\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.06488\n",
      "Epoch 69/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0774\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.06488\n",
      "Epoch 70/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0522\n",
      "\n",
      "Epoch 00070: loss improved from 0.06488 to 0.05218, saving model to Weights-LSTM-improvement-70-0.0522-bigger.hdfs\n",
      "Epoch 71/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0460\n",
      "\n",
      "Epoch 00071: loss improved from 0.05218 to 0.04597, saving model to Weights-LSTM-improvement-71-0.0460-bigger.hdfs\n",
      "Epoch 72/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0405\n",
      "\n",
      "Epoch 00072: loss improved from 0.04597 to 0.04053, saving model to Weights-LSTM-improvement-72-0.0405-bigger.hdfs\n",
      "Epoch 73/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0380\n",
      "\n",
      "Epoch 00073: loss improved from 0.04053 to 0.03804, saving model to Weights-LSTM-improvement-73-0.0380-bigger.hdfs\n",
      "Epoch 74/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0363\n",
      "\n",
      "Epoch 00074: loss improved from 0.03804 to 0.03632, saving model to Weights-LSTM-improvement-74-0.0363-bigger.hdfs\n",
      "Epoch 75/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0353\n",
      "\n",
      "Epoch 00075: loss improved from 0.03632 to 0.03525, saving model to Weights-LSTM-improvement-75-0.0353-bigger.hdfs\n",
      "Epoch 76/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0341\n",
      "\n",
      "Epoch 00076: loss improved from 0.03525 to 0.03405, saving model to Weights-LSTM-improvement-76-0.0341-bigger.hdfs\n",
      "Epoch 77/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0344\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.03405\n",
      "Epoch 78/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0336\n",
      "\n",
      "Epoch 00078: loss improved from 0.03405 to 0.03363, saving model to Weights-LSTM-improvement-78-0.0336-bigger.hdfs\n",
      "Epoch 79/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0452\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.03363\n",
      "Epoch 80/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0592\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.03363\n",
      "Epoch 81/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0705\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.03363\n",
      "Epoch 82/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.5102\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.03363\n",
      "Epoch 83/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 1.1028\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.03363\n",
      "Epoch 84/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.1977\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.03363\n",
      "Epoch 85/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0587\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.03363\n",
      "Epoch 86/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0372\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.03363\n",
      "Epoch 87/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0291\n",
      "\n",
      "Epoch 00087: loss improved from 0.03363 to 0.02911, saving model to Weights-LSTM-improvement-87-0.0291-bigger.hdfs\n",
      "Epoch 88/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0249\n",
      "\n",
      "Epoch 00088: loss improved from 0.02911 to 0.02486, saving model to Weights-LSTM-improvement-88-0.0249-bigger.hdfs\n",
      "Epoch 89/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0231\n",
      "\n",
      "Epoch 00089: loss improved from 0.02486 to 0.02311, saving model to Weights-LSTM-improvement-89-0.0231-bigger.hdfs\n",
      "Epoch 90/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0209\n",
      "\n",
      "Epoch 00090: loss improved from 0.02311 to 0.02095, saving model to Weights-LSTM-improvement-90-0.0209-bigger.hdfs\n",
      "Epoch 91/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0204\n",
      "\n",
      "Epoch 00091: loss improved from 0.02095 to 0.02036, saving model to Weights-LSTM-improvement-91-0.0204-bigger.hdfs\n",
      "Epoch 92/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0194\n",
      "\n",
      "Epoch 00092: loss improved from 0.02036 to 0.01940, saving model to Weights-LSTM-improvement-92-0.0194-bigger.hdfs\n",
      "Epoch 93/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0183\n",
      "\n",
      "Epoch 00093: loss improved from 0.01940 to 0.01827, saving model to Weights-LSTM-improvement-93-0.0183-bigger.hdfs\n",
      "Epoch 94/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0185\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.01827\n",
      "Epoch 95/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0175\n",
      "\n",
      "Epoch 00095: loss improved from 0.01827 to 0.01750, saving model to Weights-LSTM-improvement-95-0.0175-bigger.hdfs\n",
      "Epoch 96/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0165\n",
      "\n",
      "Epoch 00096: loss improved from 0.01750 to 0.01653, saving model to Weights-LSTM-improvement-96-0.0165-bigger.hdfs\n",
      "Epoch 97/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0152\n",
      "\n",
      "Epoch 00097: loss improved from 0.01653 to 0.01520, saving model to Weights-LSTM-improvement-97-0.0152-bigger.hdfs\n",
      "Epoch 98/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0176\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.01520\n",
      "Epoch 99/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0227\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.01520\n",
      "Epoch 100/100\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 0.0304\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.01520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3e7c736f10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Weights-LSTM-improvement-{epoch:02d}-{loss:.4f}-bigger.hdfs\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8544\n",
      "Total Vocab:  8534\n",
      "Total Patterns:  8444\n",
      "Seed:\n",
      "\" a zinger-filled crowd-pleaser that open-minded elvis fans ( but by no means all ) will have fun with .diggs and lathan are among the chief reasons brown sugar is such a sweet and sexy film .entirely suspenseful , extremely well-paced and ultimately ... dare i say , entertaining !the riveting performances by the incredibly flexible cast make love a joy to behold .terrific as nadia , a russian mail-order bride who comes to america speaking not a word of english , it 's kidman who holds the film together with a supremely kittenish performance that gradually accumulates more layers .with an unflappable air of decadent urbanity , everett remains a perfect wildean actor , and a relaxed firth displays impeccable comic skill .the re-release of ron howard 's apollo 13 in the imax format proves absolutely that really , really , really good things can come in enormous packages .very well written and directed with brutal honesty and respect for its audience .wonderful fencing scenes and an exciting plot make this an eminently engrossing film .it 's pretty linear and only makeup-deep , but bogdanovich ties it together with efficiency and an affection for the period .a surprisingly charming and even witty match for the best of hollywood 's comic-book adaptations .this is a superior horror flick .adaptation is simply brilliant .smart and alert , thirteen conversations about one thing is a small gem .the pleasure of read my lips is like seeing a series of perfect black pearls clicking together to form a string .we 're drawn in by the dark luster .a haunting tale of murder and mayhem .i love the opening scenes of a wintry new york city in 1899 .cinematic poetry showcases the city 's old-world charm before machines change nearly everything .it 's hard to imagine anyone managing to steal a movie not only from charismatic rising star jake gyllenhaal but also from accomplished oscar winners susan sarandon , dustin hoffman and holly hunter , yet newcomer ellen pompeo pulls off the feat with aplomb .one of the best rock documentaries ever .wilco is a phenomenal band with such an engrossing story that will capture the minds and hearts of many .ian holm conquers france as an earthy napoleonoffers big , fat , dumb laughs that may make you hate yourself for giving in .ah , what the hell .( sports ) admirable energy , full-bodied characterizations and narrative urgency .a portrait of an artist .directors brett morgen and nanette burstein have put together a bold biographical fantasia .the subtitled costume drama is set in a remote african empire before cell phones , guns , and the internal combustion engine , but the politics that thump through it are as timely as tomorrow .a tremendous piece of work .a delightful , if minor , pastry of a movie .while obviously aimed at kids , the country bears ... should keep parents amused with its low groan-to-guffaw ratio .labute masterfully balances both traditional or modern stories together in a manner that one never overwhelms the other .something for everyone .irwin is so earnest that it 's hard to resist his pleas to spare wildlife and respect their environs .there are far worse messages to teach a young audience , which will probably be perfectly happy with the sloppy slapstick comedy .leigh succeeds in delivering a dramatic slap in the face that 's simultaneously painful and refreshing .not about scares but a mood in which an ominous , pervasive , and unknown threat lurks just below the proceedings and adds an almost constant mindset of suspense .` film aficionados can not help but love cinema paradiso , whether the original version or new director 's cut . 'a fascinating glimpse into an insular world that gives the lie to many clichs and showcases a group of dedicated artists .it 's one thing to read about or rail against the ongoing - and unprecedented - construction project going on over our heads .it 's quite another to feel physically caught up in the process .contradicts everything we 've come to expect from movies nowadays .instead of simply handling conventional material in a conventional way , secretary takes the most unexpected material and handles it in the most unexpected way .could i have been more geeked when i heard that apollo 13 was going to be released in imax format ?in a word : no. .murderous maids has a lot going for it , not least the brilliant performances by testud ... and parmentier .filmmaker stacy peralta has a flashy editing style that does n't always jell with sean penn 's monotone narration , but he respects the material without sentimentalizing it .there are a couple of things that elevate `` glory '' above most of its ilk , most notably the mere presence of duvall .it 's light on the chills and heavy on the atmospheric weirdness , and there are moments of jaw-droppingly odd behavior -- yet i found it weirdly appealing .( rises ) above its oh-so-hollywood rejiggering and its conventional direction to give the film a soul and an unabashed sense of good old-fashioned escapism .a breezy blend of art , history , esoteric musings and philosophy .kids will love its fantasy and adventure , and grownups should appreciate its whimsical humor .tsai ming-liang 's ghosts are painfully aware of their not-being .leaping from one arresting image to another , songs from the second floor has all the enjoyable randomness of a very lively dream and so manages to be compelling , amusing and unsettling at the same time .sean penn , you owe nicolas cage an apology .the performances are uniformly good .she 's all-powerful , a voice for a pop-cyber culture that feeds on her bjorkness .it 's a perfect show of respect to just one of those underrated professionals who deserve but rarely receive it .for all its plot twists , and some of them verge on the bizarre as the film winds down , blood work is a strong , character-oriented piece .the story line may be 127 years old , but el crimen del padre amaro ... could n't be more timely in its despairing vision of corruption within the catholic establishment .this in-depth study of important developments of the computer industry should make it required viewing in university computer science departments for years to come .it shows us a slice of life that 's very different from our own and yet instantly recognizable .a wonderfully speculative character study that made up for its rather slow beginning by drawing me into the picture .has its share of arresting images .leave it to john sayles to take on developers , the chamber of commerce , tourism , historical pageants , and commercialism all in the same movie ... without neglecting character development for even one minute .reign of fire just might go down as one of the all-time great apocalypse movies .a smart little indie .payne has created a beautiful canvas , and nicholson proves once again that he 's the best brush in the business .try as you might to resist , if you 've got a place in your heart for smokey robinson , this movie will worm its way there .a riveting profile of law enforcement , and a visceral , nasty journey into an urban hades .director douglas mcgrath takes on nickleby with all the halfhearted zeal of an 8th grade boy delving into required reading .stands as a document of what it felt like to be a new yorker -- or , really , to be a human being -- in the weeks after 9/11 .i am not generally a huge fan of cartoons derived from tv shows , but hey arnold !the movie is clever , offbeat and even gritty enough to overcome my resistance .with not a lot of help from the screenplay ( proficient , but singularly cursory ) , ( testud ) acts with the feral intensity of the young bette davis .it 's a film that 's destined to win a wide summer audience through word-of-mouth reviews and , not far down the line , to find a place among the studio 's animated classics .slow and ponderous , but rohmer 's drama builds to an intense indoor drama about compassion , sacrifice , and christian love in the face of political corruption .if you 're not totally weirded - out by the notion of cinema as community-therapy spectacle , quitting hits home with disorienting force .austin powers for the most part is extremely funny , the first part making up for any flaws that come later .while tattoo borrows heavily from both seven and the silence of the lambs , it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in .cho continues her exploration of the outer limits of raunch with considerable brio .elvira fans could hardly ask for more .a canny , derivative , wildly gruesome portrait of a london sociopath who 's the scariest of sadists .the movie should be credited with remembering his victims .fast-paced and wonderfully edited , the film is extremely thorough .a bracing , unblinking work that serves as a painful elegy and sobering cautionary tale .hashiguchi uses the situation to evoke a japan bustling atop an undercurrent of loneliness and isolation .as if trying to grab a lump of play-doh , the harder that liman tries to squeeze his story , the more details slip out between his fingers .my big fat greek wedding is not only the best date movie of the year , it 's also a -- dare i say it twice -- delightfully charming -- and totally american , i might add -- slice of comedic bliss .few films have captured the chaos of an urban conflagration with such fury , and audience members will leave feeling as shaken as nesbitt 's cooper looks when the bullets stop flying .another love story in 2002 's remarkable procession of sweeping pictures that have reinvigorated the romance genre .it 's another retelling of alexandre dumas ' classic .why ?who knows , but it works under the direction of kevin reynolds .( f ) rom the performances and the cinematography to the outstanding soundtrack and unconventional narrative , the film is blazingly alive and admirable on many levels .shiri is an action film that delivers on the promise of excitement , but it also has a strong dramatic and emotional pull that gradually sneaks up on the audience .provides the kind of ` laugh therapy ' i need from movie comedies -- offbeat humor , amusing characters , and a happy ending .after seeing ` analyze that , ' i feel better already .a penetrating , potent exploration of sanctimony , self-awareness , self-hatred and self-determination . \"\n",
      "one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "\n",
    "import sys\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Load the network weights\n",
    "filename = \"Weights-LSTM-improvement-80-9.0447-bigger.hdfs\"\n",
    "#filename = \"Weights-LSTM-improvement-44-1.3592-bigger.hdfs\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(70):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 epochs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model_500 = Sequential()\n",
    "model_500.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model_500.add(Dropout(0.2))\n",
    "model_500.add(LSTM(256))\n",
    "model_500.add(Dropout(0.2))\n",
    "model_500.add(Dense(y.shape[1], activation='softmax'))\n",
    "model_500.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0293\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.02926, saving model to Weights-LSTM-improvement-01-9.0293-bigger.hdfs\n",
      "Epoch 2/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0815\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.02926\n",
      "Epoch 3/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0954\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.02926\n",
      "Epoch 4/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0738\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.02926\n",
      "Epoch 5/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0581\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.02926\n",
      "Epoch 6/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0537\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.02926\n",
      "Epoch 7/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0527\n",
      "\n",
      "Epoch 00007: loss did not improve from 9.02926\n",
      "Epoch 8/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0516\n",
      "\n",
      "Epoch 00008: loss did not improve from 9.02926\n",
      "Epoch 9/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0503\n",
      "\n",
      "Epoch 00009: loss did not improve from 9.02926\n",
      "Epoch 10/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0538\n",
      "\n",
      "Epoch 00010: loss did not improve from 9.02926\n",
      "Epoch 11/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0528\n",
      "\n",
      "Epoch 00011: loss did not improve from 9.02926\n",
      "Epoch 12/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0495\n",
      "\n",
      "Epoch 00012: loss did not improve from 9.02926\n",
      "Epoch 13/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0491\n",
      "\n",
      "Epoch 00013: loss did not improve from 9.02926\n",
      "Epoch 14/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0488\n",
      "\n",
      "Epoch 00014: loss did not improve from 9.02926\n",
      "Epoch 15/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0485\n",
      "\n",
      "Epoch 00015: loss did not improve from 9.02926\n",
      "Epoch 16/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0483\n",
      "\n",
      "Epoch 00016: loss did not improve from 9.02926\n",
      "Epoch 17/500\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0480\n",
      "\n",
      "Epoch 00017: loss did not improve from 9.02926\n",
      "Epoch 18/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0478\n",
      "\n",
      "Epoch 00018: loss did not improve from 9.02926\n",
      "Epoch 19/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0476\n",
      "\n",
      "Epoch 00019: loss did not improve from 9.02926\n",
      "Epoch 20/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0475\n",
      "\n",
      "Epoch 00020: loss did not improve from 9.02926\n",
      "Epoch 21/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0473\n",
      "\n",
      "Epoch 00021: loss did not improve from 9.02926\n",
      "Epoch 22/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0471\n",
      "\n",
      "Epoch 00022: loss did not improve from 9.02926\n",
      "Epoch 23/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0470\n",
      "\n",
      "Epoch 00023: loss did not improve from 9.02926\n",
      "Epoch 24/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0469\n",
      "\n",
      "Epoch 00024: loss did not improve from 9.02926\n",
      "Epoch 25/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0467\n",
      "\n",
      "Epoch 00025: loss did not improve from 9.02926\n",
      "Epoch 26/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0466\n",
      "\n",
      "Epoch 00026: loss did not improve from 9.02926\n",
      "Epoch 27/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0465\n",
      "\n",
      "Epoch 00027: loss did not improve from 9.02926\n",
      "Epoch 28/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0464\n",
      "\n",
      "Epoch 00028: loss did not improve from 9.02926\n",
      "Epoch 29/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0463\n",
      "\n",
      "Epoch 00029: loss did not improve from 9.02926\n",
      "Epoch 30/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0462\n",
      "\n",
      "Epoch 00030: loss did not improve from 9.02926\n",
      "Epoch 31/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0462\n",
      "\n",
      "Epoch 00031: loss did not improve from 9.02926\n",
      "Epoch 32/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0461\n",
      "\n",
      "Epoch 00032: loss did not improve from 9.02926\n",
      "Epoch 33/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0460\n",
      "\n",
      "Epoch 00033: loss did not improve from 9.02926\n",
      "Epoch 34/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0459\n",
      "\n",
      "Epoch 00034: loss did not improve from 9.02926\n",
      "Epoch 35/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0459\n",
      "\n",
      "Epoch 00035: loss did not improve from 9.02926\n",
      "Epoch 36/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0458\n",
      "\n",
      "Epoch 00036: loss did not improve from 9.02926\n",
      "Epoch 37/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0457\n",
      "\n",
      "Epoch 00037: loss did not improve from 9.02926\n",
      "Epoch 38/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0457\n",
      "\n",
      "Epoch 00038: loss did not improve from 9.02926\n",
      "Epoch 39/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0456\n",
      "\n",
      "Epoch 00039: loss did not improve from 9.02926\n",
      "Epoch 40/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0456\n",
      "\n",
      "Epoch 00040: loss did not improve from 9.02926\n",
      "Epoch 41/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00041: loss did not improve from 9.02926\n",
      "Epoch 42/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00042: loss did not improve from 9.02926\n",
      "Epoch 43/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00043: loss did not improve from 9.02926\n",
      "Epoch 44/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00044: loss did not improve from 9.02926\n",
      "Epoch 45/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00045: loss did not improve from 9.02926\n",
      "Epoch 46/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00046: loss did not improve from 9.02926\n",
      "Epoch 47/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00047: loss did not improve from 9.02926\n",
      "Epoch 48/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00048: loss did not improve from 9.02926\n",
      "Epoch 49/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00049: loss did not improve from 9.02926\n",
      "Epoch 50/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00050: loss did not improve from 9.02926\n",
      "Epoch 51/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00051: loss did not improve from 9.02926\n",
      "Epoch 52/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00052: loss did not improve from 9.02926\n",
      "Epoch 53/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00053: loss did not improve from 9.02926\n",
      "Epoch 54/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00054: loss did not improve from 9.02926\n",
      "Epoch 55/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00055: loss did not improve from 9.02926\n",
      "Epoch 56/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00056: loss did not improve from 9.02926\n",
      "Epoch 57/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00057: loss did not improve from 9.02926\n",
      "Epoch 58/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00058: loss did not improve from 9.02926\n",
      "Epoch 59/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00059: loss did not improve from 9.02926\n",
      "Epoch 60/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00060: loss did not improve from 9.02926\n",
      "Epoch 61/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00061: loss did not improve from 9.02926\n",
      "Epoch 62/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00062: loss did not improve from 9.02926\n",
      "Epoch 63/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00063: loss did not improve from 9.02926\n",
      "Epoch 64/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00064: loss did not improve from 9.02926\n",
      "Epoch 65/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00065: loss did not improve from 9.02926\n",
      "Epoch 66/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00066: loss did not improve from 9.02926\n",
      "Epoch 67/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00067: loss did not improve from 9.02926\n",
      "Epoch 68/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00068: loss did not improve from 9.02926\n",
      "Epoch 69/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00069: loss did not improve from 9.02926\n",
      "Epoch 70/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00070: loss did not improve from 9.02926\n",
      "Epoch 71/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00071: loss did not improve from 9.02926\n",
      "Epoch 72/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00072: loss did not improve from 9.02926\n",
      "Epoch 73/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00073: loss did not improve from 9.02926\n",
      "Epoch 74/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00074: loss did not improve from 9.02926\n",
      "Epoch 75/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00075: loss did not improve from 9.02926\n",
      "Epoch 76/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00076: loss did not improve from 9.02926\n",
      "Epoch 77/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00077: loss did not improve from 9.02926\n",
      "Epoch 78/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00078: loss did not improve from 9.02926\n",
      "Epoch 79/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00079: loss did not improve from 9.02926\n",
      "Epoch 80/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00080: loss did not improve from 9.02926\n",
      "Epoch 81/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00081: loss did not improve from 9.02926\n",
      "Epoch 82/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00082: loss did not improve from 9.02926\n",
      "Epoch 83/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00083: loss did not improve from 9.02926\n",
      "Epoch 84/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00084: loss did not improve from 9.02926\n",
      "Epoch 85/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00085: loss did not improve from 9.02926\n",
      "Epoch 86/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00086: loss did not improve from 9.02926\n",
      "Epoch 87/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00087: loss did not improve from 9.02926\n",
      "Epoch 88/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00088: loss did not improve from 9.02926\n",
      "Epoch 89/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00089: loss did not improve from 9.02926\n",
      "Epoch 90/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00090: loss did not improve from 9.02926\n",
      "Epoch 91/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00091: loss did not improve from 9.02926\n",
      "Epoch 92/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00092: loss did not improve from 9.02926\n",
      "Epoch 93/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00093: loss did not improve from 9.02926\n",
      "Epoch 94/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00094: loss did not improve from 9.02926\n",
      "Epoch 95/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00095: loss did not improve from 9.02926\n",
      "Epoch 96/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00096: loss did not improve from 9.02926\n",
      "Epoch 97/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00097: loss did not improve from 9.02926\n",
      "Epoch 98/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00098: loss did not improve from 9.02926\n",
      "Epoch 99/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00099: loss did not improve from 9.02926\n",
      "Epoch 100/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00100: loss did not improve from 9.02926\n",
      "Epoch 101/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00101: loss did not improve from 9.02926\n",
      "Epoch 102/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00102: loss did not improve from 9.02926\n",
      "Epoch 103/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00103: loss did not improve from 9.02926\n",
      "Epoch 104/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00104: loss did not improve from 9.02926\n",
      "Epoch 105/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00105: loss did not improve from 9.02926\n",
      "Epoch 106/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00106: loss did not improve from 9.02926\n",
      "Epoch 107/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00107: loss did not improve from 9.02926\n",
      "Epoch 108/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00108: loss did not improve from 9.02926\n",
      "Epoch 109/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00109: loss did not improve from 9.02926\n",
      "Epoch 110/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00110: loss did not improve from 9.02926\n",
      "Epoch 111/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00111: loss did not improve from 9.02926\n",
      "Epoch 112/500\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00112: loss did not improve from 9.02926\n",
      "Epoch 113/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00113: loss did not improve from 9.02926\n",
      "Epoch 114/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00114: loss did not improve from 9.02926\n",
      "Epoch 115/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00115: loss did not improve from 9.02926\n",
      "Epoch 116/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00116: loss did not improve from 9.02926\n",
      "Epoch 117/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00117: loss did not improve from 9.02926\n",
      "Epoch 118/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00118: loss did not improve from 9.02926\n",
      "Epoch 119/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00119: loss did not improve from 9.02926\n",
      "Epoch 120/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00120: loss did not improve from 9.02926\n",
      "Epoch 121/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00121: loss did not improve from 9.02926\n",
      "Epoch 122/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00122: loss did not improve from 9.02926\n",
      "Epoch 123/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00123: loss did not improve from 9.02926\n",
      "Epoch 124/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00124: loss did not improve from 9.02926\n",
      "Epoch 125/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00125: loss did not improve from 9.02926\n",
      "Epoch 126/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00126: loss did not improve from 9.02926\n",
      "Epoch 127/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00127: loss did not improve from 9.02926\n",
      "Epoch 128/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00128: loss did not improve from 9.02926\n",
      "Epoch 129/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00129: loss did not improve from 9.02926\n",
      "Epoch 130/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00130: loss did not improve from 9.02926\n",
      "Epoch 131/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00131: loss did not improve from 9.02926\n",
      "Epoch 132/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00132: loss did not improve from 9.02926\n",
      "Epoch 133/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00133: loss did not improve from 9.02926\n",
      "Epoch 134/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00134: loss did not improve from 9.02926\n",
      "Epoch 135/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00135: loss did not improve from 9.02926\n",
      "Epoch 136/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00136: loss did not improve from 9.02926\n",
      "Epoch 137/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00137: loss did not improve from 9.02926\n",
      "Epoch 138/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00138: loss did not improve from 9.02926\n",
      "Epoch 139/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00139: loss did not improve from 9.02926\n",
      "Epoch 140/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00140: loss did not improve from 9.02926\n",
      "Epoch 141/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00141: loss did not improve from 9.02926\n",
      "Epoch 142/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00142: loss did not improve from 9.02926\n",
      "Epoch 143/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00143: loss did not improve from 9.02926\n",
      "Epoch 144/500\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00144: loss did not improve from 9.02926\n",
      "Epoch 145/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00145: loss did not improve from 9.02926\n",
      "Epoch 146/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00146: loss did not improve from 9.02926\n",
      "Epoch 147/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00147: loss did not improve from 9.02926\n",
      "Epoch 148/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00148: loss did not improve from 9.02926\n",
      "Epoch 149/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00149: loss did not improve from 9.02926\n",
      "Epoch 150/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00150: loss did not improve from 9.02926\n",
      "Epoch 151/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00151: loss did not improve from 9.02926\n",
      "Epoch 152/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00152: loss did not improve from 9.02926\n",
      "Epoch 153/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00153: loss did not improve from 9.02926\n",
      "Epoch 154/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00154: loss did not improve from 9.02926\n",
      "Epoch 155/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00155: loss did not improve from 9.02926\n",
      "Epoch 156/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00156: loss did not improve from 9.02926\n",
      "Epoch 157/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00157: loss did not improve from 9.02926\n",
      "Epoch 158/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00158: loss did not improve from 9.02926\n",
      "Epoch 159/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00159: loss did not improve from 9.02926\n",
      "Epoch 160/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00160: loss did not improve from 9.02926\n",
      "Epoch 161/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00161: loss did not improve from 9.02926\n",
      "Epoch 162/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00162: loss did not improve from 9.02926\n",
      "Epoch 163/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00163: loss did not improve from 9.02926\n",
      "Epoch 164/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00164: loss did not improve from 9.02926\n",
      "Epoch 165/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00165: loss did not improve from 9.02926\n",
      "Epoch 166/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00166: loss did not improve from 9.02926\n",
      "Epoch 167/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00167: loss did not improve from 9.02926\n",
      "Epoch 168/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00168: loss did not improve from 9.02926\n",
      "Epoch 169/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00169: loss did not improve from 9.02926\n",
      "Epoch 170/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00170: loss did not improve from 9.02926\n",
      "Epoch 171/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00171: loss did not improve from 9.02926\n",
      "Epoch 172/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00172: loss did not improve from 9.02926\n",
      "Epoch 173/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00173: loss did not improve from 9.02926\n",
      "Epoch 174/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00174: loss did not improve from 9.02926\n",
      "Epoch 175/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00175: loss did not improve from 9.02926\n",
      "Epoch 176/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00176: loss did not improve from 9.02926\n",
      "Epoch 177/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00177: loss did not improve from 9.02926\n",
      "Epoch 178/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00178: loss did not improve from 9.02926\n",
      "Epoch 179/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00179: loss did not improve from 9.02926\n",
      "Epoch 180/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00180: loss did not improve from 9.02926\n",
      "Epoch 181/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00181: loss did not improve from 9.02926\n",
      "Epoch 182/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00182: loss did not improve from 9.02926\n",
      "Epoch 183/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00183: loss did not improve from 9.02926\n",
      "Epoch 184/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00184: loss did not improve from 9.02926\n",
      "Epoch 185/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00185: loss did not improve from 9.02926\n",
      "Epoch 186/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00186: loss did not improve from 9.02926\n",
      "Epoch 187/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00187: loss did not improve from 9.02926\n",
      "Epoch 188/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00188: loss did not improve from 9.02926\n",
      "Epoch 189/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00189: loss did not improve from 9.02926\n",
      "Epoch 190/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00190: loss did not improve from 9.02926\n",
      "Epoch 191/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00191: loss did not improve from 9.02926\n",
      "Epoch 192/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00192: loss did not improve from 9.02926\n",
      "Epoch 193/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00193: loss did not improve from 9.02926\n",
      "Epoch 194/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00194: loss did not improve from 9.02926\n",
      "Epoch 195/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00195: loss did not improve from 9.02926\n",
      "Epoch 196/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00196: loss did not improve from 9.02926\n",
      "Epoch 197/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00197: loss did not improve from 9.02926\n",
      "Epoch 198/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00198: loss did not improve from 9.02926\n",
      "Epoch 199/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00199: loss did not improve from 9.02926\n",
      "Epoch 200/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00200: loss did not improve from 9.02926\n",
      "Epoch 201/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00201: loss did not improve from 9.02926\n",
      "Epoch 202/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00202: loss did not improve from 9.02926\n",
      "Epoch 203/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00203: loss did not improve from 9.02926\n",
      "Epoch 204/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00204: loss did not improve from 9.02926\n",
      "Epoch 205/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00205: loss did not improve from 9.02926\n",
      "Epoch 206/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00206: loss did not improve from 9.02926\n",
      "Epoch 207/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00207: loss did not improve from 9.02926\n",
      "Epoch 208/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00208: loss did not improve from 9.02926\n",
      "Epoch 209/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00209: loss did not improve from 9.02926\n",
      "Epoch 210/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00210: loss did not improve from 9.02926\n",
      "Epoch 211/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00211: loss did not improve from 9.02926\n",
      "Epoch 212/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00212: loss did not improve from 9.02926\n",
      "Epoch 213/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00213: loss did not improve from 9.02926\n",
      "Epoch 214/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00214: loss did not improve from 9.02926\n",
      "Epoch 215/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00215: loss did not improve from 9.02926\n",
      "Epoch 216/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00216: loss did not improve from 9.02926\n",
      "Epoch 217/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00217: loss did not improve from 9.02926\n",
      "Epoch 218/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00218: loss did not improve from 9.02926\n",
      "Epoch 219/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00219: loss did not improve from 9.02926\n",
      "Epoch 220/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00220: loss did not improve from 9.02926\n",
      "Epoch 221/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00221: loss did not improve from 9.02926\n",
      "Epoch 222/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00222: loss did not improve from 9.02926\n",
      "Epoch 223/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00223: loss did not improve from 9.02926\n",
      "Epoch 224/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00224: loss did not improve from 9.02926\n",
      "Epoch 225/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00225: loss did not improve from 9.02926\n",
      "Epoch 226/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00226: loss did not improve from 9.02926\n",
      "Epoch 227/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00227: loss did not improve from 9.02926\n",
      "Epoch 228/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00228: loss did not improve from 9.02926\n",
      "Epoch 229/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00229: loss did not improve from 9.02926\n",
      "Epoch 230/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00230: loss did not improve from 9.02926\n",
      "Epoch 231/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00231: loss did not improve from 9.02926\n",
      "Epoch 232/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00232: loss did not improve from 9.02926\n",
      "Epoch 233/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00233: loss did not improve from 9.02926\n",
      "Epoch 234/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00234: loss did not improve from 9.02926\n",
      "Epoch 235/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00235: loss did not improve from 9.02926\n",
      "Epoch 236/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00236: loss did not improve from 9.02926\n",
      "Epoch 237/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00237: loss did not improve from 9.02926\n",
      "Epoch 238/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00238: loss did not improve from 9.02926\n",
      "Epoch 239/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00239: loss did not improve from 9.02926\n",
      "Epoch 240/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00240: loss did not improve from 9.02926\n",
      "Epoch 241/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00241: loss did not improve from 9.02926\n",
      "Epoch 242/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00242: loss did not improve from 9.02926\n",
      "Epoch 243/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00243: loss did not improve from 9.02926\n",
      "Epoch 244/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00244: loss did not improve from 9.02926\n",
      "Epoch 245/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00245: loss did not improve from 9.02926\n",
      "Epoch 246/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00246: loss did not improve from 9.02926\n",
      "Epoch 247/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00247: loss did not improve from 9.02926\n",
      "Epoch 248/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00248: loss did not improve from 9.02926\n",
      "Epoch 249/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00249: loss did not improve from 9.02926\n",
      "Epoch 250/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00250: loss did not improve from 9.02926\n",
      "Epoch 251/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00251: loss did not improve from 9.02926\n",
      "Epoch 252/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00252: loss did not improve from 9.02926\n",
      "Epoch 253/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00253: loss did not improve from 9.02926\n",
      "Epoch 254/500\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00254: loss did not improve from 9.02926\n",
      "Epoch 255/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00255: loss did not improve from 9.02926\n",
      "Epoch 256/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00256: loss did not improve from 9.02926\n",
      "Epoch 257/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00257: loss did not improve from 9.02926\n",
      "Epoch 258/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00258: loss did not improve from 9.02926\n",
      "Epoch 259/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00259: loss did not improve from 9.02926\n",
      "Epoch 260/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00260: loss did not improve from 9.02926\n",
      "Epoch 261/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00261: loss did not improve from 9.02926\n",
      "Epoch 262/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00262: loss did not improve from 9.02926\n",
      "Epoch 263/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00263: loss did not improve from 9.02926\n",
      "Epoch 264/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00264: loss did not improve from 9.02926\n",
      "Epoch 265/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00265: loss did not improve from 9.02926\n",
      "Epoch 266/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00266: loss did not improve from 9.02926\n",
      "Epoch 267/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00267: loss did not improve from 9.02926\n",
      "Epoch 268/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00268: loss did not improve from 9.02926\n",
      "Epoch 269/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00269: loss did not improve from 9.02926\n",
      "Epoch 270/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00270: loss did not improve from 9.02926\n",
      "Epoch 271/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00271: loss did not improve from 9.02926\n",
      "Epoch 272/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00272: loss did not improve from 9.02926\n",
      "Epoch 273/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00273: loss did not improve from 9.02926\n",
      "Epoch 274/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00274: loss did not improve from 9.02926\n",
      "Epoch 275/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00275: loss did not improve from 9.02926\n",
      "Epoch 276/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00276: loss did not improve from 9.02926\n",
      "Epoch 277/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00277: loss did not improve from 9.02926\n",
      "Epoch 278/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00278: loss did not improve from 9.02926\n",
      "Epoch 279/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00279: loss did not improve from 9.02926\n",
      "Epoch 280/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00280: loss did not improve from 9.02926\n",
      "Epoch 281/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00281: loss did not improve from 9.02926\n",
      "Epoch 282/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00282: loss did not improve from 9.02926\n",
      "Epoch 283/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00283: loss did not improve from 9.02926\n",
      "Epoch 284/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00284: loss did not improve from 9.02926\n",
      "Epoch 285/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00285: loss did not improve from 9.02926\n",
      "Epoch 286/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00286: loss did not improve from 9.02926\n",
      "Epoch 287/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00287: loss did not improve from 9.02926\n",
      "Epoch 288/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00288: loss did not improve from 9.02926\n",
      "Epoch 289/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00289: loss did not improve from 9.02926\n",
      "Epoch 290/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00290: loss did not improve from 9.02926\n",
      "Epoch 291/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00291: loss did not improve from 9.02926\n",
      "Epoch 292/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00292: loss did not improve from 9.02926\n",
      "Epoch 293/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00293: loss did not improve from 9.02926\n",
      "Epoch 294/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00294: loss did not improve from 9.02926\n",
      "Epoch 295/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00295: loss did not improve from 9.02926\n",
      "Epoch 296/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00296: loss did not improve from 9.02926\n",
      "Epoch 297/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00297: loss did not improve from 9.02926\n",
      "Epoch 298/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00298: loss did not improve from 9.02926\n",
      "Epoch 299/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00299: loss did not improve from 9.02926\n",
      "Epoch 300/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00300: loss did not improve from 9.02926\n",
      "Epoch 301/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00301: loss did not improve from 9.02926\n",
      "Epoch 302/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00302: loss did not improve from 9.02926\n",
      "Epoch 303/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00303: loss did not improve from 9.02926\n",
      "Epoch 304/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00304: loss did not improve from 9.02926\n",
      "Epoch 305/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00305: loss did not improve from 9.02926\n",
      "Epoch 306/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00306: loss did not improve from 9.02926\n",
      "Epoch 307/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00307: loss did not improve from 9.02926\n",
      "Epoch 308/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00308: loss did not improve from 9.02926\n",
      "Epoch 309/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00309: loss did not improve from 9.02926\n",
      "Epoch 310/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00310: loss did not improve from 9.02926\n",
      "Epoch 311/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00311: loss did not improve from 9.02926\n",
      "Epoch 312/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00312: loss did not improve from 9.02926\n",
      "Epoch 313/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00313: loss did not improve from 9.02926\n",
      "Epoch 314/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00314: loss did not improve from 9.02926\n",
      "Epoch 315/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00315: loss did not improve from 9.02926\n",
      "Epoch 316/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00316: loss did not improve from 9.02926\n",
      "Epoch 317/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00317: loss did not improve from 9.02926\n",
      "Epoch 318/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00318: loss did not improve from 9.02926\n",
      "Epoch 319/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00319: loss did not improve from 9.02926\n",
      "Epoch 320/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00320: loss did not improve from 9.02926\n",
      "Epoch 321/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00321: loss did not improve from 9.02926\n",
      "Epoch 322/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00322: loss did not improve from 9.02926\n",
      "Epoch 323/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00323: loss did not improve from 9.02926\n",
      "Epoch 324/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00324: loss did not improve from 9.02926\n",
      "Epoch 325/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00325: loss did not improve from 9.02926\n",
      "Epoch 326/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00326: loss did not improve from 9.02926\n",
      "Epoch 327/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00327: loss did not improve from 9.02926\n",
      "Epoch 328/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00328: loss did not improve from 9.02926\n",
      "Epoch 329/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00329: loss did not improve from 9.02926\n",
      "Epoch 330/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00330: loss did not improve from 9.02926\n",
      "Epoch 331/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00331: loss did not improve from 9.02926\n",
      "Epoch 332/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00332: loss did not improve from 9.02926\n",
      "Epoch 333/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00333: loss did not improve from 9.02926\n",
      "Epoch 334/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00334: loss did not improve from 9.02926\n",
      "Epoch 335/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00335: loss did not improve from 9.02926\n",
      "Epoch 336/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00336: loss did not improve from 9.02926\n",
      "Epoch 337/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00337: loss did not improve from 9.02926\n",
      "Epoch 338/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00338: loss did not improve from 9.02926\n",
      "Epoch 339/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00339: loss did not improve from 9.02926\n",
      "Epoch 340/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00340: loss did not improve from 9.02926\n",
      "Epoch 341/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00341: loss did not improve from 9.02926\n",
      "Epoch 342/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00342: loss did not improve from 9.02926\n",
      "Epoch 343/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00343: loss did not improve from 9.02926\n",
      "Epoch 344/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00344: loss did not improve from 9.02926\n",
      "Epoch 345/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00345: loss did not improve from 9.02926\n",
      "Epoch 346/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00346: loss did not improve from 9.02926\n",
      "Epoch 347/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00347: loss did not improve from 9.02926\n",
      "Epoch 348/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00348: loss did not improve from 9.02926\n",
      "Epoch 349/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00349: loss did not improve from 9.02926\n",
      "Epoch 350/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00350: loss did not improve from 9.02926\n",
      "Epoch 351/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00351: loss did not improve from 9.02926\n",
      "Epoch 352/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00352: loss did not improve from 9.02926\n",
      "Epoch 353/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00353: loss did not improve from 9.02926\n",
      "Epoch 354/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00354: loss did not improve from 9.02926\n",
      "Epoch 355/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00355: loss did not improve from 9.02926\n",
      "Epoch 356/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00356: loss did not improve from 9.02926\n",
      "Epoch 357/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00357: loss did not improve from 9.02926\n",
      "Epoch 358/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00358: loss did not improve from 9.02926\n",
      "Epoch 359/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00359: loss did not improve from 9.02926\n",
      "Epoch 360/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00360: loss did not improve from 9.02926\n",
      "Epoch 361/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00361: loss did not improve from 9.02926\n",
      "Epoch 362/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00362: loss did not improve from 9.02926\n",
      "Epoch 363/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00363: loss did not improve from 9.02926\n",
      "Epoch 364/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00364: loss did not improve from 9.02926\n",
      "Epoch 365/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00365: loss did not improve from 9.02926\n",
      "Epoch 366/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00366: loss did not improve from 9.02926\n",
      "Epoch 367/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00367: loss did not improve from 9.02926\n",
      "Epoch 368/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00368: loss did not improve from 9.02926\n",
      "Epoch 369/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00369: loss did not improve from 9.02926\n",
      "Epoch 370/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00370: loss did not improve from 9.02926\n",
      "Epoch 371/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00371: loss did not improve from 9.02926\n",
      "Epoch 372/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00372: loss did not improve from 9.02926\n",
      "Epoch 373/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00373: loss did not improve from 9.02926\n",
      "Epoch 374/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00374: loss did not improve from 9.02926\n",
      "Epoch 375/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00375: loss did not improve from 9.02926\n",
      "Epoch 376/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00376: loss did not improve from 9.02926\n",
      "Epoch 377/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00377: loss did not improve from 9.02926\n",
      "Epoch 378/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00378: loss did not improve from 9.02926\n",
      "Epoch 379/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00379: loss did not improve from 9.02926\n",
      "Epoch 380/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00380: loss did not improve from 9.02926\n",
      "Epoch 381/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00381: loss did not improve from 9.02926\n",
      "Epoch 382/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00382: loss did not improve from 9.02926\n",
      "Epoch 383/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00383: loss did not improve from 9.02926\n",
      "Epoch 384/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00384: loss did not improve from 9.02926\n",
      "Epoch 385/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00385: loss did not improve from 9.02926\n",
      "Epoch 386/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00386: loss did not improve from 9.02926\n",
      "Epoch 387/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00387: loss did not improve from 9.02926\n",
      "Epoch 388/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00388: loss did not improve from 9.02926\n",
      "Epoch 389/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00389: loss did not improve from 9.02926\n",
      "Epoch 390/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00390: loss did not improve from 9.02926\n",
      "Epoch 391/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00391: loss did not improve from 9.02926\n",
      "Epoch 392/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00392: loss did not improve from 9.02926\n",
      "Epoch 393/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00393: loss did not improve from 9.02926\n",
      "Epoch 394/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00394: loss did not improve from 9.02926\n",
      "Epoch 395/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00395: loss did not improve from 9.02926\n",
      "Epoch 396/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00396: loss did not improve from 9.02926\n",
      "Epoch 397/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00397: loss did not improve from 9.02926\n",
      "Epoch 398/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00398: loss did not improve from 9.02926\n",
      "Epoch 399/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0484\n",
      "\n",
      "Epoch 00399: loss did not improve from 9.02926\n",
      "Epoch 400/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00400: loss did not improve from 9.02926\n",
      "Epoch 401/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00401: loss did not improve from 9.02926\n",
      "Epoch 402/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00402: loss did not improve from 9.02926\n",
      "Epoch 403/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00403: loss did not improve from 9.02926\n",
      "Epoch 404/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00404: loss did not improve from 9.02926\n",
      "Epoch 405/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00405: loss did not improve from 9.02926\n",
      "Epoch 406/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00406: loss did not improve from 9.02926\n",
      "Epoch 407/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00407: loss did not improve from 9.02926\n",
      "Epoch 408/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00408: loss did not improve from 9.02926\n",
      "Epoch 409/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00409: loss did not improve from 9.02926\n",
      "Epoch 410/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00410: loss did not improve from 9.02926\n",
      "Epoch 411/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00411: loss did not improve from 9.02926\n",
      "Epoch 412/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00412: loss did not improve from 9.02926\n",
      "Epoch 413/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00413: loss did not improve from 9.02926\n",
      "Epoch 414/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00414: loss did not improve from 9.02926\n",
      "Epoch 415/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00415: loss did not improve from 9.02926\n",
      "Epoch 416/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00416: loss did not improve from 9.02926\n",
      "Epoch 417/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00417: loss did not improve from 9.02926\n",
      "Epoch 418/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00418: loss did not improve from 9.02926\n",
      "Epoch 419/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00419: loss did not improve from 9.02926\n",
      "Epoch 420/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00420: loss did not improve from 9.02926\n",
      "Epoch 421/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00421: loss did not improve from 9.02926\n",
      "Epoch 422/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00422: loss did not improve from 9.02926\n",
      "Epoch 423/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00423: loss did not improve from 9.02926\n",
      "Epoch 424/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00424: loss did not improve from 9.02926\n",
      "Epoch 425/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00425: loss did not improve from 9.02926\n",
      "Epoch 426/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00426: loss did not improve from 9.02926\n",
      "Epoch 427/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00427: loss did not improve from 9.02926\n",
      "Epoch 428/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00428: loss did not improve from 9.02926\n",
      "Epoch 429/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00429: loss did not improve from 9.02926\n",
      "Epoch 430/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00430: loss did not improve from 9.02926\n",
      "Epoch 431/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00431: loss did not improve from 9.02926\n",
      "Epoch 432/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00432: loss did not improve from 9.02926\n",
      "Epoch 433/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00433: loss did not improve from 9.02926\n",
      "Epoch 434/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00434: loss did not improve from 9.02926\n",
      "Epoch 435/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00435: loss did not improve from 9.02926\n",
      "Epoch 436/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00436: loss did not improve from 9.02926\n",
      "Epoch 437/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00437: loss did not improve from 9.02926\n",
      "Epoch 438/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00438: loss did not improve from 9.02926\n",
      "Epoch 439/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00439: loss did not improve from 9.02926\n",
      "Epoch 440/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00440: loss did not improve from 9.02926\n",
      "Epoch 441/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00441: loss did not improve from 9.02926\n",
      "Epoch 442/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00442: loss did not improve from 9.02926\n",
      "Epoch 443/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00443: loss did not improve from 9.02926\n",
      "Epoch 444/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00444: loss did not improve from 9.02926\n",
      "Epoch 445/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00445: loss did not improve from 9.02926\n",
      "Epoch 446/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00446: loss did not improve from 9.02926\n",
      "Epoch 447/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00447: loss did not improve from 9.02926\n",
      "Epoch 448/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00448: loss did not improve from 9.02926\n",
      "Epoch 449/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00449: loss did not improve from 9.02926\n",
      "Epoch 450/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00450: loss did not improve from 9.02926\n",
      "Epoch 451/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00451: loss did not improve from 9.02926\n",
      "Epoch 452/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00452: loss did not improve from 9.02926\n",
      "Epoch 453/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00453: loss did not improve from 9.02926\n",
      "Epoch 454/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00454: loss did not improve from 9.02926\n",
      "Epoch 455/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00455: loss did not improve from 9.02926\n",
      "Epoch 456/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00456: loss did not improve from 9.02926\n",
      "Epoch 457/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00457: loss did not improve from 9.02926\n",
      "Epoch 458/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00458: loss did not improve from 9.02926\n",
      "Epoch 459/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00459: loss did not improve from 9.02926\n",
      "Epoch 460/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00460: loss did not improve from 9.02926\n",
      "Epoch 461/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00461: loss did not improve from 9.02926\n",
      "Epoch 462/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00462: loss did not improve from 9.02926\n",
      "Epoch 463/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00463: loss did not improve from 9.02926\n",
      "Epoch 464/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00464: loss did not improve from 9.02926\n",
      "Epoch 465/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00465: loss did not improve from 9.02926\n",
      "Epoch 466/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00466: loss did not improve from 9.02926\n",
      "Epoch 467/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00467: loss did not improve from 9.02926\n",
      "Epoch 468/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00468: loss did not improve from 9.02926\n",
      "Epoch 469/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00469: loss did not improve from 9.02926\n",
      "Epoch 470/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00470: loss did not improve from 9.02926\n",
      "Epoch 471/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00471: loss did not improve from 9.02926\n",
      "Epoch 472/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00472: loss did not improve from 9.02926\n",
      "Epoch 473/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00473: loss did not improve from 9.02926\n",
      "Epoch 474/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00474: loss did not improve from 9.02926\n",
      "Epoch 475/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00475: loss did not improve from 9.02926\n",
      "Epoch 476/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00476: loss did not improve from 9.02926\n",
      "Epoch 477/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00477: loss did not improve from 9.02926\n",
      "Epoch 478/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00478: loss did not improve from 9.02926\n",
      "Epoch 479/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00479: loss did not improve from 9.02926\n",
      "Epoch 480/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00480: loss did not improve from 9.02926\n",
      "Epoch 481/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00481: loss did not improve from 9.02926\n",
      "Epoch 482/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00482: loss did not improve from 9.02926\n",
      "Epoch 483/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00483: loss did not improve from 9.02926\n",
      "Epoch 484/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00484: loss did not improve from 9.02926\n",
      "Epoch 485/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00485: loss did not improve from 9.02926\n",
      "Epoch 486/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00486: loss did not improve from 9.02926\n",
      "Epoch 487/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00487: loss did not improve from 9.02926\n",
      "Epoch 488/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00488: loss did not improve from 9.02926\n",
      "Epoch 489/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00489: loss did not improve from 9.02926\n",
      "Epoch 490/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00490: loss did not improve from 9.02926\n",
      "Epoch 491/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00491: loss did not improve from 9.02926\n",
      "Epoch 492/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00492: loss did not improve from 9.02926\n",
      "Epoch 493/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00493: loss did not improve from 9.02926\n",
      "Epoch 494/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00494: loss did not improve from 9.02926\n",
      "Epoch 495/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00495: loss did not improve from 9.02926\n",
      "Epoch 496/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00496: loss did not improve from 9.02926\n",
      "Epoch 497/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00497: loss did not improve from 9.02926\n",
      "Epoch 498/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00498: loss did not improve from 9.02926\n",
      "Epoch 499/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00499: loss did not improve from 9.02926\n",
      "Epoch 500/500\n",
      "8444/8444 [==============================] - 51s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00500: loss did not improve from 9.02926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3df1541e90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Weights-LSTM-improvement-{epoch:02d}-{loss:.4f}-bigger.hdfs\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "\n",
    "model_500.fit(X, y, epochs=500, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8544\n",
      "Total Vocab:  8534\n",
      "Total Patterns:  8444\n",
      "Seed:\n",
      "\" excruciatingly unfunny and pitifully unromantic .the film 's thoroughly recycled plot and tiresome jokes ... drag the movie down .it does n't offer audiences any way of gripping what its point is , or even its attitude toward its subject .this kiddie-oriented stinker is so bad that i even caught the gum stuck under my seat trying to sneak out of the theaterthough impostor deviously adopts the guise of a modern motion picture , it too is a bomb .cox offers plenty of glimpses at existing photos , but there are no movies of nijinsky , so instead the director treats us to an aimless hodgepodge .every note rings false .when the screenwriter responsible for one of the worst movies of one year directs an equally miserable film the following year , you 'd have a hard time believing it was just coincidence .it never rises to its clever what-if concept .admirably ambitious but self-indulgent .this story of unrequited love does n't sustain interest beyond the first half-hour .this angst-ridden territory was covered earlier and much better in ordinary people .the entire film is one big excuse to play one lewd scene after another .about half of them are funny , a few are sexy and none are useful in telling the story , which is paper-thin and decidedly unoriginal .a big , loud , bang-the-drum bore .( less a movie than ) an appalling , odoriferous thing ... so rotten in almost every single facet of production that you 'll want to crawl up your own *** in embarrassment .the concept behind kung pow : enter the fist is hilarious .it 's too bad nothing else is .hardman is a grating , mannered onscreen presence , which is especially unfortunate in light of the fine work done by most of the rest of her cast .el crimen del padre amaro would likely be most effective if used as a tool to rally anti-catholic protestors .interesting and thoroughly unfaithful version of carmena serious movie with serious ideas .but seriously , folks , it does n't work .there 's nothing exactly wrong here , but there 's not nearly enough that 's right .the action here is unusually tame , the characters are too simplistic to maintain interest , and the plot offers few surprises .i could n't help but feel the wasted potential of this slapstick comedy .what madonna does here ca n't properly be called acting -- more accurately , it 's moving and it 's talking and it 's occasionally gesturing , sometimes all at once .( a ) painfully flat gross-out comedy ...even if you 're an elvis person , you wo n't find anything to get excited about on this dvd .the movie certainly has its share of clever moments and biting dialogue , but there 's just not much lurking below its abstract surface .it 's bedeviled by labored writing and slack direction .i 'm sure there 's a teenage boy out there somewhere who 's dying for this kind of entertainment .the tuxedo miscalculates badly by forcing the star to play second fiddle to the dull effects that allow the suit to come to life .the film is a confusing melange of tones and styles , one moment a romantic trifle and the next a turgid drama .obviously , a lot of people wasted a lot of their time ( including mine ) on something very inconsequential .there 's a little violence and lots of sex in a bid to hold our attention , but it grows monotonous after a while , as do joan and philip 's repetitive arguments , schemes and treachery .it drowns in sap .deliberately and devotedly constructed , far from heaven is too picture postcard perfect , too neat and new pin-like , too obviously a recreation to resonate .it 's rare that a movie can be as intelligent as this one is in every regard except its storyline ; everything that 's good is ultimately scuttled by a plot that 's just too boring and obvious .i 'm giving it thumbs down due to the endlessly repetitive scenes of embarrassment .there 's got to be a more graceful way of portraying the devastation of this disease .the good thing -- the only good thing -- about extreme ops is that it 's so inane that it gave me plenty of time to ponder my thanksgiving to-do list .the modern-day characters are nowhere near as vivid as the 19th-century ones .blessed with a searing lead performance by ryan gosling ( murder by numbers ) , the movie is powerful and provocative .it 's also built on a faulty premise , one it follows into melodrama and silliness .uneven performances and a spotty script add up to a biting satire that has no teeth .director jay russell stomps in hobnail boots over natalie babbitt 's gentle , endearing 1975 children 's novel .benigni 's pinocchio is extremely straight and mind-numbingly stilted , its episodic pacing keeping the film from developing any storytelling flow .the troubling thing about clockstoppers is that it does n't make any sense .with its paint fights , motorized scooter chases and dewy-eyed sentiment , it 's a pretty listless collection of kid-movie clichs .mostly the film is just hectic and homiletic : two parts exhausting men in black mayhem to one part family values .kicks off with an inauspicious premise , mopes through a dreary tract of virtually plotless meanderings and then ends with a whimper .a rote exercise in both animation and storytelling .the material and the production itself are little more than routine .the movie 's major and most devastating flaw is its reliance on formula , though , and it 's quite enough to lessen the overall impact the movie could have had .not even steven spielberg has dreamed up such blatant and sickening product placement in a movie .it 's all surface psychodramatics .the mothman prophecies , which is mostly a bore , seems to exist only for its climactic setpiece .that frenetic spectacle ( on the tv show ) has usually been leavened by a charm that 's conspicuously missing from the girls ' big-screen blowout .kitschy , flashy , overlong soap opera .for all the time we spend with these people , we never really get inside of them .yet another arnold vehicle that fails to make adequate use of his particular talents .sandra bullock , despite downplaying her good looks , carries a little too much ai n't - she-cute baggage into her lead role as a troubled and determined homicide cop to quite pull off the heavy stuff .an undistinguished attempt to make a classic theater piece cinematic .too many scenarios in which the hero might have an opportunity to triumphantly sermonize , and too few that allow us to wonder for ourselves if things will turn out okay .there is simply not enough of interest onscreen to sustain its seventy-minute running time .a wordy wisp of a comedy .broomfield 's style of journalism is hardly journalism at all , and even those with an avid interest in the subject will grow impatient .( seagal 's ) strenuous attempt at a change in expression could very well clinch him this year 's razzie .has the disjointed feel of a bunch of strung-together tv episodes .a series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .ozpetek offers an aids subtext , skims over the realities of gay sex , and presents yet another tired old vision of the gay community as an all-inclusive world where uptight , middle class bores like antonia can feel good about themselves .a dopey movie clothed in excess layers of hipness .the sweetest thing is expressly for idiots who do n't care what kind of sewage they shovel into their mental gullets to simulate sustenance .sinks so low in a poorly played game of absurd plot twists , idiotic court maneuvers and stupid characters that even freeman ca n't save it .i realized that no matter how fantastic reign of fire looked , its story was making no sense at all .it made me realize that we really have n't had a good cheesy b-movie playing in theaters since ... well ... since last week 's reign of fire .some movies were made for the big screen , some for the small screen , and some , like ballistic : ecks vs. sever , were made for the palm screen .sc2 is an autopilot hollywood concoction lacking in imagination and authentic christmas spirit , yet it 's geared toward an audience full of masters of both .after all the big build-up , the payoff for the audience , as well as the characters , is messy , murky , unsatisfying .seems content to dog-paddle in the mediocre end of the pool , and it 's a sad , sick sight .it 's refreshing that someone understands the need for the bad boy ; diesel , with his brawny frame and cool , composed delivery , fits the bill perfectly .if all of eight legged freaks was as entertaining as the final hour , i would have no problem giving it an unqualified recommendation .suffers from a flat script and a low budget .there are deeply religious and spiritual people in this world who would argue that entering a church , synagogue or temple does n't mean you have to check your brain at the door .the same should go for movie theaters .seemingly a vehicle to showcase the canadian 's inane ramblings , stealing harvard is a smorgasbord of soliloquies about nothing delivered by the former mr. drew barrymore .the film tries to touch on spousal abuse but veers off course and becomes just another revenge film .as it stands it 's an opera movie for the buffs .this franchise has not spawned a single good film .the crap continues .its inescapable absurdities are tantamount to insulting the intelligence of anyone who has n't been living under a rock ( since sept. 11 ) .high drama , disney-style - a wing and a prayer and a hunky has-been pursuing his castle in the sky .like its script , which nurses plot holes gaping enough to pilot an entire olympic swim team through , the characters in swimfan seem motivated by nothing short of dull , brain-deadening hangover .one big blustery movie where nothing really happens .when it comes out on video , then it 's the perfect cure for insomnia .like a comedian who starts off promisingly but then proceeds to flop , comedian runs out of steam after a half hour .the pairing does sound promising in theory ... but their lack of chemistry makes eddie murphy and robert deniro in showtime look like old , familiar vaudeville partners .director chris eyre is going through the paces again with his usual high melodramatic style of filmmaking .as it stands , there 's some fine sex onscreen , and some tense arguing , but not a whole lot more . \"\n",
      "one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "\n",
    "import sys\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model_500 = Sequential()\n",
    "model_500.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model_500.add(Dropout(0.2))\n",
    "model_500.add(LSTM(256))\n",
    "model_500.add(Dropout(0.2))\n",
    "model_500.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Load the network weights\n",
    "filename = \"Weights-LSTM-improvement-80-9.0447-bigger.hdfs\"\n",
    "#filename = \"Weights-LSTM-improvement-44-1.3592-bigger.hdfs\"\n",
    "model_500.load_weights(filename)\n",
    "\n",
    "model_500.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(70):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model_500.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 epochs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model_1000 = Sequential()\n",
    "model_1000.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model_1000.add(Dropout(0.2))\n",
    "model_1000.add(LSTM(256))\n",
    "model_1000.add(Dropout(0.2))\n",
    "model_1000.add(Dense(y.shape[1], activation='softmax'))\n",
    "model_1000.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0053\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.00526, saving model to Weights-LSTM-improvement-01-9.0053-bigger.hdfs\n",
      "Epoch 2/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0621\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.00526\n",
      "Epoch 3/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0938\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.00526\n",
      "Epoch 4/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0683\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.00526\n",
      "Epoch 5/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0542\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.00526\n",
      "Epoch 6/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0523\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.00526\n",
      "Epoch 7/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0516\n",
      "\n",
      "Epoch 00007: loss did not improve from 9.00526\n",
      "Epoch 8/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0511\n",
      "\n",
      "Epoch 00008: loss did not improve from 9.00526\n",
      "Epoch 9/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0506\n",
      "\n",
      "Epoch 00009: loss did not improve from 9.00526\n",
      "Epoch 10/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0502\n",
      "\n",
      "Epoch 00010: loss did not improve from 9.00526\n",
      "Epoch 11/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0497\n",
      "\n",
      "Epoch 00011: loss did not improve from 9.00526\n",
      "Epoch 12/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0494\n",
      "\n",
      "Epoch 00012: loss did not improve from 9.00526\n",
      "Epoch 13/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0491\n",
      "\n",
      "Epoch 00013: loss did not improve from 9.00526\n",
      "Epoch 14/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0488\n",
      "\n",
      "Epoch 00014: loss did not improve from 9.00526\n",
      "Epoch 15/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0485\n",
      "\n",
      "Epoch 00015: loss did not improve from 9.00526\n",
      "Epoch 16/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0483\n",
      "\n",
      "Epoch 00016: loss did not improve from 9.00526\n",
      "Epoch 17/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0481\n",
      "\n",
      "Epoch 00017: loss did not improve from 9.00526\n",
      "Epoch 18/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0479\n",
      "\n",
      "Epoch 00018: loss did not improve from 9.00526\n",
      "Epoch 19/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0477\n",
      "\n",
      "Epoch 00019: loss did not improve from 9.00526\n",
      "Epoch 20/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0475\n",
      "\n",
      "Epoch 00020: loss did not improve from 9.00526\n",
      "Epoch 21/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0473\n",
      "\n",
      "Epoch 00021: loss did not improve from 9.00526\n",
      "Epoch 22/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0472\n",
      "\n",
      "Epoch 00022: loss did not improve from 9.00526\n",
      "Epoch 23/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0470\n",
      "\n",
      "Epoch 00023: loss did not improve from 9.00526\n",
      "Epoch 24/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0469\n",
      "\n",
      "Epoch 00024: loss did not improve from 9.00526\n",
      "Epoch 25/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0468\n",
      "\n",
      "Epoch 00025: loss did not improve from 9.00526\n",
      "Epoch 26/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0466\n",
      "\n",
      "Epoch 00026: loss did not improve from 9.00526\n",
      "Epoch 27/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0465\n",
      "\n",
      "Epoch 00027: loss did not improve from 9.00526\n",
      "Epoch 28/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0464\n",
      "\n",
      "Epoch 00028: loss did not improve from 9.00526\n",
      "Epoch 29/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0463\n",
      "\n",
      "Epoch 00029: loss did not improve from 9.00526\n",
      "Epoch 30/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0462\n",
      "\n",
      "Epoch 00030: loss did not improve from 9.00526\n",
      "Epoch 31/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0462\n",
      "\n",
      "Epoch 00031: loss did not improve from 9.00526\n",
      "Epoch 32/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0461\n",
      "\n",
      "Epoch 00032: loss did not improve from 9.00526\n",
      "Epoch 33/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0460\n",
      "\n",
      "Epoch 00033: loss did not improve from 9.00526\n",
      "Epoch 34/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0459\n",
      "\n",
      "Epoch 00034: loss did not improve from 9.00526\n",
      "Epoch 35/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0459\n",
      "\n",
      "Epoch 00035: loss did not improve from 9.00526\n",
      "Epoch 36/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0458\n",
      "\n",
      "Epoch 00036: loss did not improve from 9.00526\n",
      "Epoch 37/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0457\n",
      "\n",
      "Epoch 00037: loss did not improve from 9.00526\n",
      "Epoch 38/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0457\n",
      "\n",
      "Epoch 00038: loss did not improve from 9.00526\n",
      "Epoch 39/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0456\n",
      "\n",
      "Epoch 00039: loss did not improve from 9.00526\n",
      "Epoch 40/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0456\n",
      "\n",
      "Epoch 00040: loss did not improve from 9.00526\n",
      "Epoch 41/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00041: loss did not improve from 9.00526\n",
      "Epoch 42/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00042: loss did not improve from 9.00526\n",
      "Epoch 43/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0455\n",
      "\n",
      "Epoch 00043: loss did not improve from 9.00526\n",
      "Epoch 44/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00044: loss did not improve from 9.00526\n",
      "Epoch 45/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0454\n",
      "\n",
      "Epoch 00045: loss did not improve from 9.00526\n",
      "Epoch 46/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00046: loss did not improve from 9.00526\n",
      "Epoch 47/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00047: loss did not improve from 9.00526\n",
      "Epoch 48/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0453\n",
      "\n",
      "Epoch 00048: loss did not improve from 9.00526\n",
      "Epoch 49/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00049: loss did not improve from 9.00526\n",
      "Epoch 50/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00050: loss did not improve from 9.00526\n",
      "Epoch 51/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0452\n",
      "\n",
      "Epoch 00051: loss did not improve from 9.00526\n",
      "Epoch 52/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00052: loss did not improve from 9.00526\n",
      "Epoch 53/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00053: loss did not improve from 9.00526\n",
      "Epoch 54/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00054: loss did not improve from 9.00526\n",
      "Epoch 55/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0451\n",
      "\n",
      "Epoch 00055: loss did not improve from 9.00526\n",
      "Epoch 56/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00056: loss did not improve from 9.00526\n",
      "Epoch 57/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00057: loss did not improve from 9.00526\n",
      "Epoch 58/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00058: loss did not improve from 9.00526\n",
      "Epoch 59/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00059: loss did not improve from 9.00526\n",
      "Epoch 60/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00060: loss did not improve from 9.00526\n",
      "Epoch 61/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0450\n",
      "\n",
      "Epoch 00061: loss did not improve from 9.00526\n",
      "Epoch 62/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00062: loss did not improve from 9.00526\n",
      "Epoch 63/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00063: loss did not improve from 9.00526\n",
      "Epoch 64/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00064: loss did not improve from 9.00526\n",
      "Epoch 65/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00065: loss did not improve from 9.00526\n",
      "Epoch 66/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00066: loss did not improve from 9.00526\n",
      "Epoch 67/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0449\n",
      "\n",
      "Epoch 00067: loss did not improve from 9.00526\n",
      "Epoch 68/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00068: loss did not improve from 9.00526\n",
      "Epoch 69/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00069: loss did not improve from 9.00526\n",
      "Epoch 70/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00070: loss did not improve from 9.00526\n",
      "Epoch 71/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00071: loss did not improve from 9.00526\n",
      "Epoch 72/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00072: loss did not improve from 9.00526\n",
      "Epoch 73/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00073: loss did not improve from 9.00526\n",
      "Epoch 74/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00074: loss did not improve from 9.00526\n",
      "Epoch 75/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00075: loss did not improve from 9.00526\n",
      "Epoch 76/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0448\n",
      "\n",
      "Epoch 00076: loss did not improve from 9.00526\n",
      "Epoch 77/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00077: loss did not improve from 9.00526\n",
      "Epoch 78/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00078: loss did not improve from 9.00526\n",
      "Epoch 79/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00079: loss did not improve from 9.00526\n",
      "Epoch 80/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00080: loss did not improve from 9.00526\n",
      "Epoch 81/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00081: loss did not improve from 9.00526\n",
      "Epoch 82/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00082: loss did not improve from 9.00526\n",
      "Epoch 83/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00083: loss did not improve from 9.00526\n",
      "Epoch 84/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00084: loss did not improve from 9.00526\n",
      "Epoch 85/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00085: loss did not improve from 9.00526\n",
      "Epoch 86/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00086: loss did not improve from 9.00526\n",
      "Epoch 87/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00087: loss did not improve from 9.00526\n",
      "Epoch 88/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00088: loss did not improve from 9.00526\n",
      "Epoch 89/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0447\n",
      "\n",
      "Epoch 00089: loss did not improve from 9.00526\n",
      "Epoch 90/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00090: loss did not improve from 9.00526\n",
      "Epoch 91/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00091: loss did not improve from 9.00526\n",
      "Epoch 92/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00092: loss did not improve from 9.00526\n",
      "Epoch 93/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00093: loss did not improve from 9.00526\n",
      "Epoch 94/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00094: loss did not improve from 9.00526\n",
      "Epoch 95/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00095: loss did not improve from 9.00526\n",
      "Epoch 96/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00096: loss did not improve from 9.00526\n",
      "Epoch 97/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00097: loss did not improve from 9.00526\n",
      "Epoch 98/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00098: loss did not improve from 9.00526\n",
      "Epoch 99/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00099: loss did not improve from 9.00526\n",
      "Epoch 100/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00100: loss did not improve from 9.00526\n",
      "Epoch 101/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00101: loss did not improve from 9.00526\n",
      "Epoch 102/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00102: loss did not improve from 9.00526\n",
      "Epoch 103/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00103: loss did not improve from 9.00526\n",
      "Epoch 104/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00104: loss did not improve from 9.00526\n",
      "Epoch 105/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00105: loss did not improve from 9.00526\n",
      "Epoch 106/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00106: loss did not improve from 9.00526\n",
      "Epoch 107/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00107: loss did not improve from 9.00526\n",
      "Epoch 108/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00108: loss did not improve from 9.00526\n",
      "Epoch 109/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00109: loss did not improve from 9.00526\n",
      "Epoch 110/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00110: loss did not improve from 9.00526\n",
      "Epoch 111/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00111: loss did not improve from 9.00526\n",
      "Epoch 112/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00112: loss did not improve from 9.00526\n",
      "Epoch 113/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00113: loss did not improve from 9.00526\n",
      "Epoch 114/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00114: loss did not improve from 9.00526\n",
      "Epoch 115/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00115: loss did not improve from 9.00526\n",
      "Epoch 116/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00116: loss did not improve from 9.00526\n",
      "Epoch 117/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00117: loss did not improve from 9.00526\n",
      "Epoch 118/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00118: loss did not improve from 9.00526\n",
      "Epoch 119/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00119: loss did not improve from 9.00526\n",
      "Epoch 120/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0446\n",
      "\n",
      "Epoch 00120: loss did not improve from 9.00526\n",
      "Epoch 121/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00121: loss did not improve from 9.00526\n",
      "Epoch 122/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00122: loss did not improve from 9.00526\n",
      "Epoch 123/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00123: loss did not improve from 9.00526\n",
      "Epoch 124/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00124: loss did not improve from 9.00526\n",
      "Epoch 125/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00125: loss did not improve from 9.00526\n",
      "Epoch 126/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00126: loss did not improve from 9.00526\n",
      "Epoch 127/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00127: loss did not improve from 9.00526\n",
      "Epoch 128/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00128: loss did not improve from 9.00526\n",
      "Epoch 129/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00129: loss did not improve from 9.00526\n",
      "Epoch 130/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00130: loss did not improve from 9.00526\n",
      "Epoch 131/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00131: loss did not improve from 9.00526\n",
      "Epoch 132/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00132: loss did not improve from 9.00526\n",
      "Epoch 133/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00133: loss did not improve from 9.00526\n",
      "Epoch 134/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00134: loss did not improve from 9.00526\n",
      "Epoch 135/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00135: loss did not improve from 9.00526\n",
      "Epoch 136/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00136: loss did not improve from 9.00526\n",
      "Epoch 137/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00137: loss did not improve from 9.00526\n",
      "Epoch 138/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00138: loss did not improve from 9.00526\n",
      "Epoch 139/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00139: loss did not improve from 9.00526\n",
      "Epoch 140/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00140: loss did not improve from 9.00526\n",
      "Epoch 141/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00141: loss did not improve from 9.00526\n",
      "Epoch 142/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00142: loss did not improve from 9.00526\n",
      "Epoch 143/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00143: loss did not improve from 9.00526\n",
      "Epoch 144/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00144: loss did not improve from 9.00526\n",
      "Epoch 145/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00145: loss did not improve from 9.00526\n",
      "Epoch 146/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00146: loss did not improve from 9.00526\n",
      "Epoch 147/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00147: loss did not improve from 9.00526\n",
      "Epoch 148/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00148: loss did not improve from 9.00526\n",
      "Epoch 149/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00149: loss did not improve from 9.00526\n",
      "Epoch 150/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00150: loss did not improve from 9.00526\n",
      "Epoch 151/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00151: loss did not improve from 9.00526\n",
      "Epoch 152/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00152: loss did not improve from 9.00526\n",
      "Epoch 153/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00153: loss did not improve from 9.00526\n",
      "Epoch 154/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00154: loss did not improve from 9.00526\n",
      "Epoch 155/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00155: loss did not improve from 9.00526\n",
      "Epoch 156/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00156: loss did not improve from 9.00526\n",
      "Epoch 157/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00157: loss did not improve from 9.00526\n",
      "Epoch 158/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00158: loss did not improve from 9.00526\n",
      "Epoch 159/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00159: loss did not improve from 9.00526\n",
      "Epoch 160/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00160: loss did not improve from 9.00526\n",
      "Epoch 161/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00161: loss did not improve from 9.00526\n",
      "Epoch 162/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00162: loss did not improve from 9.00526\n",
      "Epoch 163/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00163: loss did not improve from 9.00526\n",
      "Epoch 164/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00164: loss did not improve from 9.00526\n",
      "Epoch 165/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00165: loss did not improve from 9.00526\n",
      "Epoch 166/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00166: loss did not improve from 9.00526\n",
      "Epoch 167/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00167: loss did not improve from 9.00526\n",
      "Epoch 168/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00168: loss did not improve from 9.00526\n",
      "Epoch 169/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00169: loss did not improve from 9.00526\n",
      "Epoch 170/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00170: loss did not improve from 9.00526\n",
      "Epoch 171/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00171: loss did not improve from 9.00526\n",
      "Epoch 172/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00172: loss did not improve from 9.00526\n",
      "Epoch 173/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00173: loss did not improve from 9.00526\n",
      "Epoch 174/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00174: loss did not improve from 9.00526\n",
      "Epoch 175/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00175: loss did not improve from 9.00526\n",
      "Epoch 176/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00176: loss did not improve from 9.00526\n",
      "Epoch 177/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00177: loss did not improve from 9.00526\n",
      "Epoch 178/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00178: loss did not improve from 9.00526\n",
      "Epoch 179/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00179: loss did not improve from 9.00526\n",
      "Epoch 180/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00180: loss did not improve from 9.00526\n",
      "Epoch 181/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00181: loss did not improve from 9.00526\n",
      "Epoch 182/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00182: loss did not improve from 9.00526\n",
      "Epoch 183/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00183: loss did not improve from 9.00526\n",
      "Epoch 184/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00184: loss did not improve from 9.00526\n",
      "Epoch 185/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00185: loss did not improve from 9.00526\n",
      "Epoch 186/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00186: loss did not improve from 9.00526\n",
      "Epoch 187/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00187: loss did not improve from 9.00526\n",
      "Epoch 188/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00188: loss did not improve from 9.00526\n",
      "Epoch 189/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00189: loss did not improve from 9.00526\n",
      "Epoch 190/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00190: loss did not improve from 9.00526\n",
      "Epoch 191/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00191: loss did not improve from 9.00526\n",
      "Epoch 192/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00192: loss did not improve from 9.00526\n",
      "Epoch 193/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00193: loss did not improve from 9.00526\n",
      "Epoch 194/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00194: loss did not improve from 9.00526\n",
      "Epoch 195/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00195: loss did not improve from 9.00526\n",
      "Epoch 196/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00196: loss did not improve from 9.00526\n",
      "Epoch 197/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00197: loss did not improve from 9.00526\n",
      "Epoch 198/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00198: loss did not improve from 9.00526\n",
      "Epoch 199/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00199: loss did not improve from 9.00526\n",
      "Epoch 200/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00200: loss did not improve from 9.00526\n",
      "Epoch 201/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00201: loss did not improve from 9.00526\n",
      "Epoch 202/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00202: loss did not improve from 9.00526\n",
      "Epoch 203/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00203: loss did not improve from 9.00526\n",
      "Epoch 204/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00204: loss did not improve from 9.00526\n",
      "Epoch 205/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00205: loss did not improve from 9.00526\n",
      "Epoch 206/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00206: loss did not improve from 9.00526\n",
      "Epoch 207/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00207: loss did not improve from 9.00526\n",
      "Epoch 208/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00208: loss did not improve from 9.00526\n",
      "Epoch 209/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00209: loss did not improve from 9.00526\n",
      "Epoch 210/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00210: loss did not improve from 9.00526\n",
      "Epoch 211/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00211: loss did not improve from 9.00526\n",
      "Epoch 212/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00212: loss did not improve from 9.00526\n",
      "Epoch 213/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00213: loss did not improve from 9.00526\n",
      "Epoch 214/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00214: loss did not improve from 9.00526\n",
      "Epoch 215/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00215: loss did not improve from 9.00526\n",
      "Epoch 216/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00216: loss did not improve from 9.00526\n",
      "Epoch 217/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00217: loss did not improve from 9.00526\n",
      "Epoch 218/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00218: loss did not improve from 9.00526\n",
      "Epoch 219/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00219: loss did not improve from 9.00526\n",
      "Epoch 220/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00220: loss did not improve from 9.00526\n",
      "Epoch 221/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00221: loss did not improve from 9.00526\n",
      "Epoch 222/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00222: loss did not improve from 9.00526\n",
      "Epoch 223/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00223: loss did not improve from 9.00526\n",
      "Epoch 224/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00224: loss did not improve from 9.00526\n",
      "Epoch 225/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00225: loss did not improve from 9.00526\n",
      "Epoch 226/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00226: loss did not improve from 9.00526\n",
      "Epoch 227/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00227: loss did not improve from 9.00526\n",
      "Epoch 228/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00228: loss did not improve from 9.00526\n",
      "Epoch 229/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00229: loss did not improve from 9.00526\n",
      "Epoch 230/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00230: loss did not improve from 9.00526\n",
      "Epoch 231/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00231: loss did not improve from 9.00526\n",
      "Epoch 232/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00232: loss did not improve from 9.00526\n",
      "Epoch 233/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00233: loss did not improve from 9.00526\n",
      "Epoch 234/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00234: loss did not improve from 9.00526\n",
      "Epoch 235/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00235: loss did not improve from 9.00526\n",
      "Epoch 236/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00236: loss did not improve from 9.00526\n",
      "Epoch 237/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00237: loss did not improve from 9.00526\n",
      "Epoch 238/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00238: loss did not improve from 9.00526\n",
      "Epoch 239/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00239: loss did not improve from 9.00526\n",
      "Epoch 240/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00240: loss did not improve from 9.00526\n",
      "Epoch 241/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00241: loss did not improve from 9.00526\n",
      "Epoch 242/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00242: loss did not improve from 9.00526\n",
      "Epoch 243/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00243: loss did not improve from 9.00526\n",
      "Epoch 244/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00244: loss did not improve from 9.00526\n",
      "Epoch 245/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00245: loss did not improve from 9.00526\n",
      "Epoch 246/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00246: loss did not improve from 9.00526\n",
      "Epoch 247/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00247: loss did not improve from 9.00526\n",
      "Epoch 248/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00248: loss did not improve from 9.00526\n",
      "Epoch 249/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00249: loss did not improve from 9.00526\n",
      "Epoch 250/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00250: loss did not improve from 9.00526\n",
      "Epoch 251/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00251: loss did not improve from 9.00526\n",
      "Epoch 252/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00252: loss did not improve from 9.00526\n",
      "Epoch 253/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00253: loss did not improve from 9.00526\n",
      "Epoch 254/1000\n",
      "8444/8444 [==============================] - 54s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00254: loss did not improve from 9.00526\n",
      "Epoch 255/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00255: loss did not improve from 9.00526\n",
      "Epoch 256/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00256: loss did not improve from 9.00526\n",
      "Epoch 257/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00257: loss did not improve from 9.00526\n",
      "Epoch 258/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00258: loss did not improve from 9.00526\n",
      "Epoch 259/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00259: loss did not improve from 9.00526\n",
      "Epoch 260/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00260: loss did not improve from 9.00526\n",
      "Epoch 261/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00261: loss did not improve from 9.00526\n",
      "Epoch 262/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00262: loss did not improve from 9.00526\n",
      "Epoch 263/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00263: loss did not improve from 9.00526\n",
      "Epoch 264/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00264: loss did not improve from 9.00526\n",
      "Epoch 265/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00265: loss did not improve from 9.00526\n",
      "Epoch 266/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00266: loss did not improve from 9.00526\n",
      "Epoch 267/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00267: loss did not improve from 9.00526\n",
      "Epoch 268/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00268: loss did not improve from 9.00526\n",
      "Epoch 269/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00269: loss did not improve from 9.00526\n",
      "Epoch 270/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00270: loss did not improve from 9.00526\n",
      "Epoch 271/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00271: loss did not improve from 9.00526\n",
      "Epoch 272/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00272: loss did not improve from 9.00526\n",
      "Epoch 273/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00273: loss did not improve from 9.00526\n",
      "Epoch 274/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00274: loss did not improve from 9.00526\n",
      "Epoch 275/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00275: loss did not improve from 9.00526\n",
      "Epoch 276/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00276: loss did not improve from 9.00526\n",
      "Epoch 277/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00277: loss did not improve from 9.00526\n",
      "Epoch 278/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00278: loss did not improve from 9.00526\n",
      "Epoch 279/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00279: loss did not improve from 9.00526\n",
      "Epoch 280/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00280: loss did not improve from 9.00526\n",
      "Epoch 281/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00281: loss did not improve from 9.00526\n",
      "Epoch 282/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00282: loss did not improve from 9.00526\n",
      "Epoch 283/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00283: loss did not improve from 9.00526\n",
      "Epoch 284/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00284: loss did not improve from 9.00526\n",
      "Epoch 285/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00285: loss did not improve from 9.00526\n",
      "Epoch 286/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00286: loss did not improve from 9.00526\n",
      "Epoch 287/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00287: loss did not improve from 9.00526\n",
      "Epoch 288/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00288: loss did not improve from 9.00526\n",
      "Epoch 289/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00289: loss did not improve from 9.00526\n",
      "Epoch 290/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00290: loss did not improve from 9.00526\n",
      "Epoch 291/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00291: loss did not improve from 9.00526\n",
      "Epoch 292/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00292: loss did not improve from 9.00526\n",
      "Epoch 293/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00293: loss did not improve from 9.00526\n",
      "Epoch 294/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00294: loss did not improve from 9.00526\n",
      "Epoch 295/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00295: loss did not improve from 9.00526\n",
      "Epoch 296/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00296: loss did not improve from 9.00526\n",
      "Epoch 297/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00297: loss did not improve from 9.00526\n",
      "Epoch 298/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00298: loss did not improve from 9.00526\n",
      "Epoch 299/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00299: loss did not improve from 9.00526\n",
      "Epoch 300/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00300: loss did not improve from 9.00526\n",
      "Epoch 301/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00301: loss did not improve from 9.00526\n",
      "Epoch 302/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00302: loss did not improve from 9.00526\n",
      "Epoch 303/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00303: loss did not improve from 9.00526\n",
      "Epoch 304/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00304: loss did not improve from 9.00526\n",
      "Epoch 305/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00305: loss did not improve from 9.00526\n",
      "Epoch 306/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00306: loss did not improve from 9.00526\n",
      "Epoch 307/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00307: loss did not improve from 9.00526\n",
      "Epoch 308/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00308: loss did not improve from 9.00526\n",
      "Epoch 309/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00309: loss did not improve from 9.00526\n",
      "Epoch 310/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00310: loss did not improve from 9.00526\n",
      "Epoch 311/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00311: loss did not improve from 9.00526\n",
      "Epoch 312/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00312: loss did not improve from 9.00526\n",
      "Epoch 313/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00313: loss did not improve from 9.00526\n",
      "Epoch 314/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00314: loss did not improve from 9.00526\n",
      "Epoch 315/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00315: loss did not improve from 9.00526\n",
      "Epoch 316/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00316: loss did not improve from 9.00526\n",
      "Epoch 317/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00317: loss did not improve from 9.00526\n",
      "Epoch 318/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00318: loss did not improve from 9.00526\n",
      "Epoch 319/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00319: loss did not improve from 9.00526\n",
      "Epoch 320/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00320: loss did not improve from 9.00526\n",
      "Epoch 321/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00321: loss did not improve from 9.00526\n",
      "Epoch 322/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00322: loss did not improve from 9.00526\n",
      "Epoch 323/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00323: loss did not improve from 9.00526\n",
      "Epoch 324/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00324: loss did not improve from 9.00526\n",
      "Epoch 325/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00325: loss did not improve from 9.00526\n",
      "Epoch 326/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00326: loss did not improve from 9.00526\n",
      "Epoch 327/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00327: loss did not improve from 9.00526\n",
      "Epoch 328/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00328: loss did not improve from 9.00526\n",
      "Epoch 329/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00329: loss did not improve from 9.00526\n",
      "Epoch 330/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00330: loss did not improve from 9.00526\n",
      "Epoch 331/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00331: loss did not improve from 9.00526\n",
      "Epoch 332/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00332: loss did not improve from 9.00526\n",
      "Epoch 333/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00333: loss did not improve from 9.00526\n",
      "Epoch 334/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00334: loss did not improve from 9.00526\n",
      "Epoch 335/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00335: loss did not improve from 9.00526\n",
      "Epoch 336/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00336: loss did not improve from 9.00526\n",
      "Epoch 337/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00337: loss did not improve from 9.00526\n",
      "Epoch 338/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00338: loss did not improve from 9.00526\n",
      "Epoch 339/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00339: loss did not improve from 9.00526\n",
      "Epoch 340/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00340: loss did not improve from 9.00526\n",
      "Epoch 341/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00341: loss did not improve from 9.00526\n",
      "Epoch 342/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00342: loss did not improve from 9.00526\n",
      "Epoch 343/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00343: loss did not improve from 9.00526\n",
      "Epoch 344/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00344: loss did not improve from 9.00526\n",
      "Epoch 345/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00345: loss did not improve from 9.00526\n",
      "Epoch 346/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00346: loss did not improve from 9.00526\n",
      "Epoch 347/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00347: loss did not improve from 9.00526\n",
      "Epoch 348/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00348: loss did not improve from 9.00526\n",
      "Epoch 349/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00349: loss did not improve from 9.00526\n",
      "Epoch 350/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00350: loss did not improve from 9.00526\n",
      "Epoch 351/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00351: loss did not improve from 9.00526\n",
      "Epoch 352/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00352: loss did not improve from 9.00526\n",
      "Epoch 353/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00353: loss did not improve from 9.00526\n",
      "Epoch 354/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00354: loss did not improve from 9.00526\n",
      "Epoch 355/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00355: loss did not improve from 9.00526\n",
      "Epoch 356/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00356: loss did not improve from 9.00526\n",
      "Epoch 357/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00357: loss did not improve from 9.00526\n",
      "Epoch 358/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00358: loss did not improve from 9.00526\n",
      "Epoch 359/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00359: loss did not improve from 9.00526\n",
      "Epoch 360/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00360: loss did not improve from 9.00526\n",
      "Epoch 361/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00361: loss did not improve from 9.00526\n",
      "Epoch 362/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00362: loss did not improve from 9.00526\n",
      "Epoch 363/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00363: loss did not improve from 9.00526\n",
      "Epoch 364/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00364: loss did not improve from 9.00526\n",
      "Epoch 365/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00365: loss did not improve from 9.00526\n",
      "Epoch 366/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00366: loss did not improve from 9.00526\n",
      "Epoch 367/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00367: loss did not improve from 9.00526\n",
      "Epoch 368/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00368: loss did not improve from 9.00526\n",
      "Epoch 369/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00369: loss did not improve from 9.00526\n",
      "Epoch 370/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00370: loss did not improve from 9.00526\n",
      "Epoch 371/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00371: loss did not improve from 9.00526\n",
      "Epoch 372/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00372: loss did not improve from 9.00526\n",
      "Epoch 373/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00373: loss did not improve from 9.00526\n",
      "Epoch 374/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00374: loss did not improve from 9.00526\n",
      "Epoch 375/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00375: loss did not improve from 9.00526\n",
      "Epoch 376/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00376: loss did not improve from 9.00526\n",
      "Epoch 377/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00377: loss did not improve from 9.00526\n",
      "Epoch 378/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00378: loss did not improve from 9.00526\n",
      "Epoch 379/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00379: loss did not improve from 9.00526\n",
      "Epoch 380/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00380: loss did not improve from 9.00526\n",
      "Epoch 381/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00381: loss did not improve from 9.00526\n",
      "Epoch 382/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00382: loss did not improve from 9.00526\n",
      "Epoch 383/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00383: loss did not improve from 9.00526\n",
      "Epoch 384/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00384: loss did not improve from 9.00526\n",
      "Epoch 385/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00385: loss did not improve from 9.00526\n",
      "Epoch 386/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00386: loss did not improve from 9.00526\n",
      "Epoch 387/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00387: loss did not improve from 9.00526\n",
      "Epoch 388/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00388: loss did not improve from 9.00526\n",
      "Epoch 389/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00389: loss did not improve from 9.00526\n",
      "Epoch 390/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00390: loss did not improve from 9.00526\n",
      "Epoch 391/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00391: loss did not improve from 9.00526\n",
      "Epoch 392/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00392: loss did not improve from 9.00526\n",
      "Epoch 393/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00393: loss did not improve from 9.00526\n",
      "Epoch 394/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00394: loss did not improve from 9.00526\n",
      "Epoch 395/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00395: loss did not improve from 9.00526\n",
      "Epoch 396/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00396: loss did not improve from 9.00526\n",
      "Epoch 397/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00397: loss did not improve from 9.00526\n",
      "Epoch 398/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00398: loss did not improve from 9.00526\n",
      "Epoch 399/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00399: loss did not improve from 9.00526\n",
      "Epoch 400/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00400: loss did not improve from 9.00526\n",
      "Epoch 401/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00401: loss did not improve from 9.00526\n",
      "Epoch 402/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00402: loss did not improve from 9.00526\n",
      "Epoch 403/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00403: loss did not improve from 9.00526\n",
      "Epoch 404/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00404: loss did not improve from 9.00526\n",
      "Epoch 405/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00405: loss did not improve from 9.00526\n",
      "Epoch 406/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00406: loss did not improve from 9.00526\n",
      "Epoch 407/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00407: loss did not improve from 9.00526\n",
      "Epoch 408/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00408: loss did not improve from 9.00526\n",
      "Epoch 409/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00409: loss did not improve from 9.00526\n",
      "Epoch 410/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00410: loss did not improve from 9.00526\n",
      "Epoch 411/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00411: loss did not improve from 9.00526\n",
      "Epoch 412/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00412: loss did not improve from 9.00526\n",
      "Epoch 413/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00413: loss did not improve from 9.00526\n",
      "Epoch 414/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00414: loss did not improve from 9.00526\n",
      "Epoch 415/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00415: loss did not improve from 9.00526\n",
      "Epoch 416/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00416: loss did not improve from 9.00526\n",
      "Epoch 417/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00417: loss did not improve from 9.00526\n",
      "Epoch 418/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00418: loss did not improve from 9.00526\n",
      "Epoch 419/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00419: loss did not improve from 9.00526\n",
      "Epoch 420/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00420: loss did not improve from 9.00526\n",
      "Epoch 421/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00421: loss did not improve from 9.00526\n",
      "Epoch 422/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00422: loss did not improve from 9.00526\n",
      "Epoch 423/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00423: loss did not improve from 9.00526\n",
      "Epoch 424/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00424: loss did not improve from 9.00526\n",
      "Epoch 425/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00425: loss did not improve from 9.00526\n",
      "Epoch 426/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00426: loss did not improve from 9.00526\n",
      "Epoch 427/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00427: loss did not improve from 9.00526\n",
      "Epoch 428/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00428: loss did not improve from 9.00526\n",
      "Epoch 429/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00429: loss did not improve from 9.00526\n",
      "Epoch 430/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00430: loss did not improve from 9.00526\n",
      "Epoch 431/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00431: loss did not improve from 9.00526\n",
      "Epoch 432/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00432: loss did not improve from 9.00526\n",
      "Epoch 433/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00433: loss did not improve from 9.00526\n",
      "Epoch 434/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00434: loss did not improve from 9.00526\n",
      "Epoch 435/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00435: loss did not improve from 9.00526\n",
      "Epoch 436/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00436: loss did not improve from 9.00526\n",
      "Epoch 437/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00437: loss did not improve from 9.00526\n",
      "Epoch 438/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00438: loss did not improve from 9.00526\n",
      "Epoch 439/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00439: loss did not improve from 9.00526\n",
      "Epoch 440/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00440: loss did not improve from 9.00526\n",
      "Epoch 441/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00441: loss did not improve from 9.00526\n",
      "Epoch 442/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00442: loss did not improve from 9.00526\n",
      "Epoch 443/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00443: loss did not improve from 9.00526\n",
      "Epoch 444/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00444: loss did not improve from 9.00526\n",
      "Epoch 445/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00445: loss did not improve from 9.00526\n",
      "Epoch 446/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00446: loss did not improve from 9.00526\n",
      "Epoch 447/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00447: loss did not improve from 9.00526\n",
      "Epoch 448/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00448: loss did not improve from 9.00526\n",
      "Epoch 449/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00449: loss did not improve from 9.00526\n",
      "Epoch 450/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00450: loss did not improve from 9.00526\n",
      "Epoch 451/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00451: loss did not improve from 9.00526\n",
      "Epoch 452/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00452: loss did not improve from 9.00526\n",
      "Epoch 453/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00453: loss did not improve from 9.00526\n",
      "Epoch 454/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00454: loss did not improve from 9.00526\n",
      "Epoch 455/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00455: loss did not improve from 9.00526\n",
      "Epoch 456/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00456: loss did not improve from 9.00526\n",
      "Epoch 457/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00457: loss did not improve from 9.00526\n",
      "Epoch 458/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00458: loss did not improve from 9.00526\n",
      "Epoch 459/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00459: loss did not improve from 9.00526\n",
      "Epoch 460/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00460: loss did not improve from 9.00526\n",
      "Epoch 461/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00461: loss did not improve from 9.00526\n",
      "Epoch 462/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00462: loss did not improve from 9.00526\n",
      "Epoch 463/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00463: loss did not improve from 9.00526\n",
      "Epoch 464/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00464: loss did not improve from 9.00526\n",
      "Epoch 465/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00465: loss did not improve from 9.00526\n",
      "Epoch 466/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00466: loss did not improve from 9.00526\n",
      "Epoch 467/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00467: loss did not improve from 9.00526\n",
      "Epoch 468/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00468: loss did not improve from 9.00526\n",
      "Epoch 469/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00469: loss did not improve from 9.00526\n",
      "Epoch 470/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00470: loss did not improve from 9.00526\n",
      "Epoch 471/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00471: loss did not improve from 9.00526\n",
      "Epoch 472/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00472: loss did not improve from 9.00526\n",
      "Epoch 473/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00473: loss did not improve from 9.00526\n",
      "Epoch 474/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00474: loss did not improve from 9.00526\n",
      "Epoch 475/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00475: loss did not improve from 9.00526\n",
      "Epoch 476/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00476: loss did not improve from 9.00526\n",
      "Epoch 477/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00477: loss did not improve from 9.00526\n",
      "Epoch 478/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00478: loss did not improve from 9.00526\n",
      "Epoch 479/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00479: loss did not improve from 9.00526\n",
      "Epoch 480/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00480: loss did not improve from 9.00526\n",
      "Epoch 481/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00481: loss did not improve from 9.00526\n",
      "Epoch 482/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00482: loss did not improve from 9.00526\n",
      "Epoch 483/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00483: loss did not improve from 9.00526\n",
      "Epoch 484/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00484: loss did not improve from 9.00526\n",
      "Epoch 485/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00485: loss did not improve from 9.00526\n",
      "Epoch 486/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00486: loss did not improve from 9.00526\n",
      "Epoch 487/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00487: loss did not improve from 9.00526\n",
      "Epoch 488/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00488: loss did not improve from 9.00526\n",
      "Epoch 489/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00489: loss did not improve from 9.00526\n",
      "Epoch 490/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00490: loss did not improve from 9.00526\n",
      "Epoch 491/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00491: loss did not improve from 9.00526\n",
      "Epoch 492/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00492: loss did not improve from 9.00526\n",
      "Epoch 493/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00493: loss did not improve from 9.00526\n",
      "Epoch 494/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00494: loss did not improve from 9.00526\n",
      "Epoch 495/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00495: loss did not improve from 9.00526\n",
      "Epoch 496/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00496: loss did not improve from 9.00526\n",
      "Epoch 497/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00497: loss did not improve from 9.00526\n",
      "Epoch 498/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00498: loss did not improve from 9.00526\n",
      "Epoch 499/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00499: loss did not improve from 9.00526\n",
      "Epoch 500/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00500: loss did not improve from 9.00526\n",
      "Epoch 501/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00501: loss did not improve from 9.00526\n",
      "Epoch 502/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00502: loss did not improve from 9.00526\n",
      "Epoch 503/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00503: loss did not improve from 9.00526\n",
      "Epoch 504/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00504: loss did not improve from 9.00526\n",
      "Epoch 505/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00505: loss did not improve from 9.00526\n",
      "Epoch 506/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00506: loss did not improve from 9.00526\n",
      "Epoch 507/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00507: loss did not improve from 9.00526\n",
      "Epoch 508/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00508: loss did not improve from 9.00526\n",
      "Epoch 509/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00509: loss did not improve from 9.00526\n",
      "Epoch 510/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00510: loss did not improve from 9.00526\n",
      "Epoch 511/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00511: loss did not improve from 9.00526\n",
      "Epoch 512/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00512: loss did not improve from 9.00526\n",
      "Epoch 513/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00513: loss did not improve from 9.00526\n",
      "Epoch 514/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00514: loss did not improve from 9.00526\n",
      "Epoch 515/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00515: loss did not improve from 9.00526\n",
      "Epoch 516/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00516: loss did not improve from 9.00526\n",
      "Epoch 517/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00517: loss did not improve from 9.00526\n",
      "Epoch 518/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00518: loss did not improve from 9.00526\n",
      "Epoch 519/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00519: loss did not improve from 9.00526\n",
      "Epoch 520/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00520: loss did not improve from 9.00526\n",
      "Epoch 521/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00521: loss did not improve from 9.00526\n",
      "Epoch 522/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00522: loss did not improve from 9.00526\n",
      "Epoch 523/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00523: loss did not improve from 9.00526\n",
      "Epoch 524/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00524: loss did not improve from 9.00526\n",
      "Epoch 525/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00525: loss did not improve from 9.00526\n",
      "Epoch 526/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00526: loss did not improve from 9.00526\n",
      "Epoch 527/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00527: loss did not improve from 9.00526\n",
      "Epoch 528/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00528: loss did not improve from 9.00526\n",
      "Epoch 529/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00529: loss did not improve from 9.00526\n",
      "Epoch 530/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00530: loss did not improve from 9.00526\n",
      "Epoch 531/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00531: loss did not improve from 9.00526\n",
      "Epoch 532/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00532: loss did not improve from 9.00526\n",
      "Epoch 533/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00533: loss did not improve from 9.00526\n",
      "Epoch 534/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00534: loss did not improve from 9.00526\n",
      "Epoch 535/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00535: loss did not improve from 9.00526\n",
      "Epoch 536/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00536: loss did not improve from 9.00526\n",
      "Epoch 537/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00537: loss did not improve from 9.00526\n",
      "Epoch 538/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00538: loss did not improve from 9.00526\n",
      "Epoch 539/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00539: loss did not improve from 9.00526\n",
      "Epoch 540/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00540: loss did not improve from 9.00526\n",
      "Epoch 541/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00541: loss did not improve from 9.00526\n",
      "Epoch 542/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00542: loss did not improve from 9.00526\n",
      "Epoch 543/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00543: loss did not improve from 9.00526\n",
      "Epoch 544/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00544: loss did not improve from 9.00526\n",
      "Epoch 545/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00545: loss did not improve from 9.00526\n",
      "Epoch 546/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00546: loss did not improve from 9.00526\n",
      "Epoch 547/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00547: loss did not improve from 9.00526\n",
      "Epoch 548/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00548: loss did not improve from 9.00526\n",
      "Epoch 549/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00549: loss did not improve from 9.00526\n",
      "Epoch 550/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00550: loss did not improve from 9.00526\n",
      "Epoch 551/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00551: loss did not improve from 9.00526\n",
      "Epoch 552/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00552: loss did not improve from 9.00526\n",
      "Epoch 553/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00553: loss did not improve from 9.00526\n",
      "Epoch 554/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00554: loss did not improve from 9.00526\n",
      "Epoch 555/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00555: loss did not improve from 9.00526\n",
      "Epoch 556/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00556: loss did not improve from 9.00526\n",
      "Epoch 557/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00557: loss did not improve from 9.00526\n",
      "Epoch 558/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00558: loss did not improve from 9.00526\n",
      "Epoch 559/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00559: loss did not improve from 9.00526\n",
      "Epoch 560/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00560: loss did not improve from 9.00526\n",
      "Epoch 561/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00561: loss did not improve from 9.00526\n",
      "Epoch 562/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00562: loss did not improve from 9.00526\n",
      "Epoch 563/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00563: loss did not improve from 9.00526\n",
      "Epoch 564/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00564: loss did not improve from 9.00526\n",
      "Epoch 565/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00565: loss did not improve from 9.00526\n",
      "Epoch 566/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00566: loss did not improve from 9.00526\n",
      "Epoch 567/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00567: loss did not improve from 9.00526\n",
      "Epoch 568/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00568: loss did not improve from 9.00526\n",
      "Epoch 569/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00569: loss did not improve from 9.00526\n",
      "Epoch 570/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00570: loss did not improve from 9.00526\n",
      "Epoch 571/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00571: loss did not improve from 9.00526\n",
      "Epoch 572/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00572: loss did not improve from 9.00526\n",
      "Epoch 573/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00573: loss did not improve from 9.00526\n",
      "Epoch 574/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00574: loss did not improve from 9.00526\n",
      "Epoch 575/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00575: loss did not improve from 9.00526\n",
      "Epoch 576/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00576: loss did not improve from 9.00526\n",
      "Epoch 577/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00577: loss did not improve from 9.00526\n",
      "Epoch 578/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00578: loss did not improve from 9.00526\n",
      "Epoch 579/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00579: loss did not improve from 9.00526\n",
      "Epoch 580/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00580: loss did not improve from 9.00526\n",
      "Epoch 581/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00581: loss did not improve from 9.00526\n",
      "Epoch 582/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00582: loss did not improve from 9.00526\n",
      "Epoch 583/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00583: loss did not improve from 9.00526\n",
      "Epoch 584/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00584: loss did not improve from 9.00526\n",
      "Epoch 585/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00585: loss did not improve from 9.00526\n",
      "Epoch 586/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00586: loss did not improve from 9.00526\n",
      "Epoch 587/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00587: loss did not improve from 9.00526\n",
      "Epoch 588/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00588: loss did not improve from 9.00526\n",
      "Epoch 589/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00589: loss did not improve from 9.00526\n",
      "Epoch 590/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00590: loss did not improve from 9.00526\n",
      "Epoch 591/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00591: loss did not improve from 9.00526\n",
      "Epoch 592/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00592: loss did not improve from 9.00526\n",
      "Epoch 593/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00593: loss did not improve from 9.00526\n",
      "Epoch 594/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00594: loss did not improve from 9.00526\n",
      "Epoch 595/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00595: loss did not improve from 9.00526\n",
      "Epoch 596/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00596: loss did not improve from 9.00526\n",
      "Epoch 597/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00597: loss did not improve from 9.00526\n",
      "Epoch 598/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00598: loss did not improve from 9.00526\n",
      "Epoch 599/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00599: loss did not improve from 9.00526\n",
      "Epoch 600/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00600: loss did not improve from 9.00526\n",
      "Epoch 601/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00601: loss did not improve from 9.00526\n",
      "Epoch 602/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00602: loss did not improve from 9.00526\n",
      "Epoch 603/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00603: loss did not improve from 9.00526\n",
      "Epoch 604/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00604: loss did not improve from 9.00526\n",
      "Epoch 605/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00605: loss did not improve from 9.00526\n",
      "Epoch 606/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00606: loss did not improve from 9.00526\n",
      "Epoch 607/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00607: loss did not improve from 9.00526\n",
      "Epoch 608/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00608: loss did not improve from 9.00526\n",
      "Epoch 609/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00609: loss did not improve from 9.00526\n",
      "Epoch 610/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00610: loss did not improve from 9.00526\n",
      "Epoch 611/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00611: loss did not improve from 9.00526\n",
      "Epoch 612/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00612: loss did not improve from 9.00526\n",
      "Epoch 613/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00613: loss did not improve from 9.00526\n",
      "Epoch 614/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00614: loss did not improve from 9.00526\n",
      "Epoch 615/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00615: loss did not improve from 9.00526\n",
      "Epoch 616/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00616: loss did not improve from 9.00526\n",
      "Epoch 617/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00617: loss did not improve from 9.00526\n",
      "Epoch 618/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00618: loss did not improve from 9.00526\n",
      "Epoch 619/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00619: loss did not improve from 9.00526\n",
      "Epoch 620/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00620: loss did not improve from 9.00526\n",
      "Epoch 621/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00621: loss did not improve from 9.00526\n",
      "Epoch 622/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00622: loss did not improve from 9.00526\n",
      "Epoch 623/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00623: loss did not improve from 9.00526\n",
      "Epoch 624/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00624: loss did not improve from 9.00526\n",
      "Epoch 625/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00625: loss did not improve from 9.00526\n",
      "Epoch 626/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00626: loss did not improve from 9.00526\n",
      "Epoch 627/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00627: loss did not improve from 9.00526\n",
      "Epoch 628/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00628: loss did not improve from 9.00526\n",
      "Epoch 629/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00629: loss did not improve from 9.00526\n",
      "Epoch 630/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00630: loss did not improve from 9.00526\n",
      "Epoch 631/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00631: loss did not improve from 9.00526\n",
      "Epoch 632/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00632: loss did not improve from 9.00526\n",
      "Epoch 633/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00633: loss did not improve from 9.00526\n",
      "Epoch 634/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00634: loss did not improve from 9.00526\n",
      "Epoch 635/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00635: loss did not improve from 9.00526\n",
      "Epoch 636/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00636: loss did not improve from 9.00526\n",
      "Epoch 637/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00637: loss did not improve from 9.00526\n",
      "Epoch 638/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00638: loss did not improve from 9.00526\n",
      "Epoch 639/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00639: loss did not improve from 9.00526\n",
      "Epoch 640/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00640: loss did not improve from 9.00526\n",
      "Epoch 641/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00641: loss did not improve from 9.00526\n",
      "Epoch 642/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00642: loss did not improve from 9.00526\n",
      "Epoch 643/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00643: loss did not improve from 9.00526\n",
      "Epoch 644/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00644: loss did not improve from 9.00526\n",
      "Epoch 645/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00645: loss did not improve from 9.00526\n",
      "Epoch 646/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00646: loss did not improve from 9.00526\n",
      "Epoch 647/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00647: loss did not improve from 9.00526\n",
      "Epoch 648/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00648: loss did not improve from 9.00526\n",
      "Epoch 649/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00649: loss did not improve from 9.00526\n",
      "Epoch 650/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00650: loss did not improve from 9.00526\n",
      "Epoch 651/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00651: loss did not improve from 9.00526\n",
      "Epoch 652/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00652: loss did not improve from 9.00526\n",
      "Epoch 653/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00653: loss did not improve from 9.00526\n",
      "Epoch 654/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00654: loss did not improve from 9.00526\n",
      "Epoch 655/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00655: loss did not improve from 9.00526\n",
      "Epoch 656/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00656: loss did not improve from 9.00526\n",
      "Epoch 657/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00657: loss did not improve from 9.00526\n",
      "Epoch 658/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00658: loss did not improve from 9.00526\n",
      "Epoch 659/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00659: loss did not improve from 9.00526\n",
      "Epoch 660/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00660: loss did not improve from 9.00526\n",
      "Epoch 661/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00661: loss did not improve from 9.00526\n",
      "Epoch 662/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00662: loss did not improve from 9.00526\n",
      "Epoch 663/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00663: loss did not improve from 9.00526\n",
      "Epoch 664/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00664: loss did not improve from 9.00526\n",
      "Epoch 665/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00665: loss did not improve from 9.00526\n",
      "Epoch 666/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00666: loss did not improve from 9.00526\n",
      "Epoch 667/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00667: loss did not improve from 9.00526\n",
      "Epoch 668/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00668: loss did not improve from 9.00526\n",
      "Epoch 669/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00669: loss did not improve from 9.00526\n",
      "Epoch 670/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00670: loss did not improve from 9.00526\n",
      "Epoch 671/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00671: loss did not improve from 9.00526\n",
      "Epoch 672/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00672: loss did not improve from 9.00526\n",
      "Epoch 673/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00673: loss did not improve from 9.00526\n",
      "Epoch 674/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00674: loss did not improve from 9.00526\n",
      "Epoch 675/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00675: loss did not improve from 9.00526\n",
      "Epoch 676/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00676: loss did not improve from 9.00526\n",
      "Epoch 677/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00677: loss did not improve from 9.00526\n",
      "Epoch 678/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00678: loss did not improve from 9.00526\n",
      "Epoch 679/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00679: loss did not improve from 9.00526\n",
      "Epoch 680/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00680: loss did not improve from 9.00526\n",
      "Epoch 681/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00681: loss did not improve from 9.00526\n",
      "Epoch 682/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00682: loss did not improve from 9.00526\n",
      "Epoch 683/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00683: loss did not improve from 9.00526\n",
      "Epoch 684/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00684: loss did not improve from 9.00526\n",
      "Epoch 685/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00685: loss did not improve from 9.00526\n",
      "Epoch 686/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00686: loss did not improve from 9.00526\n",
      "Epoch 687/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00687: loss did not improve from 9.00526\n",
      "Epoch 688/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00688: loss did not improve from 9.00526\n",
      "Epoch 689/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00689: loss did not improve from 9.00526\n",
      "Epoch 690/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00690: loss did not improve from 9.00526\n",
      "Epoch 691/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00691: loss did not improve from 9.00526\n",
      "Epoch 692/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00692: loss did not improve from 9.00526\n",
      "Epoch 693/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00693: loss did not improve from 9.00526\n",
      "Epoch 694/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00694: loss did not improve from 9.00526\n",
      "Epoch 695/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00695: loss did not improve from 9.00526\n",
      "Epoch 696/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00696: loss did not improve from 9.00526\n",
      "Epoch 697/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00697: loss did not improve from 9.00526\n",
      "Epoch 698/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00698: loss did not improve from 9.00526\n",
      "Epoch 699/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00699: loss did not improve from 9.00526\n",
      "Epoch 700/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00700: loss did not improve from 9.00526\n",
      "Epoch 701/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00701: loss did not improve from 9.00526\n",
      "Epoch 702/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00702: loss did not improve from 9.00526\n",
      "Epoch 703/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00703: loss did not improve from 9.00526\n",
      "Epoch 704/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00704: loss did not improve from 9.00526\n",
      "Epoch 705/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00705: loss did not improve from 9.00526\n",
      "Epoch 706/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00706: loss did not improve from 9.00526\n",
      "Epoch 707/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00707: loss did not improve from 9.00526\n",
      "Epoch 708/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00708: loss did not improve from 9.00526\n",
      "Epoch 709/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00709: loss did not improve from 9.00526\n",
      "Epoch 710/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00710: loss did not improve from 9.00526\n",
      "Epoch 711/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00711: loss did not improve from 9.00526\n",
      "Epoch 712/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00712: loss did not improve from 9.00526\n",
      "Epoch 713/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00713: loss did not improve from 9.00526\n",
      "Epoch 714/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00714: loss did not improve from 9.00526\n",
      "Epoch 715/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00715: loss did not improve from 9.00526\n",
      "Epoch 716/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00716: loss did not improve from 9.00526\n",
      "Epoch 717/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00717: loss did not improve from 9.00526\n",
      "Epoch 718/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00718: loss did not improve from 9.00526\n",
      "Epoch 719/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00719: loss did not improve from 9.00526\n",
      "Epoch 720/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00720: loss did not improve from 9.00526\n",
      "Epoch 721/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00721: loss did not improve from 9.00526\n",
      "Epoch 722/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00722: loss did not improve from 9.00526\n",
      "Epoch 723/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00723: loss did not improve from 9.00526\n",
      "Epoch 724/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00724: loss did not improve from 9.00526\n",
      "Epoch 725/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00725: loss did not improve from 9.00526\n",
      "Epoch 726/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00726: loss did not improve from 9.00526\n",
      "Epoch 727/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00727: loss did not improve from 9.00526\n",
      "Epoch 728/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00728: loss did not improve from 9.00526\n",
      "Epoch 729/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00729: loss did not improve from 9.00526\n",
      "Epoch 730/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00730: loss did not improve from 9.00526\n",
      "Epoch 731/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00731: loss did not improve from 9.00526\n",
      "Epoch 732/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00732: loss did not improve from 9.00526\n",
      "Epoch 733/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00733: loss did not improve from 9.00526\n",
      "Epoch 734/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00734: loss did not improve from 9.00526\n",
      "Epoch 735/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00735: loss did not improve from 9.00526\n",
      "Epoch 736/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00736: loss did not improve from 9.00526\n",
      "Epoch 737/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00737: loss did not improve from 9.00526\n",
      "Epoch 738/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00738: loss did not improve from 9.00526\n",
      "Epoch 739/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00739: loss did not improve from 9.00526\n",
      "Epoch 740/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00740: loss did not improve from 9.00526\n",
      "Epoch 741/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00741: loss did not improve from 9.00526\n",
      "Epoch 742/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00742: loss did not improve from 9.00526\n",
      "Epoch 743/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00743: loss did not improve from 9.00526\n",
      "Epoch 744/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00744: loss did not improve from 9.00526\n",
      "Epoch 745/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00745: loss did not improve from 9.00526\n",
      "Epoch 746/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00746: loss did not improve from 9.00526\n",
      "Epoch 747/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00747: loss did not improve from 9.00526\n",
      "Epoch 748/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00748: loss did not improve from 9.00526\n",
      "Epoch 749/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00749: loss did not improve from 9.00526\n",
      "Epoch 750/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00750: loss did not improve from 9.00526\n",
      "Epoch 751/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00751: loss did not improve from 9.00526\n",
      "Epoch 752/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00752: loss did not improve from 9.00526\n",
      "Epoch 753/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00753: loss did not improve from 9.00526\n",
      "Epoch 754/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00754: loss did not improve from 9.00526\n",
      "Epoch 755/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00755: loss did not improve from 9.00526\n",
      "Epoch 756/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00756: loss did not improve from 9.00526\n",
      "Epoch 757/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00757: loss did not improve from 9.00526\n",
      "Epoch 758/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00758: loss did not improve from 9.00526\n",
      "Epoch 759/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00759: loss did not improve from 9.00526\n",
      "Epoch 760/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00760: loss did not improve from 9.00526\n",
      "Epoch 761/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00761: loss did not improve from 9.00526\n",
      "Epoch 762/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00762: loss did not improve from 9.00526\n",
      "Epoch 763/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00763: loss did not improve from 9.00526\n",
      "Epoch 764/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00764: loss did not improve from 9.00526\n",
      "Epoch 765/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00765: loss did not improve from 9.00526\n",
      "Epoch 766/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00766: loss did not improve from 9.00526\n",
      "Epoch 767/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00767: loss did not improve from 9.00526\n",
      "Epoch 768/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00768: loss did not improve from 9.00526\n",
      "Epoch 769/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00769: loss did not improve from 9.00526\n",
      "Epoch 770/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00770: loss did not improve from 9.00526\n",
      "Epoch 771/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00771: loss did not improve from 9.00526\n",
      "Epoch 772/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00772: loss did not improve from 9.00526\n",
      "Epoch 773/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00773: loss did not improve from 9.00526\n",
      "Epoch 774/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00774: loss did not improve from 9.00526\n",
      "Epoch 775/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00775: loss did not improve from 9.00526\n",
      "Epoch 776/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00776: loss did not improve from 9.00526\n",
      "Epoch 777/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00777: loss did not improve from 9.00526\n",
      "Epoch 778/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00778: loss did not improve from 9.00526\n",
      "Epoch 779/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00779: loss did not improve from 9.00526\n",
      "Epoch 780/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00780: loss did not improve from 9.00526\n",
      "Epoch 781/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00781: loss did not improve from 9.00526\n",
      "Epoch 782/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00782: loss did not improve from 9.00526\n",
      "Epoch 783/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00783: loss did not improve from 9.00526\n",
      "Epoch 784/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00784: loss did not improve from 9.00526\n",
      "Epoch 785/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00785: loss did not improve from 9.00526\n",
      "Epoch 786/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00786: loss did not improve from 9.00526\n",
      "Epoch 787/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00787: loss did not improve from 9.00526\n",
      "Epoch 788/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00788: loss did not improve from 9.00526\n",
      "Epoch 789/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00789: loss did not improve from 9.00526\n",
      "Epoch 790/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00790: loss did not improve from 9.00526\n",
      "Epoch 791/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00791: loss did not improve from 9.00526\n",
      "Epoch 792/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00792: loss did not improve from 9.00526\n",
      "Epoch 793/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00793: loss did not improve from 9.00526\n",
      "Epoch 794/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00794: loss did not improve from 9.00526\n",
      "Epoch 795/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00795: loss did not improve from 9.00526\n",
      "Epoch 796/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00796: loss did not improve from 9.00526\n",
      "Epoch 797/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00797: loss did not improve from 9.00526\n",
      "Epoch 798/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00798: loss did not improve from 9.00526\n",
      "Epoch 799/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00799: loss did not improve from 9.00526\n",
      "Epoch 800/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00800: loss did not improve from 9.00526\n",
      "Epoch 801/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00801: loss did not improve from 9.00526\n",
      "Epoch 802/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00802: loss did not improve from 9.00526\n",
      "Epoch 803/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00803: loss did not improve from 9.00526\n",
      "Epoch 804/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00804: loss did not improve from 9.00526\n",
      "Epoch 805/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00805: loss did not improve from 9.00526\n",
      "Epoch 806/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00806: loss did not improve from 9.00526\n",
      "Epoch 807/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00807: loss did not improve from 9.00526\n",
      "Epoch 808/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00808: loss did not improve from 9.00526\n",
      "Epoch 809/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00809: loss did not improve from 9.00526\n",
      "Epoch 810/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00810: loss did not improve from 9.00526\n",
      "Epoch 811/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00811: loss did not improve from 9.00526\n",
      "Epoch 812/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00812: loss did not improve from 9.00526\n",
      "Epoch 813/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00813: loss did not improve from 9.00526\n",
      "Epoch 814/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00814: loss did not improve from 9.00526\n",
      "Epoch 815/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00815: loss did not improve from 9.00526\n",
      "Epoch 816/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00816: loss did not improve from 9.00526\n",
      "Epoch 817/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00817: loss did not improve from 9.00526\n",
      "Epoch 818/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00818: loss did not improve from 9.00526\n",
      "Epoch 819/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00819: loss did not improve from 9.00526\n",
      "Epoch 820/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00820: loss did not improve from 9.00526\n",
      "Epoch 821/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00821: loss did not improve from 9.00526\n",
      "Epoch 822/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00822: loss did not improve from 9.00526\n",
      "Epoch 823/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00823: loss did not improve from 9.00526\n",
      "Epoch 824/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00824: loss did not improve from 9.00526\n",
      "Epoch 825/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00825: loss did not improve from 9.00526\n",
      "Epoch 826/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00826: loss did not improve from 9.00526\n",
      "Epoch 827/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00827: loss did not improve from 9.00526\n",
      "Epoch 828/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00828: loss did not improve from 9.00526\n",
      "Epoch 829/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00829: loss did not improve from 9.00526\n",
      "Epoch 830/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00830: loss did not improve from 9.00526\n",
      "Epoch 831/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00831: loss did not improve from 9.00526\n",
      "Epoch 832/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00832: loss did not improve from 9.00526\n",
      "Epoch 833/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00833: loss did not improve from 9.00526\n",
      "Epoch 834/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00834: loss did not improve from 9.00526\n",
      "Epoch 835/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00835: loss did not improve from 9.00526\n",
      "Epoch 836/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00836: loss did not improve from 9.00526\n",
      "Epoch 837/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00837: loss did not improve from 9.00526\n",
      "Epoch 838/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00838: loss did not improve from 9.00526\n",
      "Epoch 839/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00839: loss did not improve from 9.00526\n",
      "Epoch 840/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00840: loss did not improve from 9.00526\n",
      "Epoch 841/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00841: loss did not improve from 9.00526\n",
      "Epoch 842/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00842: loss did not improve from 9.00526\n",
      "Epoch 843/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00843: loss did not improve from 9.00526\n",
      "Epoch 844/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00844: loss did not improve from 9.00526\n",
      "Epoch 845/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00845: loss did not improve from 9.00526\n",
      "Epoch 846/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00846: loss did not improve from 9.00526\n",
      "Epoch 847/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00847: loss did not improve from 9.00526\n",
      "Epoch 848/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00848: loss did not improve from 9.00526\n",
      "Epoch 849/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00849: loss did not improve from 9.00526\n",
      "Epoch 850/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00850: loss did not improve from 9.00526\n",
      "Epoch 851/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00851: loss did not improve from 9.00526\n",
      "Epoch 852/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00852: loss did not improve from 9.00526\n",
      "Epoch 853/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00853: loss did not improve from 9.00526\n",
      "Epoch 854/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00854: loss did not improve from 9.00526\n",
      "Epoch 855/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00855: loss did not improve from 9.00526\n",
      "Epoch 856/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00856: loss did not improve from 9.00526\n",
      "Epoch 857/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00857: loss did not improve from 9.00526\n",
      "Epoch 858/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00858: loss did not improve from 9.00526\n",
      "Epoch 859/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00859: loss did not improve from 9.00526\n",
      "Epoch 860/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00860: loss did not improve from 9.00526\n",
      "Epoch 861/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00861: loss did not improve from 9.00526\n",
      "Epoch 862/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00862: loss did not improve from 9.00526\n",
      "Epoch 863/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00863: loss did not improve from 9.00526\n",
      "Epoch 864/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00864: loss did not improve from 9.00526\n",
      "Epoch 865/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00865: loss did not improve from 9.00526\n",
      "Epoch 866/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00866: loss did not improve from 9.00526\n",
      "Epoch 867/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00867: loss did not improve from 9.00526\n",
      "Epoch 868/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00868: loss did not improve from 9.00526\n",
      "Epoch 869/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00869: loss did not improve from 9.00526\n",
      "Epoch 870/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00870: loss did not improve from 9.00526\n",
      "Epoch 871/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00871: loss did not improve from 9.00526\n",
      "Epoch 872/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00872: loss did not improve from 9.00526\n",
      "Epoch 873/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00873: loss did not improve from 9.00526\n",
      "Epoch 874/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00874: loss did not improve from 9.00526\n",
      "Epoch 875/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00875: loss did not improve from 9.00526\n",
      "Epoch 876/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00876: loss did not improve from 9.00526\n",
      "Epoch 877/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00877: loss did not improve from 9.00526\n",
      "Epoch 878/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00878: loss did not improve from 9.00526\n",
      "Epoch 879/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00879: loss did not improve from 9.00526\n",
      "Epoch 880/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00880: loss did not improve from 9.00526\n",
      "Epoch 881/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00881: loss did not improve from 9.00526\n",
      "Epoch 882/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00882: loss did not improve from 9.00526\n",
      "Epoch 883/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00883: loss did not improve from 9.00526\n",
      "Epoch 884/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00884: loss did not improve from 9.00526\n",
      "Epoch 885/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00885: loss did not improve from 9.00526\n",
      "Epoch 886/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00886: loss did not improve from 9.00526\n",
      "Epoch 887/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00887: loss did not improve from 9.00526\n",
      "Epoch 888/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00888: loss did not improve from 9.00526\n",
      "Epoch 889/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00889: loss did not improve from 9.00526\n",
      "Epoch 890/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00890: loss did not improve from 9.00526\n",
      "Epoch 891/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00891: loss did not improve from 9.00526\n",
      "Epoch 892/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00892: loss did not improve from 9.00526\n",
      "Epoch 893/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00893: loss did not improve from 9.00526\n",
      "Epoch 894/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00894: loss did not improve from 9.00526\n",
      "Epoch 895/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00895: loss did not improve from 9.00526\n",
      "Epoch 896/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00896: loss did not improve from 9.00526\n",
      "Epoch 897/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00897: loss did not improve from 9.00526\n",
      "Epoch 898/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00898: loss did not improve from 9.00526\n",
      "Epoch 899/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00899: loss did not improve from 9.00526\n",
      "Epoch 900/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00900: loss did not improve from 9.00526\n",
      "Epoch 901/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00901: loss did not improve from 9.00526\n",
      "Epoch 902/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00902: loss did not improve from 9.00526\n",
      "Epoch 903/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00903: loss did not improve from 9.00526\n",
      "Epoch 904/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00904: loss did not improve from 9.00526\n",
      "Epoch 905/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00905: loss did not improve from 9.00526\n",
      "Epoch 906/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00906: loss did not improve from 9.00526\n",
      "Epoch 907/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00907: loss did not improve from 9.00526\n",
      "Epoch 908/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00908: loss did not improve from 9.00526\n",
      "Epoch 909/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00909: loss did not improve from 9.00526\n",
      "Epoch 910/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00910: loss did not improve from 9.00526\n",
      "Epoch 911/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00911: loss did not improve from 9.00526\n",
      "Epoch 912/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00912: loss did not improve from 9.00526\n",
      "Epoch 913/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00913: loss did not improve from 9.00526\n",
      "Epoch 914/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00914: loss did not improve from 9.00526\n",
      "Epoch 915/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00915: loss did not improve from 9.00526\n",
      "Epoch 916/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00916: loss did not improve from 9.00526\n",
      "Epoch 917/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00917: loss did not improve from 9.00526\n",
      "Epoch 918/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00918: loss did not improve from 9.00526\n",
      "Epoch 919/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00919: loss did not improve from 9.00526\n",
      "Epoch 920/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00920: loss did not improve from 9.00526\n",
      "Epoch 921/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00921: loss did not improve from 9.00526\n",
      "Epoch 922/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00922: loss did not improve from 9.00526\n",
      "Epoch 923/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00923: loss did not improve from 9.00526\n",
      "Epoch 924/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00924: loss did not improve from 9.00526\n",
      "Epoch 925/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00925: loss did not improve from 9.00526\n",
      "Epoch 926/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00926: loss did not improve from 9.00526\n",
      "Epoch 927/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00927: loss did not improve from 9.00526\n",
      "Epoch 928/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00928: loss did not improve from 9.00526\n",
      "Epoch 929/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00929: loss did not improve from 9.00526\n",
      "Epoch 930/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00930: loss did not improve from 9.00526\n",
      "Epoch 931/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00931: loss did not improve from 9.00526\n",
      "Epoch 932/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00932: loss did not improve from 9.00526\n",
      "Epoch 933/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00933: loss did not improve from 9.00526\n",
      "Epoch 934/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00934: loss did not improve from 9.00526\n",
      "Epoch 935/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00935: loss did not improve from 9.00526\n",
      "Epoch 936/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00936: loss did not improve from 9.00526\n",
      "Epoch 937/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00937: loss did not improve from 9.00526\n",
      "Epoch 938/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00938: loss did not improve from 9.00526\n",
      "Epoch 939/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00939: loss did not improve from 9.00526\n",
      "Epoch 940/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00940: loss did not improve from 9.00526\n",
      "Epoch 941/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00941: loss did not improve from 9.00526\n",
      "Epoch 942/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00942: loss did not improve from 9.00526\n",
      "Epoch 943/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00943: loss did not improve from 9.00526\n",
      "Epoch 944/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00944: loss did not improve from 9.00526\n",
      "Epoch 945/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00945: loss did not improve from 9.00526\n",
      "Epoch 946/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00946: loss did not improve from 9.00526\n",
      "Epoch 947/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00947: loss did not improve from 9.00526\n",
      "Epoch 948/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00948: loss did not improve from 9.00526\n",
      "Epoch 949/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00949: loss did not improve from 9.00526\n",
      "Epoch 950/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00950: loss did not improve from 9.00526\n",
      "Epoch 951/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00951: loss did not improve from 9.00526\n",
      "Epoch 952/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00952: loss did not improve from 9.00526\n",
      "Epoch 953/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00953: loss did not improve from 9.00526\n",
      "Epoch 954/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00954: loss did not improve from 9.00526\n",
      "Epoch 955/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00955: loss did not improve from 9.00526\n",
      "Epoch 956/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00956: loss did not improve from 9.00526\n",
      "Epoch 957/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00957: loss did not improve from 9.00526\n",
      "Epoch 958/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00958: loss did not improve from 9.00526\n",
      "Epoch 959/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00959: loss did not improve from 9.00526\n",
      "Epoch 960/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00960: loss did not improve from 9.00526\n",
      "Epoch 961/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00961: loss did not improve from 9.00526\n",
      "Epoch 962/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00962: loss did not improve from 9.00526\n",
      "Epoch 963/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00963: loss did not improve from 9.00526\n",
      "Epoch 964/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00964: loss did not improve from 9.00526\n",
      "Epoch 965/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00965: loss did not improve from 9.00526\n",
      "Epoch 966/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00966: loss did not improve from 9.00526\n",
      "Epoch 967/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00967: loss did not improve from 9.00526\n",
      "Epoch 968/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00968: loss did not improve from 9.00526\n",
      "Epoch 969/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00969: loss did not improve from 9.00526\n",
      "Epoch 970/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00970: loss did not improve from 9.00526\n",
      "Epoch 971/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00971: loss did not improve from 9.00526\n",
      "Epoch 972/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00972: loss did not improve from 9.00526\n",
      "Epoch 973/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00973: loss did not improve from 9.00526\n",
      "Epoch 974/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00974: loss did not improve from 9.00526\n",
      "Epoch 975/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00975: loss did not improve from 9.00526\n",
      "Epoch 976/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00976: loss did not improve from 9.00526\n",
      "Epoch 977/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00977: loss did not improve from 9.00526\n",
      "Epoch 978/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00978: loss did not improve from 9.00526\n",
      "Epoch 979/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00979: loss did not improve from 9.00526\n",
      "Epoch 980/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00980: loss did not improve from 9.00526\n",
      "Epoch 981/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00981: loss did not improve from 9.00526\n",
      "Epoch 982/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00982: loss did not improve from 9.00526\n",
      "Epoch 983/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00983: loss did not improve from 9.00526\n",
      "Epoch 984/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00984: loss did not improve from 9.00526\n",
      "Epoch 985/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00985: loss did not improve from 9.00526\n",
      "Epoch 986/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00986: loss did not improve from 9.00526\n",
      "Epoch 987/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00987: loss did not improve from 9.00526\n",
      "Epoch 988/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00988: loss did not improve from 9.00526\n",
      "Epoch 989/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00989: loss did not improve from 9.00526\n",
      "Epoch 990/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00990: loss did not improve from 9.00526\n",
      "Epoch 991/1000\n",
      "8444/8444 [==============================] - 53s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00991: loss did not improve from 9.00526\n",
      "Epoch 992/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00992: loss did not improve from 9.00526\n",
      "Epoch 993/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00993: loss did not improve from 9.00526\n",
      "Epoch 994/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00994: loss did not improve from 9.00526\n",
      "Epoch 995/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00995: loss did not improve from 9.00526\n",
      "Epoch 996/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00996: loss did not improve from 9.00526\n",
      "Epoch 997/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00997: loss did not improve from 9.00526\n",
      "Epoch 998/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00998: loss did not improve from 9.00526\n",
      "Epoch 999/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 00999: loss did not improve from 9.00526\n",
      "Epoch 1000/1000\n",
      "8444/8444 [==============================] - 52s 6ms/step - loss: 9.0445\n",
      "\n",
      "Epoch 01000: loss did not improve from 9.00526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3f26f634d0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Weights-LSTM-improvement-{epoch:02d}-{loss:.4f}-bigger.hdfs\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "\n",
    "model_1000.fit(X, y, epochs=1000, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8544\n",
      "Total Vocab:  8534\n",
      "Total Patterns:  8444\n",
      "Seed:\n",
      "\" it 's a fairy tale that comes from a renowned indian film culture that allows americans to finally revel in its splendor .at once subtle and visceral , the film never succumbs to the trap of the maudlin or tearful , offering instead with its unflinching gaze a measure of faith in the future .the performances of the children , untrained in acting , have an honesty and dignity that breaks your heart .despite its lavish formalism and intellectual austerity , the film manages to keep you at the edge of your seat with its shape-shifting perils , political intrigue and brushes with calamity .this rush to profits has created a predictably efficient piece of business notable largely for its overwhelming creepiness , for an eagerness to create images you wish you had n't seen , which , in this day and age , is of course the point .adams , with four scriptwriters , takes care with the characters , who are so believable that you feel what they feel .a completely spooky piece of business that gets under your skin and , some plot blips aside , stays there for the duration .superbly photographed and staged by mendes with a series of riveting set pieces the likes of which mainstream audiences have rarely seen .the ensemble cast turns in a collectively stellar performance , and the writing is tight and truthful , full of funny situations and honest observations .not quite as miraculous as its dreamworks makers would have you believe , but it more than adequately fills the eyes and stirs the emotions .a properly spooky film about the power of spirits to influence us whether we believe in them or not .the lightest , most breezy movie steven spielberg has made in more than a decade .and the positive change in tone here seems to have recharged him .like edward norton in american history x , ryan gosling ( murder by numbers ) delivers a magnetic performance .this is a very funny , heartwarming film .it has fun with the quirks of family life , but it also treats the subject with fondness and respect .rarely , indeed almost never , is such high-wattage brainpower coupled with pitch-perfect acting and an exquisite , unfakable sense of cinema .the leanest and meanest of solondz 's misanthropic comedies .a dark , quirky road movie that constantly defies expectation .there are some movies that hit you from the first scene and you know it 's going to be a trip .igby goes down is one of those movies .often messy and frustrating , but very pleasing at its best moments , it 's very much like life itself .a burst of color , music , and dance that only the most practiced curmudgeon could fail to crack a smile at .an energetic , violent movie with a momentum that never lets up .lasker 's canny , meditative script distances sex and love , as byron and luther ... realize they ca n't get no satisfaction without the latter .it turns out to be smarter and more diabolical than you could have guessed at the beginning .cage makes an unusual but pleasantly haunting debut behind the camera .noyce has worked wonders with the material .it 's mostly a pleasure to watch .and the reason for that is a self-aware , often self-mocking , intelligence .the chateau is a risky venture that never quite goes where you expect and often surprises you with unexpected comedy .a very well-meaning movie , and it will stand in future years as an eloquent memorial to the world trade center tragedy .there are n't many conclusive answers in the film , but there is an interesting story of pointed personalities , courage , tragedy and the little guys vs. the big guys .vividly demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .a four star performance from kevin kline who unfortunately works with a two star script .dogtown & z-boys evokes the blithe rebel fantasy with the kind of insouciance embedded in the sexy demise of james dean .if you do n't flee , you might be seduced .if you do n't laugh , flee .payne constructs a hilarious ode to middle america and middle age with this unlikely odyssey , featuring a pathetic , endearing hero who is all too human .koury frighteningly and honestly exposes one teenager 's uncomfortable class resentment and , in turn , his self-inflicted retaliation .the santa clause 2 proves itself a more streamlined and thought out encounter than the original could ever have hoped to be .now as a former gong show addict , i 'll admit it , my only complaint is that we did n't get more re-creations of all those famous moments from the show .succeeds where its recent predecessor miserably fails because it demands that you suffer the dreadfulness of war from both sides .the first bond movie in ages that is n't fake fun .this odd , poetic road movie , spiked by jolts of pop music , pretty much takes place in morton 's ever-watchful gaze -- and it 's a tribute to the actress , and to her inventive director , that the journey is such a mesmerizing one .a film centering on a traditional indian wedding in contemporary new delhi may not sound like specialized fare , but mira nair 's film is an absolute delight for all audiences .a weird and wonderful comedy .the movie should jolt you out of your seat a couple of times , give you a few laughs , and leave you feeling like it was worth your seven bucks , even though it does turn out to be a bit of a cheat in the end .has the capability of effecting change and inspiring hope .a first-class , thoroughly involving b movie that effectively combines two surefire , beloved genres -- the prison flick and the fight film .labute 's careful handling makes the material seem genuine rather than pandering .in between all the emotional seesawing , it 's hard to figure the depth of these two literary figures , and even the times in which they lived .but they fascinate in their recklessness .death to smoochy is often very funny , but what 's even more remarkable is the integrity of devito 's misanthropic vision .a beautiful , entertaining two hours .you get the idea , though , that kapur intended the film to be more than that .a wonderful , ghastly film .amid the new populist comedies that underscore the importance of family tradition and familial community , one would be hard-pressed to find a movie with a bigger , fatter heart than barbershop .parris ' performance is credible and remarkably mature .` enigma ' is the kind of engaging historical drama that hollywood appears to have given up on in favor of sentimental war movies in the vein of ` we were soldiers . 'munch 's screenplay is tenderly observant of his characters .he watches them as they float within the seas of their personalities .his scenes are short and often unexpected .it grabs you in the dark and shakes you vigorously for its duration .leigh 's daring here is that without once denying the hardscrabble lives of people on the economic fringes of margaret thatcher 's ruinous legacy , he insists on the importance of those moments when people can connect and express their love for each other .hashiguchi vividly captures the way young japanese live now , chafing against their culture 's manic mix of millennial brusqueness and undying , traditional politesse .uneven but a lot of fun .i know that i 'll never listen to marvin gaye or the supremes the same way againthe two leads , nearly perfect in their roles , bring a heart and reality that buoy the film , and at times , elevate it to a superior crime movie .not as good as the full monty , but a really strong second effort .whenever it threatens to get bogged down in earnest dramaturgy , a stirring visual sequence like a surge through swirling rapids or a leap from pinnacle to pinnacle rouses us .if horses could fly , this is surely what they 'd look like .unfolds as one of the most politically audacious films of recent decades from any country , but especially from france .this real-life hollywood fairy-tale is more engaging than the usual fantasies hollywood produces .the graphic carnage and re-creation of war-torn croatia is uncomfortably timely , relevant , and sickeningly real .left me with the visceral sensation of longing , lasting traces of charlotte 's web of desire and desperation .the characters are more deeply thought through than in most ` right-thinking ' films .crammed with incident , and bristles with passion and energy .it 's fun , splashy and entertainingly nasty .a simple tale of an unlikely friendship , but thanks to the gorgeous locales and exceptional lead performances , it has considerable charm .it might be ` easier ' to watch on video at home , but that should n't stop die-hard french film connoisseurs from going out and enjoying the big-screen experience .there 's very little sense to what 's going on here , but the makers serve up the cliches with considerable dash .witty , contemplative , and sublimely beautiful .a surprisingly ` solid ' achievement by director malcolm d. lee and writer john ridley .woven together handsomely , recalling sixties ' rockumentary milestones from lonely boy to do n't look back .this is pure , exciting moviemaking .you wo n't exactly know what 's happening but you 'll be blissfully exhausted .the 1960s rebellion was misdirected : you ca n't fight your culture .works because reno does n't become smug or sanctimonious towards the audience .nettelbeck ... has a pleasing way with a metaphor .a pure participatory event that malnourished intellectuals will gulp down in a frenzy .the cast delivers without sham the raw-nerved story .steven soderbergh 's digital video experiment is a clever and cutting , quick and dirty look at modern living and movie life .the film 's highlight is definitely its screenplay , both for the rhapsodic dialogue that jumps off the page , and for the memorable character creations .it lets you brush up against the humanity of a psycho , without making him any less psycho .sillier , cuter , and shorter than the first ( as best i remember ) , but still a very good time at the cinema .the film is bright and flashy in all the right ways .elegant and eloquent ( meditation ) on death and that most elusive of passions , love .cut through the layers of soap-opera emotion and you find a scathing portrayal of a powerful entity strangling the life out of the people who want to believe in it the most .filmmaker tian zhuangzhuang triumphantly returns to narrative filmmaking with a visually masterful work of quiet power . \"\n",
      "one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .one of the worst movies of the year .\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "\n",
    "import sys\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model_1000 = Sequential()\n",
    "model_1000.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model_1000.add(Dropout(0.2))\n",
    "model_1000.add(LSTM(256))\n",
    "model_1000.add(Dropout(0.2))\n",
    "model_1000.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Load the network weights\n",
    "filename = \"Weights-LSTM-improvement-80-9.0447-bigger.hdfs\"\n",
    "#filename = \"Weights-LSTM-improvement-44-1.3592-bigger.hdfs\"\n",
    "model_1000.load_weights(filename)\n",
    "\n",
    "model_1000.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(70):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model_1000.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Discussion of the model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the models did pretty well at the initial text character generation. The model trained with 100 epochs, unlike that of 500 epochs and 1000 epochs,  did not make so much sense at the end of the first paragraph. All the models failed at the generation of the second paragraph. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
